{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2377fadb-db27-403d-99b1-262cd6113d71",
   "metadata": {},
   "source": [
    "# Summarize any length of text with Amazon Bedrock\n",
    "In this notebook, we will set up a system to summarize text from one long source, or multiple documents.  This is guided by these three main goals:\n",
    "  1) Accept any length of input documents, and any number of seperate documents.\n",
    "  2) Allow guided summaries.  These are manual or automatically defined suggestions on what should be included in the summary.\n",
    "          -manual guidance can be to look for particular trends or insights and is helpful for creating longer summaries.\n",
    "          -automatic guidance allows the LLM to critique its own summary, and rebuild it based on what should have been included. (goal 3 below)\n",
    "  3) Create an automated feedback loop, allowing the LLM to critically identify and correct weaknesses in the summary.\n",
    "\n",
    "This notebook contains two main functions, one for summarizing single, long texts, and one for summarizing groups of documents.  These tasks are different enough that they require different approached, and therefore are two different functions.  Supporting both of these function, we create four basic helper functions: get_chunks() for breaking long documents down, get_prompt() for creating the many different prompts used, ask_claude for connecting to Bedrock and sending our prompt to the LLM, and finally ask_claude_threaded which allows us to make multiple calls to Claude at the same time..\n",
    "\n",
    "  1) get_chunks - this takes a long text, and returns chunks of appropriate sizes for summarazation.  By default, this includes overlap between chunks.\n",
    "  2) get_prompt - this helper function builds an appropriate prompt for Claude 2, based on options passed in.  In general, this includes prompts for creating summaries, as well as prompts for assessing summary quality.\n",
    "  3) ask_claude - this helper function sends a prompt to Claude 2, and handles some errors that may pop up from the Bedrock endpoint.\n",
    "  4) ask_claude_threaded - a wrapper for ask_claude that allows many requests to be sent at the same time.\n",
    "  \n",
    "This notebook follows this layout:\n",
    "\n",
    "  1) Set up the environment.\n",
    "  2) Define our four helper functions.\n",
    "  3) Set up the two main summarizing functions, using the four helper functions from part (2).\n",
    "  4) Explore using the main summarizing functions for maximum summary quality.\n",
    "  \n",
    "NOTE:  This notebook by default will load a cache of calls to Claude into memeory.  This is helpful when testing and itterating on multi-step prompts, but should be turned off if you would like to generate new versions of the same request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ddb49-e60a-4c46-bd8a-e114f7f7f800",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1) Set up the environment\n",
    "First, let's install some dependances:\n",
    "\n",
    "Note: We install the Anthropic SDK to do local token counting, but do not actually use this in any way to call out to Anthropic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "293ba85a-cb3e-48e2-9d21-38d295037ddd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anthropic in /opt/conda/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from anthropic) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from anthropic) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from anthropic) (0.25.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from anthropic) (2.4.2)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in /opt/conda/lib/python3.10/site-packages (from anthropic) (0.14.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /opt/conda/lib/python3.10/site-packages (from anthropic) (4.8.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<4,>=3.5.0->anthropic) (3.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<4,>=3.5.0->anthropic) (1.2.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->anthropic) (2023.7.22)\n",
      "Requirement already satisfied: httpcore<0.19.0,>=0.18.0 in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->anthropic) (0.18.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->anthropic) (2.10.1)\n",
      "Requirement already satisfied: huggingface_hub<0.18,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers>=0.13.0->anthropic) (0.17.3)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore<0.19.0,>=0.18.0->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.6.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.0->anthropic) (2022.7.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.0->anthropic) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.0->anthropic) (6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.0->anthropic) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.0.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a1debc86-c81d-4ceb-b960-9fca1be17a04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from anthropic import Anthropic\n",
    "client = Anthropic()\n",
    "def count_tokens(text):\n",
    "    return client.count_tokens(text)\n",
    "#count_tokens('Hello world what is up?!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e4ef311-5fc5-4a76-a0ee-1fd02b694282",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#we'll use pickle to load text from our text cleaning notebook, but you can load any text into the full_text variable.  It should be stored as a single string.\n",
    "import pickle, os, re, json\n",
    "#we'll use time to track how long Bedrock takes to respond, which helps to estimate how long a job will take.\n",
    "import time\n",
    "\n",
    "#for ask_claude_threaded, import our threading libraries.\n",
    "from queue import Queue\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e5a6519-78a5-46c7-be15-198db4fef119",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Claude Cache is enabled.  Responses may be stale, and this should be turned off in helper_functions during production use to prevent memory overflow.\n"
     ]
    }
   ],
   "source": [
    "#set CACHE_RESPONCES to true to save all responses from Claude for reuse later.\n",
    "#when true, and request to Claude that has been made before will be served from this cache, rather then sending a request to Bedrock.\n",
    "CACHE_RESPONCES = False\n",
    "if CACHE_RESPONCES: print (\"WARNING: Claude Cache is enabled.  Responses may be stale, and this should be turned off in helper_functions during production use to prevent memory overflow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1007714a-804d-46f2-ac00-1e7fb8a9d113",
   "metadata": {},
   "source": [
    "Next, let's set up the connection to Bedrock:\n",
    "If needed, install at least 1.28.57 of Boto3 so that Bedrock is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b4147e0-80b3-4eb8-98f9-bfe3088d91aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install update boto3==1.28.57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14936cf-b84e-4341-8206-8020747b4465",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for connecting with Bedrock, use Boto3\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "#increase the standard time out limits in boto3, because Bedrock may take a while to respond to large requests.\n",
    "my_config = Config(\n",
    "    connect_timeout=60*3,\n",
    "    read_timeout=60*3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6173c11-e878-4cd6-a1c6-3adc79057d01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock = boto3.client(service_name='bedrock-runtime',config=my_config)\n",
    "bedrock_service = boto3.client(service_name='bedrock',config=my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "05a4a8be-9bdd-4c9a-8583-82092b107a6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claud-v2 found!\n"
     ]
    }
   ],
   "source": [
    "#check that it's working:\n",
    "models = bedrock_service.list_foundation_models()\n",
    "if \"anthropic.claude-v2\" in str(models):\n",
    "    pass#print(\"Claud-v2 found!\")\n",
    "else:\n",
    "    print (\"Error, no model found.\")\n",
    "max_token_count = 12000 #property of Claude 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aa572e-c5c7-496e-b854-f5df51f1d407",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2) Set up the four main functions: get_chunks, get_prompt, ask_claude, and ask_claude_threaded\n",
    "\n",
    "When we have a text to summarize, we'll use get_chunks to break it into the best size for preserving information.  get_prompt will build a prompt based on what we're trying to do, and finally ask_claude will send the prompt to Bedrock while ask_claude_threaded calls Bedrock in parallel on multiple threads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a0fb258-50f2-4f4b-a101-d295332e613e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_chunks(full_text, OVERLAP=True, DEBUG=False):\n",
    "    '''\n",
    "    This will take a text and return an array with sliced chunks of the text in optimal sizing for summarization.  Note that by default, this does include overlaping text in each chunk.\n",
    "    Overlaping allows more cohesion between text, and should only be turned off when trying to count specific numbers and no duplicated text is a requirment.\n",
    "    \n",
    "    We could just drop text up to the maximum context window of our model, but that actually doesn't work very well.\n",
    "    Part of the reason for this is because no matter the input length, the output length is about the same.\n",
    "    For example, if you drop in a paragraph or 10 pages, you get about a paragraph in response.\n",
    "    To mitigate this, we create chunks using the lesser of two values: 25% of the total token count or 2k tokens.\n",
    "    We'll also overlap our chunks by about a paragraph of text or so, in order to provide continuity between chunks.\n",
    "    (Logic taken from https://gist.github.com/Donavan/4fdb489a467efdc1faac0077a151407a)\n",
    "    '''\n",
    "    DEBUG = False #debugging at this level is usually not very helpful.\n",
    "    \n",
    "    #Following testing, it was found that chunks should be 2000 tokens, or 25% of the doc, whichever is shorter.\n",
    "    #max chunk size in tokens\n",
    "    chunk_length_tokens = 2000\n",
    "    #chunk length may be shortened later for shorter docs.\n",
    "    \n",
    "    #a paragraph is about 200 words, which is about 260 tokens on average\n",
    "    #we'll overlap our chunks by a paragraph to provide cohesion to the final summaries.\n",
    "    overlap_tokens = 260\n",
    "    if not OVERLAP: overlap_tokens = 0\n",
    "    \n",
    "    #anything this short doesn't need to be chunked further.\n",
    "    min_chunk_length = 260 + overlap_tokens*2\n",
    "    \n",
    "    \n",
    "    #grab basic info about the text to be chunked.\n",
    "    char_count = len(full_text)\n",
    "    word_count = len(full_text.split(\" \"))#rough estimate\n",
    "    token_count = count_tokens(full_text)\n",
    "    token_per_charater = token_count/char_count\n",
    "\n",
    "    \n",
    "    #don't chunk tiny texts\n",
    "    if token_count <= min_chunk_length:\n",
    "        if DEBUG: print(\"Text is too small to be chunked further\")\n",
    "        return [full_text]\n",
    "    \n",
    "    \n",
    "    if DEBUG:\n",
    "        print (\"Chunk DEBUG mode is on, information about the text and chunking will be printed out.\")\n",
    "        print (\"Estimated character count:\",char_count)\n",
    "        print (\"Estimated word count:\",word_count)\n",
    "        print (\"Estimated token count:\",token_count)\n",
    "        print (\"Estimated tokens per character:\",token_per_charater)\n",
    "\n",
    "        print(\"Full text tokens: \",count_tokens(full_text))\n",
    "        print(\"How many times bigger than max context window: \",round(count_tokens(full_text)/max_token_count,2))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #if the text is shorter, use smaller chunks\n",
    "    if (token_count/4<chunk_length_tokens):\n",
    "        overlap_tokens = int((overlap_tokens/chunk_length_tokens)*int(token_count/4))\n",
    "        chunk_length_tokens = int(token_count/4)\n",
    "        \n",
    "        if DEBUG: \n",
    "            print(\"Short doc detected:\")\n",
    "            print(\"New chunk length:\",chunk_length_tokens)\n",
    "            print(\"New overlap length:\",overlap_tokens)\n",
    "        \n",
    "    #convert to charaters for easy slicing using our approximate tokens per character for this text.\n",
    "    overlap_chars = int(overlap_tokens/token_per_charater)\n",
    "    chunk_length_chars = int(chunk_length_tokens/token_per_charater)\n",
    "    \n",
    "    #itterate and create the chunks from the full text.\n",
    "    chunks = []\n",
    "    start_chunk = 0\n",
    "    end_chunk = chunk_length_chars + overlap_chars\n",
    "    \n",
    "    last_chunk = False\n",
    "    while not last_chunk:\n",
    "        #the last chunk may not be the full length.\n",
    "        if(end_chunk>=char_count):\n",
    "            end_chunk=char_count\n",
    "            last_chunk=True\n",
    "        chunks.append(full_text[start_chunk:end_chunk])\n",
    "        \n",
    "        #move our slice location\n",
    "        if start_chunk == 0:\n",
    "            start_chunk += chunk_length_chars - overlap_chars\n",
    "        else:\n",
    "            start_chunk += chunk_length_chars\n",
    "        \n",
    "        end_chunk = start_chunk + chunk_length_chars + 2 * overlap_chars\n",
    "        \n",
    "    if DEBUG:print (\"Created %s chunks.\"%len(chunks))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cec7db2b-b816-4976-af9a-013663aa7462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\\n\\nHuman:  I am going to give you a text{{GUIDANCE_1}}.  This text is extracted from a larger document.  Here is the text:\n",
    "\n",
    "<text>\n",
    "{{TEXT}}\n",
    "</text>\n",
    "{{GUIDANCE_2}}\n",
    "{{STYLE}}{{REQUEST}}{{FORMAT}}{{GUIDANCE_3}}\n",
    "\\nAssistant:  Here is what you asked for:\n",
    "\"\"\"\n",
    "\n",
    "merge_prompt_template = \"\"\"\\n\\nHuman:  Here are a number of related summaries:\n",
    "\n",
    "{{TEXT}}\n",
    "Please merge these summaries into a highly detailed single summary in {{FORMAT}} format, preserving as much detail as possible, using less than 1000 tokens.\n",
    "\\nAssistant:  Here is what you asked for:\n",
    "\"\"\"\n",
    "\n",
    "#this is inserted into the prompt template above, in the {{GUIDANCE_2}} section.\n",
    "guidance_tempate = \"\"\"\n",
    "Here is the additional guidance:\n",
    "<guidance>\n",
    "{{GUIDANCE}}\n",
    "</guidance>\n",
    "\"\"\"\n",
    "\n",
    "#this prompt asks the LLM to be a newpaper reporter, extracting facts from a document to be used in a later report.  Good for summarizing factual sets of documents.\n",
    "reporter_prompt = \"\"\"\\n\\nHuman:  You are a newspaper reporter, collecting facts to be used in writing an article later.  Consider this source text:\n",
    "<text>\n",
    "{{TEXT}}\n",
    "</text>\n",
    "{{DOCS_DESCRIPTION}}  Please create a {{FORMAT}} of all the relevant facts from this text which will be useful in answering the question \"{{GUIDANCE}}\".  To make your list as clear as possible, do not use and pronouns or ambigious phrases.  For example, use a company's name rather than saying \"the company\" or they.\n",
    "\\nAssistant:  Here is the {{FORMAT}} of relevant facts:\n",
    "\"\"\"\n",
    "\n",
    "reporter_summary_prompt = \"\"\"\\n\\nHuman:  You are a newspaper reporter, collecting facts to be used in writing an article later.  Consider these notes, each one derived from a different source text:\n",
    "{{TEXT}}\n",
    "Please create a {{FORMAT}} of all the relevant facts and trends from these notes which will be useful in answering the question \"{{GUIDANCE}}\"{{STYLE}}.  To make your list as clear as possible, do not use and pronouns or ambigious phrases.  For example, use a company's name rather than saying \"the company\" or \"they\".\n",
    "\\nAssistant:  Here is the list of relevant facts:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "reporter_final_prompt = \"\"\"\\n\\nHuman:  You are a newspaper reporter, writing an article based on facts that were collected and summarized earlier.  Consider these summaries:\n",
    "{{TEXT}}\n",
    "Each summary is a collection of facts extracted from a number of source reports.  Each source report was written by an AWS team talking about their interactions with their individual customer.  Please create a {{FORMAT}} of all the relevant trends and details from these summaries which will be useful in answering the question \"{{GUIDANCE}}\".\n",
    "\\nAssistant:  Here is the narrative:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def get_prompt(text,prompt_type,format_type, manual_guidance, style_guide, docs_description=\"\"):\n",
    "    '''\n",
    "    text should be a single string of the raw text to be sent to the gen ai model.\n",
    "    prompt_type must be \"summary\" or \"interrogate\" or \"answers\"\n",
    "            -summary means summarize the text\n",
    "            -interrogate means look at the text and ask questions about what is missing\n",
    "            -answers means looking at the test, provide only details that may help answer the questions according to the Guidance.\n",
    "            -merge_answers takes a summary as text, and merges in the facts in the guidance section\n",
    "            -merge_summaries takes 2 or more summaries and merges them together.  The summaries to be merged must be in list format for best results.\n",
    "            -reporter - like a new reporter, extract details that help answer the guidance questions\n",
    "            -reporter_summary - like a news reporter looking at a bunch of notes, create a list summary.  Intended as an intermediate step. \n",
    "            reporter_final - generative a narrative based on the reporter_summary outputs.\n",
    "    format_type must be \"narrative\" or \"list\"\n",
    "    manual_guidance Extra instructions to guide the process, usually from the user.\n",
    "    style_guide TBD\n",
    "    \n",
    "    Note that merge_summaries is handled differntly than all other options because it iteratively adds in multiple texts.\n",
    "    '''\n",
    "    \n",
    "    #answers mode is a bit different, so handle that first.\n",
    "    if prompt_type == \"answers\":\n",
    "        format_type = \"in list format, using less than 1000 tokens.  \"\n",
    "        prompt_type = \"Please provide a list of any facts from the text that could be relevant to answering the questions from the guidance section \"\n",
    "        guidance_1 = \" and some guidance\"\n",
    "        guidance_2 = guidance_tempate.replace(\"{{GUIDANCE}}\",manual_guidance)\n",
    "        guidance_3 = \"You should ignore any questions that can not be answered by this text.\"\n",
    "    elif prompt_type == \"reporter\":\n",
    "        return reporter_prompt.replace(\"{{TEXT}}\",text).replace(\"{{FORMAT}}\",format_type).replace(\"{{GUIDANCE}}\",manual_guidance).replace(\"{{DOCS_DESCRIPTION}}\",docs_description)\n",
    "    elif prompt_type == \"reporter_summary\":\n",
    "        summaries_text = \"\"\n",
    "        for x,summary in enumerate(text):\n",
    "            summaries_text += \"<note_%s>\\n%s</note_%s>\\n\"%(x+1,summary,x+1)\n",
    "        final_prompt = reporter_summary_prompt.replace(\"{{TEXT}}\",summaries_text).replace(\"{{FORMAT}}\",format_type).replace(\"{{GUIDANCE}}\",manual_guidance).replace(\"{{STYLE}}\",style_guide)\n",
    "        return final_prompt\n",
    "    elif prompt_type == \"reporter_final\":\n",
    "        summaries_text = \"\"\n",
    "        for x,summary in enumerate(text):\n",
    "            summaries_text += \"<summary_%s>\\n%s</summary_%s>\\n\"%(x+1,summary,x+1)\n",
    "        final_prompt = reporter_final_prompt.replace(\"{{TEXT}}\",summaries_text).replace(\"{{FORMAT}}\",format_type).replace(\"{{GUIDANCE}}\",manual_guidance)\n",
    "        return final_prompt\n",
    "    elif prompt_type == \"merge_summaries\":\n",
    "        summaries_text = \"\"\n",
    "        for x,summary in enumerate(text):\n",
    "            summaries_text += \"<summary_%s>\\n%s</summary_%s>\\n\"%(x+1,summary,x+1)\n",
    "        final_prompt = merge_prompt_template.replace(\"{{TEXT}}\",summaries_text).replace(\"{{FORMAT}}\",format_type)\n",
    "        return final_prompt\n",
    "        \n",
    "    elif prompt_type == \"merge_answers\":\n",
    "        prompt_type = \"The text is a good summary which may lack a few details.  However, the additional information found in the guidance section can be used to make the summary even better.  Starting with the text, please use the details in the guidance section to make the text more detailed.  The new summary shoud use less than 1000 tokens.  \"\n",
    "        format_type = \"\"\n",
    "        guidance_1 = \" and some guidance\"\n",
    "        guidance_2 = guidance_tempate.replace(\"{{GUIDANCE}}\",manual_guidance)\n",
    "        guidance_3 = \"You should ignore any comments in the guidance section indicating that answers could not be found.\"\n",
    "    else:\n",
    "        #Based on the options passed in, grab the correct text to eventually use to build the prompt.\n",
    "        #select the correct type of output format desired, list or summary.  Note that list for interrogate prompts is empty because the request for list is built into that prompt.\n",
    "        if prompt_type == \"interrogate\" and format_type != \"list\":\n",
    "            raise ValueError(\"Only list format is supported for interrogate prompts.\")\n",
    "        if format_type == \"list\":\n",
    "            if prompt_type == \"interrogate\":\n",
    "                format_type = \"\"#already in the prompt so no format needed.\n",
    "            else:\n",
    "                format_type = \"in list format, using less than 1000 tokens.\"\n",
    "        elif format_type == \"narrative\":\n",
    "            format_type = \"in narrative format, using less than 1000 tokens.\"\n",
    "        else:\n",
    "            raise ValueError(\"format_type must be 'narrative' or 'list'.\")\n",
    "\n",
    "        #select the correct prompt type language\n",
    "        if prompt_type == \"summary\":\n",
    "            prompt_type = \"Please provide a highly detailed summary of this text \"\n",
    "        elif prompt_type == \"interrogate\":\n",
    "            prompt_type = \"This text is a summary that lacks detail.  Please provide a list of the top 10 most important questions about this text that can not be answered by the text.\"\n",
    "        else:\n",
    "            raise ValueError(\"prompt_type must be 'summary' or 'interrogate'.\")\n",
    "\n",
    "        if manual_guidance == \"\":\n",
    "            guidance_1 = \"\"\n",
    "            guidance_2 = \"\"\n",
    "            guidance_3 = \"\"\n",
    "        else:\n",
    "            guidance_1 = \" and some guidance\"\n",
    "            guidance_2 = guidance_tempate.replace(\"{{GUIDANCE}}\",manual_guidance)\n",
    "            guidance_3 = \"  As much as possible, also follow the guidance from the guidance section above.  You should ignore guidance that does not seem relevant to this text.\"\n",
    "        \n",
    "    #TBD\n",
    "    style_guide = \"\"\n",
    "    #print (prompt_template.replace(\"{{GUIDANCE_1}}\",guidance_1).replace(\"{{GUIDANCE_2}}\",guidance_2).replace(\"{{GUIDANCE_3}}\",guidance_3).replace(\"{{STYLE}}\",style_guide).replace(\"{{REQUEST}}\",prompt_type).replace(\"{{FORMAT}}\",format_type))\n",
    "    final_prompt = prompt_template.replace(\"{{TEXT}}\",text).replace(\"{{GUIDANCE_1}}\",guidance_1).replace(\"{{GUIDANCE_2}}\",guidance_2).replace(\"{{GUIDANCE_3}}\",guidance_3).replace(\"{{STYLE}}\",style_guide).replace(\"{{REQUEST}}\",prompt_type).replace(\"{{FORMAT}}\",format_type)\n",
    "    return final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f23b8c22-6cfb-4cd3-aedf-16e4c57a69a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new, empty cache of Claude calls.\n"
     ]
    }
   ],
   "source": [
    "#Save our cache of calls to Claude\n",
    "#this speeds things up when testing, because we're often making the same calls to Claude over and over.\n",
    "#where the pickle is may change depending on where this file is run.\n",
    "claude_cache_pickel = \"claude_cache.pkl\"\n",
    "    \n",
    "def save_calls(claude_cache):\n",
    "    with open(claude_cache_pickel, 'wb') as file:\n",
    "        pickle.dump(claude_cache,file)\n",
    "#load our cached calls to Claude\n",
    "def load_calls():\n",
    "    with open(claude_cache_pickel, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "def clear_cache():\n",
    "    claude_cache = {}\n",
    "    save_calls()\n",
    "#a cache of recent requests, to speed up itteration while testing\n",
    "claude_cache = {}\n",
    "\n",
    "if not os.path.exists(claude_cache_pickel):\n",
    "    print (\"Creating new, empty cache of Claude calls.\")\n",
    "    save_calls(claude_cache)\n",
    "\n",
    "if CACHE_RESPONCES:\n",
    "    claude_cache = load_calls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0dd85552-4bb8-41af-a852-30d4d1ac1731",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_ATTEMPTS = 30 #how many times to retry if Claude is not working.\n",
    "def ask_claude(prompt_text, DEBUG=False):\n",
    "    '''\n",
    "    Send a prompt to Bedrock, and return the response.  Debug is used to see exactly what is being sent to and from Bedrock.\n",
    "    TODO:  Add error checking and retry on hitting the throttling limit.\n",
    "    '''\n",
    "    #usually, the prompt will have \"human\" and \"assistant\" tags already.  These are required, so if they are not there, add them in.\n",
    "    if not \"Assistant:\" in prompt_text:\n",
    "        prompt_text = \"\\n\\nHuman:\"+prompt_text+\"\\n\\Assistant: \"\n",
    "        \n",
    "    promt_json = {\n",
    "        \"prompt\": prompt_text,\n",
    "        \"max_tokens_to_sample\": 3000,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_k\": 250,\n",
    "        \"top_p\": 0.7,\n",
    "        \"stop_sequences\": [\"\\n\\nHuman:\"]\n",
    "    }\n",
    "    body = json.dumps(promt_json)\n",
    "    \n",
    "    #returned cashed results, if any\n",
    "    if body in claude_cache:\n",
    "        return claude_cache[body]\n",
    "    \n",
    "    if DEBUG: print(\"sending:\",prompt_text)\n",
    "    modelId = 'anthropic.claude-v2'\n",
    "    accept = 'application/json'\n",
    "    contentType = 'application/json'\n",
    "    \n",
    "    start_time = time.time()\n",
    "    attempt = 1\n",
    "    while True:\n",
    "        try:\n",
    "            query_start_time = time.time()\n",
    "            response = bedrock.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "            response_body = json.loads(response.get('body').read())\n",
    "\n",
    "            raw_results = response_body.get(\"completion\").strip()\n",
    "\n",
    "            #strip out HTML tags that Claude sometimes adds, such as <text>\n",
    "            results = re.sub('<[^<]+?>', '', raw_results)\n",
    "            request_time = round(time.time()-start_time,2)\n",
    "            if DEBUG:\n",
    "                print(\"Recieved:\",results)\n",
    "                print(\"request time (sec):\",request_time)\n",
    "            total_tokens = count_tokens(prompt_text+raw_results)\n",
    "            output_tokens = count_tokens(raw_results)\n",
    "            tokens_per_sec = round(total_tokens/request_time,2)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"Error with calling Bedrock: \"+str(e))\n",
    "            attempt+=1\n",
    "            if attempt>MAX_ATTEMPTS:\n",
    "                print(\"Max attempts reached!\")\n",
    "                results = str(e)\n",
    "                request_time = -1\n",
    "                total_tokens = -1\n",
    "                output_tokens = -1\n",
    "                tokens_per_sec = -1\n",
    "                break\n",
    "            else:#retry in 10 seconds\n",
    "                time.sleep(10)\n",
    "    #store in cache only if it was not an error:\n",
    "    if request_time>0:\n",
    "        claude_cache[body] = (prompt_text,results,total_tokens,output_tokens,request_time,tokens_per_sec,query_start_time)\n",
    "    \n",
    "    return(prompt_text,results,total_tokens,output_tokens,request_time,tokens_per_sec,query_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "94bded51-59a8-4c50-bd9c-7363c1f95e7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "# Threaded function for queue processing.\n",
    "def thread_request(q, result):\n",
    "    while not q.empty():\n",
    "        work = q.get()                      #fetch new work from the Queue\n",
    "        thread_start_time = time.time()\n",
    "        try:\n",
    "            data = ask_claude(work[1])\n",
    "            result[work[0]] = data          #Store data back at correct index\n",
    "        except Exception as e:\n",
    "            error_time = time.time()\n",
    "            print('Error with prompt!',str(e))\n",
    "            result[work[0]] = (work[1],str(e),count_tokens(work[1]),0,round(error_time-thread_start_time,2),0,thread_start_time)\n",
    "        #signal to the queue that task has been processed\n",
    "        q.task_done()\n",
    "    return True\n",
    "\n",
    "def ask_claude_threaded(prompts,DEBUG=False):\n",
    "    '''\n",
    "    Call ask_claude, but multi-threaded.\n",
    "    Returns a dict of the prompts and responces.\n",
    "    '''\n",
    "    q = Queue(maxsize=0)\n",
    "    num_theads = min(50, len(prompts))\n",
    "    \n",
    "    #Populating Queue with tasks\n",
    "    results = [{} for x in prompts];\n",
    "    #load up the queue with the promts to fetch and the index for each job (as a tuple):\n",
    "    for i in range(len(prompts)):\n",
    "        #need the index and the url in each queue item.\n",
    "        q.put((i,prompts[i]))\n",
    "        \n",
    "    #Starting worker threads on queue processing\n",
    "    for i in range(num_theads):\n",
    "        #print('Starting thread ', i)\n",
    "        worker = Thread(target=thread_request, args=(q,results))\n",
    "        worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to \n",
    "                                  #exit eventually even if these dont finish \n",
    "                                  #correctly.\n",
    "        worker.start()\n",
    "\n",
    "    #now we wait until the queue has been processed\n",
    "    q.join()\n",
    "\n",
    "    if DEBUG:print('All tasks completed.')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7b4000b4-ffe8-49d0-8a99-01ce2903efcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing!  Is Claude working? I'm afraid I don't have enough information to know if someone named Claude is working or not. As an AI assistant without personal knowledge of specific people, I can't make assumptions about whether a particular person is currently working or not. I'd need more context to determine that.\n"
     ]
    }
   ],
   "source": [
    "#test if a singe Claude call is working\n",
    "#print(\"Testing!  Is Claude working? \"+ask_claude(\"Is Claude working?\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b00be8c-d926-41ec-a05f-b413b2965582",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2825/662785133.py:38: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n",
      "  worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tasks completed.\n",
      "[('\\n\\nHuman:Please say the number one.\\n\\\\Assistant: ', 'One.', 16, 2, 0.55, 29.09, 1698343933.9464455), ('\\n\\nHuman:Please say the number two.\\n\\\\Assistant: ', 'Two.', 16, 2, 2.65, 6.04, 1698343936.056085)]\n"
     ]
    }
   ],
   "source": [
    "#test if our threaded Claude calls are working\n",
    "#print(ask_claude_threaded([\"Please say the number one.\",\"Please say the number two.\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ba8530-8ed0-4933-8eb8-72a8e11552f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##   3) Set up the main summarizing itterative functions, single and multi doc, using the four helper functions from part (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6e7dbefb-e327-49f7-8e4e-828ad3fb35f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_summary_from_chunks(chunks, prompt_options,DEBUG=False, chunks_already_summarized=False):\n",
    "    \"\"\"\n",
    "    This function itterates through a list of chunks, summarizes them, then merges those summaries together into one.\n",
    "    chunks_already_summarized is used when the chunks passed in are chunks resulting from summerizing docs.\n",
    "    If the chunks are taken from a source document directly, chunks_already_summarized should be set to False.\n",
    "    \"\"\"\n",
    "    partial_summaries = {}\n",
    "    if not chunks_already_summarized:#chunks are from a source doc, so summarize them.\n",
    "        partial_summaries_prompts = []\n",
    "        partial_summaries_prompt2chunk = {}\n",
    "        for x,chunk in enumerate(chunks):\n",
    "            #if DEBUG: print (\"Working on chunk\",x+1,end = '')\n",
    "            start_chunk_time = time.time()\n",
    "            #note that partial summaries are always done in list format to maximize information captured.\n",
    "            custom_prompt = get_prompt(chunk,prompt_options['prompt_type'],'list', prompt_options['manual_guidance'], prompt_options['style_guide'])\n",
    "            #partial_summaries[chunk] = ask_claude(custom_prompt,DEBUG=False)\n",
    "            partial_summaries_prompts.append(custom_prompt)\n",
    "            partial_summaries_prompt2chunk[custom_prompt]=chunk\n",
    "        \n",
    "        partial_summaries_results = ask_claude_threaded(partial_summaries_prompts)\n",
    "        for prompt_text,results,total_tokens,output_tokens,request_time,tokens_per_sec,query_start_time in partial_summaries_results:\n",
    "            partial_summaries[partial_summaries_prompt2chunk[prompt_text]] = results\n",
    "\n",
    "        if DEBUG: \n",
    "            print (\"Partial summary chunks done!\")\n",
    "            print (\"Creating joint summary...\")\n",
    "    else:\n",
    "        for chunk in chunks:\n",
    "            partial_summaries[chunk] = chunk\n",
    "        if DEBUG: \n",
    "            print (\"Summarized chunks detected!\")\n",
    "            print (\"Creating joint summary...\")\n",
    "            \n",
    "    summaries_list = []\n",
    "    summaries_list_token_count = 0\n",
    "    for chunk in chunks:\n",
    "        summaries_list.append(partial_summaries[chunk]) \n",
    "        summaries_list_token_count+=count_tokens(partial_summaries[chunk])\n",
    "        \n",
    "    if DEBUG: print(\"Chunk summaries token count:\",summaries_list_token_count)\n",
    "    \n",
    "    #check to see if the joint summary is too long.  If it is, recursivly itterate down.\n",
    "    #we do this, rather than chunking again, so that summaries are not split.\n",
    "    #it needs to be under 3000 tokens in order to be helpful to the summary (4000 is an expiremental number and may need to be adjusted.)\n",
    "    #this may be higher than the 2000 used for text originally, because this data is in list format.\n",
    "    recombine_token_target = 3000\n",
    "    #summaries_list_token_count = recombine_token_target+1 #set this to target+1 so that we do at least one recombonation for shorter documents.\n",
    "    while summaries_list_token_count>recombine_token_target:\n",
    "        if DEBUG: print(\"Starting reduction loop to merge chunks.  Total token count is %s\"%summaries_list_token_count)\n",
    "        new_summaries_list = []\n",
    "        summaries_list_token_count = 0\n",
    "        temp_summary_group = []\n",
    "        temp_summary_group_token_length = 0\n",
    "        for summary in summaries_list:\n",
    "            if temp_summary_group_token_length + count_tokens(summary) > recombine_token_target:\n",
    "                #the next summary added would push us over the edge, so summarize the current list, and then add it.\n",
    "                #note that partial summaries are always done in list format to maximize information captured.\n",
    "                if DEBUG: print(\"Reducing %s partial summaries into one...\"%(len(temp_summary_group)))\n",
    "                custom_prompt = get_prompt(temp_summary_group,\"merge_summaries\",\"list\", prompt_options['manual_guidance'], prompt_options['style_guide'])\n",
    "                temp_summary = ask_claude(custom_prompt,DEBUG=False)[1]\n",
    "                new_summaries_list.append(temp_summary)\n",
    "                summaries_list_token_count+= count_tokens(temp_summary)\n",
    "                temp_summary_group = []\n",
    "                temp_summary_group_token_length = 0\n",
    "            \n",
    "            temp_summary_group.append(summary)\n",
    "            temp_summary_group_token_length+= count_tokens(summary)\n",
    "        \n",
    "        #summarize whever extra summaries are still in the temp list\n",
    "        if len(temp_summary_group)>1:\n",
    "            if DEBUG: print(\"Starting final reduction of %s partial summaries into one...\"%(len(temp_summary_group)))\n",
    "            custom_prompt = get_prompt(temp_summary_group,\"merge_summaries\",\"list\", prompt_options['manual_guidance'], prompt_options['style_guide'])\n",
    "            temp_summary = ask_claude(custom_prompt,DEBUG=False)[1]\n",
    "            new_summaries_list.append(temp_summary)\n",
    "            summaries_list_token_count+= count_tokens(temp_summary)\n",
    "        elif len(temp_summary_group)==1:\n",
    "            if DEBUG: print(\"Tacking on an extra partial summary\")\n",
    "            new_summaries_list.append(temp_summary_group[0])\n",
    "            summaries_list_token_count+= count_tokens(temp_summary_group[0])\n",
    "            \n",
    "        summaries_list = new_summaries_list\n",
    "        \n",
    "    if DEBUG: print (\"Final merge of summary chunks, merging %s summaries.\"%(len(summaries_list)))\n",
    "    custom_prompt = get_prompt(summaries_list,\"merge_summaries\",prompt_options['format_type'], prompt_options['manual_guidance'], prompt_options['style_guide'])\n",
    "    full_summary = ask_claude(custom_prompt,DEBUG=False)[1]\n",
    "    #full_summary_prompt = get_prompt(\"/n\".join(summaries_list),prompt_options['prompt_type'],prompt_options['format_type'], prompt_options['manual_guidance'], prompt_options['style_guide'])\n",
    "    #full_summary = ask_claude(full_summary_prompt,DEBUG=False)\n",
    "    \n",
    "    return full_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "74c71917-9e39-4a88-94d2-2624d81ec91f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_single_doc_summary(full_text, prompt_options,AUTO_REFINE=True, DEBUG=False,ALREADY_CHUNKED_AND_SUMMED=False):\n",
    "    \"\"\"\n",
    "    This function uses the three helper functions, as well as the generate_summary_from_chunks above, to iteratively generate high quality summaries.\n",
    "    AUTO_REFINE, if true, has the LLM generate a list of questions, and then recursivly calls this function with those questions for guidance.\n",
    "    ALREADY_CHUNKED_AND_SUMMED, if true, means that this is being called using a list of summarized documents which should not be chunked or summarized further.\n",
    "    \"\"\"\n",
    "    #first break this document into chunks\n",
    "    chunks = []        \n",
    "    \n",
    "    if ALREADY_CHUNKED_AND_SUMMED:\n",
    "        chunks = full_text\n",
    "    else:\n",
    "        chunks = get_chunks(full_text,DEBUG=DEBUG)\n",
    "        \n",
    "    if DEBUG:\n",
    "        if prompt_options['prompt_type'] == \"answers\":\n",
    "            print (\"Generating answers using %s chunks.\"%(len(chunks)))\n",
    "        else:\n",
    "            print (\"Generating a new combined summary for %s chunks.\"%(len(chunks)))\n",
    "        if ALREADY_CHUNKED_AND_SUMMED:\n",
    "            print (\"Input has already been chunked and summarized, skipping initial chunking.\")\n",
    "        \n",
    "            \n",
    "    first_summary = generate_summary_from_chunks(chunks,prompt_options,DEBUG=DEBUG, chunks_already_summarized=ALREADY_CHUNKED_AND_SUMMED)\n",
    "    \n",
    "    if DEBUG and AUTO_REFINE: \n",
    "        print (\"First summary:\")\n",
    "        print (first_summary)\n",
    "        \n",
    "    if AUTO_REFINE: \n",
    "        if DEBUG: print (\"Asking the LLM to find weaknesses in this summary...\")\n",
    "        #now that we have a rough summary, let's grab some questions about it.\n",
    "        questions_prompt = get_prompt(first_summary,\"interrogate\",\"list\", \"\", \"\")\n",
    "        questions_list = ask_claude(questions_prompt,DEBUG=False)[1]\n",
    "\n",
    "        if DEBUG: \n",
    "            print (\"Questions from the LLM:\")\n",
    "            print (questions_list)\n",
    "            \n",
    "        original_guidance = prompt_options['manual_guidance']\n",
    "        original_prompt_type = prompt_options['prompt_type']\n",
    "        prompt_options['manual_guidance'] = prompt_options['manual_guidance'] + questions_list\n",
    "        prompt_options['prompt_type'] = \"answers\"\n",
    "        add_details = generate_single_doc_summary(full_text, prompt_options,AUTO_REFINE=False, DEBUG=DEBUG, ALREADY_CHUNKED_AND_SUMMED=ALREADY_CHUNKED_AND_SUMMED)\n",
    "        if DEBUG: \n",
    "            print(\"Additional Details:\")\n",
    "            print (add_details)\n",
    "            print(\"Merging details into original summary...\")\n",
    "        \n",
    "        prompt_options['manual_guidance'] = original_guidance + add_details\n",
    "        prompt_options['prompt_type'] = \"merge_answers\"\n",
    "        custom_prompt = get_prompt(first_summary,prompt_options['prompt_type'],prompt_options['format_type'], prompt_options['manual_guidance'], prompt_options['style_guide'])\n",
    "        final_summary = ask_claude(custom_prompt,DEBUG=False)[1]\n",
    "        \n",
    "        #return this back to the original to prevent weird errors between calls of this function.\n",
    "        prompt_options['manual_guidance'] = original_guidance\n",
    "        prompt_options['prompt_type'] = original_prompt_type\n",
    "        return final_summary\n",
    "    \n",
    "    else:\n",
    "        return first_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bc210047-59ab-4050-9395-14d4951d24fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grab_set_chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\n",
    "    This is a helper function for the multidoc summarization function.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ee133fb0-fb23-42b4-93a5-0ba0915345c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_multiple_docs_summary(docs, questions, docs_description, DEBUG=False):\n",
    "    \"\"\"\n",
    "    This function uses the three helper functions to read the documents passed in, and create a summary answer for each question passed in.\n",
    "    If the documents are longer than two pages or so, it is reccoemended that you first summaize each document.\n",
    "    docs_description is a single sentance describing what the documents are such as \"The texts are a collection of product reviews for a Pickle Ball paddle.\"\n",
    "    \"\"\"\n",
    "    #get answers from each doc for each question.\n",
    "    answers = {}\n",
    "    prompt2quetion_doc = {}\n",
    "    prompts = []\n",
    "    max_docs_to_scan = 500\n",
    "    \n",
    "    #build the queries to be passed into Bedrock\n",
    "    for question in questions:\n",
    "        for x,doc in enumerate(docs):\n",
    "            if x>max_docs_to_scan:break#limit for testing\n",
    "            \n",
    "            #print (\"Asking the LLM to find extract answers from this doc:\",doc)\n",
    "            questions_prompt = get_prompt(docs[doc],\"reporter\",\"list\", question, \"\",docs_description)\n",
    "            prompt2quetion_doc[questions_prompt] = (question,doc) \n",
    "            prompts.append(questions_prompt)\n",
    "        \n",
    "    if DEBUG:print(\"Starting %s worker threads.\"%len(prompts))\n",
    "    prompts_answers = ask_claude_threaded(prompts,DEBUG=False)\n",
    "    save_calls(claude_cache)\n",
    "    \n",
    "    for question in questions:\n",
    "        answers[question] = []    \n",
    "    \n",
    "    for prompt,answer,total_tokens,output_tokens,request_time,tokens_per_sec,query_start_time in prompts_answers:\n",
    "        question,doc = prompt2quetion_doc[prompt]\n",
    "        answers[question].append(answer)\n",
    "        \n",
    "    \n",
    "    current_answer_count = len(docs)\n",
    "    if DEBUG: print(\"All documents have been read.  Reducing answers into the final summary...\")\n",
    "    #reduce this down to 5 or less docs for the final summary by combining the individual answers.\n",
    "    while current_answer_count > 5:\n",
    "        #summarize the answers\n",
    "        prompts = []\n",
    "        prompts2question = {}\n",
    "        \n",
    "        max_docs_to_scan = max(min(current_answer_count,8),3)\n",
    "        if DEBUG: print(\"Combining %s chunks.  (Currently there are %s answers to each question.)\"%(max_docs_to_scan,current_answer_count))\n",
    "        for question in questions:\n",
    "            #print (\"Asking the LLM to summarize answers for this question:\",question)\n",
    "            #You want chunks of roughly 2K tokens\n",
    "            for partial_chunks in grab_set_chunks(answers[question],max_docs_to_scan):\n",
    "                questions_prompt = get_prompt(partial_chunks,\"reporter_summary\",\"list\", question, \" in less than 1000 tokens\")\n",
    "                prompts.append(questions_prompt)\n",
    "                prompts2question[questions_prompt] = question\n",
    "        \n",
    "        if DEBUG:print(\"Starting %s worker threads.\"%len(prompts))\n",
    "        prompts_answers = ask_claude_threaded(prompts,DEBUG=False)\n",
    "        save_calls(claude_cache)\n",
    "        \n",
    "        for question in questions:\n",
    "            answers[question] = []    \n",
    "        for prompt,answer,total_tokens,output_tokens,request_time,tokens_per_sec,query_start_time in prompts_answers:\n",
    "            answers[prompts2question[prompt]].append(answer)        \n",
    "\n",
    "        current_answer_count = len(answers[questions[0]])\n",
    "        \n",
    "    if DEBUG: print(\"Creating the final summary for each question.\")\n",
    "    #write the final article:\n",
    "    prompts = []\n",
    "    prompts2question = {}\n",
    "    for question in questions:\n",
    "        #print (\"Asking the LLM to finalize the answer for this question:\",question)\n",
    "        questions_prompt = get_prompt(answers[question],\"reporter_final\",\"narrative\", question, \"\")\n",
    "        prompts.append(questions_prompt)\n",
    "        prompts2question[questions_prompt] = question\n",
    "\n",
    "    if DEBUG:print(\"Starting %s worker threads.\"%len(prompts))\n",
    "    prompts_answers = ask_claude_threaded(prompts,DEBUG=False)\n",
    "    save_calls(claude_cache)\n",
    "    \n",
    "    answers = {}\n",
    "    for prompt,answer,total_tokens,output_tokens,request_time,tokens_per_sec,query_start_time in prompts_answers:\n",
    "        answers[prompts2question[prompt]] = answer\n",
    "    return answers\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b98b19-95a2-4185-8446-ed445ec9f9f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4) Explore using the main summarizing function for maximum summary quality.\n",
    "For this notebook, we will load sample text from pickles created in the Data Collection and Cleaning notebook included in this repo.\n",
    "If you want to use your own data, check out the Data Collection and Cleaning for examples of how to format the pickles used here.\n",
    "\n",
    "### Example using one long document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8e5dd802-0ae9-4443-8923-27b76587be66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#change to True to run examples.  It's false by default so that this notebook can be used as a resouce elsewhere.\n",
    "RUN_EXAMPLES = False\n",
    "if __name__ == '__main__':\n",
    "    RUN_EXAMPLES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "92387cca-66fa-4f14-b097-b548296cb20d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140397\n"
     ]
    }
   ],
   "source": [
    "if RUN_EXAMPLES:\n",
    "    #file locations for pickels of text.  These are a single string containing the whole text.\n",
    "    #shore, medium, and long texts are provided as exampels.\n",
    "    text_to_open_short = 'sample texts/hills.pkl'  #2-3 page story, Hills like White Elephants\n",
    "    text_to_open_mid = 'sample texts/algernon.pkl'  #short story, Flowers for Algernon\n",
    "    text_to_open_long = 'sample texts/frankenstien.pkl' #short novel, Frankenstine\n",
    "    text_to_open_short_factual = 'sample texts/elvis.pkl'  #longest wikipedia article, Elvis.\n",
    "\n",
    "    with open(text_to_open_mid, 'rb') as file:\n",
    "        full_text = pickle.load(file)\n",
    "    print (len(full_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1cc4d5ad-7011-479d-a36a-048aea7d3f15",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a new combined summary for 18 chunks.\n",
      "Working on chunk 1Working on chunk 2Working on chunk 3Working on chunk 4Working on chunk 5Working on chunk 6Working on chunk 7Working on chunk 8Working on chunk 9Working on chunk 10Working on chunk 11Working on chunk 12Working on chunk 13Working on chunk 14Working on chunk 15Working on chunk 16Working on chunk 17Working on chunk 18All tasks completed.\n",
      "Partial summary chunks done!\n",
      "Creating joint summary...\n",
      "Chunk summaries token count: 4619\n",
      "Starting reduction loop to merge chunks.  Total token count is 4619\n",
      "Reducing 11 partial summaries into one...\n",
      "Starting final reduction of 7 partial summaries into one...\n",
      "Final merge of summary chunks, merging 2 summaries.\n",
      "First summary:\n",
      "Charlie Gordon is a man with an intellectual disability who undergoes an experimental surgery to increase his intelligence. He starts writing progress reports to document the changes. Charlie goes to the Beekman College Center for Retarded Adults to learn reading, writing, and spelling from his teacher Miss Kinnian, who recommended him for the experiment. Psychologists Dr. Strauss and Professor Nemur argue about using Charlie in the experiment, with Dr. Strauss thinking Charlie is a good candidate. Charlie struggles with tests like Rorschach inkblots, mazes, puzzles and remembering details. He feels tricked when he sees things in the inkblots that aren't really there. Charlie's coworkers at Donner's Bakery used to befriend him just to mock him about his disability. Charlie stops going to work there after realizing this. Mr. Donner fires Charlie, saying he doesn't need the job now that he's getting smarter. Charlie's coworkers Frank and Joe are hostile, thinking Charlie is a know-it-all. \n",
      "\n",
      "Charlie sets up a maze at home for Algernon, a mouse that had surgery like Charlie's. At first Charlie beats Algernon at running mazes, but then Algernon starts winning. Charlie goes to movies alone but feels guilty. At a diner, he recognizes a new dishwasher is mentally disabled. Charlie initially laughs at the boy breaking dishes before feeling angry at the cruelty. Charlie reflects on his own experience being mocked for his disability. He wants to use his intelligence to help others. Charlie calls his friend Alice to talk through his thoughts. He wants to do independent work at the lab to research increasing intelligence, but the lab staff is cold to this idea. Algernon starts acting strangely and bites Charlie, so Charlie takes him back to the lab for evaluation.\n",
      "\n",
      "Charlie realizes his increased intelligence has driven a wedge between him and his old friends. He feels alone. Charlie remembers his parents leaving him with a neighbor when his mother went to the hospital to have his sister. His mother slapped him when he tried to pick up his crying sister to comfort her. Miss Kinnian implies Charlie may face challenges from people as he gets smarter. She cries after suggesting this. Charlie overhears Dr. Strauss and Professor Nemur arguing about who deserves credit for Charlie's progress. Charlie realizes they are using him. Charlie asks Dr. Strauss if he can keep some reports private now that he knows others read them. He doesn't want to share all his thoughts openly anymore.\n",
      "\n",
      "Charlie returned to the lab after getting approval to independently study Algernon's behavior. Nemur was cold and formal with Charlie, unhappy that Charlie went over his head. Burt said Algernon's complex responses were wiped out, and he was solving problems at a primitive level. Charlie spent 4 hours learning lab procedures he'd need to know. Charlie asked about plans for him if the experiment fails. Nemur said they had arranged for Charlie to go to the Warren State Home. Charlie wants to visit Warren while he can still do something about his future. Charlie has been immersing himself in psychology texts to understand intelligence and memory. Time feels distorted as Charlie works intensely searching for answers about what is happening to Algernon. Alice helps Charlie by bringing him food, without making demands. \n",
      "\n",
      "Charlie wrote a report showing the increase in intelligence is temporary. He called this the Algernon-Gordon Effect, where artificially increased intelligence deteriorates over time proportional to the amount of increase. Charlie predicts his own mental deterioration will be rapid based on his formulas. He sent his report to Nemur, who had others verify the results. Charlie wants no one to feel guilty about what happens to him. The experiment had good intentions. Charlie recommends no more human testing until more animal research is done.\n",
      "\n",
      "Charlie is losing abilities and memories, with the most recently acquired ones disappearing first. Algernon, the mouse who underwent the same procedure, has died and showed physical brain changes like Charlie's predicted mental decline. Charlie is becoming irritable and having angry outbursts as his condition worsens. He is afraid of regressing completely and wants to document his experience while he still can. Charlie has reunited with Alice, who understands his situation and wants to spend time with him while possible. He struggles with wanting to cherish this time with Alice but also isolate himself as he declines. Charlie is angry and confused when Alice organizes his scattered belongings, seeing it as humoring his condition. He recognizes he is losing motor control, coordination and memory, frequently blaming Alice until realizing it's his own impairment. Charlie questions his unfair reactions to Alice's patient caregiving despite his condition worsening. Charlie relates to a book about Don Quixote, who sees things incorrectly. Charlie realizes he is also seeing things wrong.\n",
      "\n",
      "Charlie Gordon has memory and cognitive issues after the experiment to increase his intelligence. He does not want to see his friend Alice or Dr. Strauss. He wants to be left alone. His landlady Mrs. Mooney brings him food and checks on him. Charlie tries to read but has difficulty understanding books now. He gets frustrated. He misses his mouse Algernon who died. He puts flowers on Algernon's grave. Charlie lost his rabbit's foot and horseshoe which he thinks caused his bad luck. Charlie can no longer read his old progress reports from the experiment. \n",
      "\n",
      "Charlie asks for his old bakery job back. His coworkers are nice to him. One new coworker Meyer Klaus twists Charlie's arm and laughs at him. Charlie wets his pants. Other coworkers stand up for Charlie and threaten to get Klaus fired. But Charlie asks them to give Klaus another chance. Charlie mistakenly goes back to his old classroom. His teacher Miss Kinnian gets upset. Charlie realizes his mistake and leaves. Charlie decides to move to the Warren Home school to avoid embarrassing situations. Charlie took some books with him to keep practicing reading and writing, hoping to get a little smarter again. He also took lucky charms to help. Charlie said he was glad for the chance to be smart for a while. He learned about his family and the world. Charlie doesn't know why he lost his intelligence again. He wishes he could be smart once more and read books all the time. Charlie believes he contributed something important to science as the first \"dumb person\" to gain intelligence. Charlie asked them to put flowers on Algernon's grave.\n",
      "Asking the LLM to find weaknesses in this summary...\n",
      "Questions from the LLM:\n",
      "1. What was the exact experimental surgery Charlie Gordon underwent to increase his intelligence? The text does not provide any details on the procedure.\n",
      "\n",
      "2. What were the specific tests and assessments used by the psychologists to evaluate Charlie's intelligence and progress? The text only mentions general examples like inkblots and mazes.\n",
      "\n",
      "3. What was Charlie's exact diagnosis and level of intellectual disability prior to the experiment? The text just states he had an \"intellectual disability.\"\n",
      "\n",
      "4. What were the qualifications and backgrounds of Dr. Strauss and Professor Nemur? The text does not provide any details on their credentials.\n",
      "\n",
      "5. What was the timeline for the changes in Charlie's intelligence and Algernon's behavior? The text is vague on the rate of change.\n",
      "\n",
      "6. What were the specific physical brain changes observed in Algernon after his intelligence increase deteriorated? The text just states there were changes.\n",
      "\n",
      "7. What was the full methodology and data behind Charlie's report on the Algernon-Gordon Effect? The text only summarizes his findings.\n",
      "\n",
      "8. What did Charlie's independent lab work on Algernon's behavior involve specifically? The text does not provide details. \n",
      "\n",
      "9. What was Warren State Home? The text does not describe this facility Charlie was supposed to go to.\n",
      "\n",
      "10. What were the key insights from the psychology texts Charlie immersed himself in? The content of these texts is not provided.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2825/506474523.py:39: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n",
      "  worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answers using 18 chunks.\n",
      "Working on chunk 1Working on chunk 2Working on chunk 3Working on chunk 4Working on chunk 5Working on chunk 6Working on chunk 7Working on chunk 8Working on chunk 9Working on chunk 10Working on chunk 11Working on chunk 12Working on chunk 13Working on chunk 14Working on chunk 15Working on chunk 16Working on chunk 17Working on chunk 18All tasks completed.\n",
      "Partial summary chunks done!\n",
      "Creating joint summary...\n",
      "Chunk summaries token count: 3967\n",
      "Starting reduction loop to merge chunks.  Total token count is 3967\n",
      "Reducing 13 partial summaries into one...\n",
      "Starting final reduction of 5 partial summaries into one...\n",
      "Final merge of summary chunks, merging 2 summaries.\n",
      "Additional Details:\n",
      "Charlie underwent an experimental surgery and injections to increase his intelligence, though the specifics of the procedure were not detailed. Psychologists administered various tests to evaluate Charlie's progress, including inkblots, mazes, and puzzles, but did not provide the official names of the assessments. Prior to the experiment, Charlie had an unspecified intellectual disability and low IQ. The backgrounds and qualifications of Dr. Strauss and Professor Nemur, who oversaw the experiment, were omitted. After the procedure, there was a timeline showing increased intelligence for both Charlie and Algernon, who also underwent the treatment, but the rate of change was vague. Physical brain changes were observed in Algernon when his intelligence regressed, but the details were not provided. Charlie summarized findings on the Algernon-Gordon Effect, related to the intelligence increases, but the full methodology and data were only summarized, not included. Charlie also did independent lab work involving Algernon's behavior, but the specifics were not given. Warren State Home was mentioned as a facility Charlie would be sent to, but it was not described. Additionally, Charlie immersed himself in psychology texts, but the key insights from them were not provided. The text omitted many key details about the procedure, assessments, personnel, results, and Charlie's learnings.\n",
      "Merging details into original summary...\n",
      "Final Summary:\n",
      "\n",
      "Charlie Gordon is a man with an intellectual disability who undergoes an experimental surgery and injections to increase his intelligence, though the specifics of the procedure were not detailed. Psychologists Dr. Strauss and Professor Nemur administered various tests like Rorschach inkblots, mazes, and puzzles to evaluate Charlie's progress, but did not provide the official names of the assessments. Prior to the experiment, Charlie had an unspecified intellectual disability and low IQ. The backgrounds and qualifications of Dr. Strauss and Professor Nemur, who oversaw the experiment, were omitted. After the procedure, there was a timeline showing increased intelligence for both Charlie and Algernon, who also underwent the treatment, but the rate of change was vague. Physical brain changes were observed in Algernon when his intelligence regressed, but the details were not provided. Charlie summarized findings on the Algernon-Gordon Effect, related to the intelligence increases, but the full methodology and data were only summarized, not included. Charlie also did independent lab work involving Algernon's behavior, but the specifics were not given. Warren State Home was mentioned as a facility Charlie would be sent to, but it was not described. Additionally, Charlie immersed himself in psychology texts, but the key insights from them were not provided. The text omitted many key details about the procedure, assessments, personnel, results, and Charlie's learnings.\n",
      "\n",
      "Charlie goes to the Beekman College Center for Retarded Adults to learn reading, writing, and spelling from his teacher Miss Kinnian, who recommended him for the experiment. Psychologists Dr. Strauss and Professor Nemur argue about using Charlie in the experiment, with Dr. Strauss thinking Charlie is a good candidate. Charlie struggles with tests like Rorschach inkblots, mazes, puzzles and remembering details. He feels tricked when he sees things in the inkblots that aren't really there. Charlie's coworkers at Donner's Bakery used to befriend him just to mock him about his disability. Charlie stops going to work there after realizing this. Mr. Donner fires Charlie, saying he doesn't need the job now that he's getting smarter. Charlie's coworkers Frank and Joe are hostile, thinking Charlie is a know-it-all.\n",
      "\n",
      "Charlie sets up a maze at home for Algernon, a mouse that had surgery like Charlie's. At first Charlie beats Algernon at running mazes, but then Algernon starts winning. Charlie goes to movies alone but feels guilty. At a diner, he recognizes a new dishwasher is mentally disabled. Charlie initially laughs at the boy breaking dishes before feeling angry at the cruelty. Charlie reflects on his own experience being mocked for his disability. He wants to use his intelligence to help others. Charlie calls his friend Alice to talk through his thoughts. He wants to do independent work at the lab to research increasing intelligence, but the lab staff is cold to this idea. Algernon starts acting strangely and bites Charlie, so Charlie takes him back to the lab for evaluation. \n",
      "\n",
      "Charlie realizes his increased intelligence has driven a wedge between him and his old friends. He feels alone. Charlie remembers his parents leaving him with a neighbor when his mother went to the hospital to have his sister. His mother slapped him when he tried to pick up his crying sister to comfort her. Miss Kinnian implies Charlie may face challenges from people as he gets smarter. She cries after suggesting this. Charlie overhears Dr. Strauss and Professor Nemur arguing about who deserves credit for Charlie's progress. Charlie realizes they are using him. Charlie asks Dr. Strauss if he can keep some reports private now that he knows others read them. He doesn't want to share all his thoughts openly anymore.\n",
      "\n",
      "Charlie returned to the lab after getting approval to independently study Algernon's behavior. Nemur was cold and formal with Charlie, unhappy that Charlie went over his head. Burt said Algernon's complex responses were wiped out, and he was solving problems at a primitive level. Charlie spent 4 hours learning lab procedures he'd need to know. Charlie asked about plans for him if the experiment fails. Nemur said they had arranged for Charlie to go to the Warren State Home. Charlie wants to visit Warren while he can still do something about his future. Charlie has been immersing himself in psychology texts to understand intelligence and memory. Time feels distorted as Charlie works intensely searching for answers about what is happening to Algernon. Alice helps Charlie by bringing him food, without making demands.\n",
      "\n",
      "Charlie wrote a report showing the increase in intelligence is temporary. He called this the Algernon-Gordon Effect, where artificially increased intelligence deteriorates over time proportional to the amount of increase. Charlie predicts his own mental deterioration will be rapid based on his formulas. He sent his report to Nemur, who had others verify the results. Charlie wants no one to feel guilty about what happens to him. The experiment had good intentions. Charlie recommends no more human testing until more animal research is done.\n",
      "\n",
      "Charlie is losing abilities and memories, with the most recently acquired ones disappearing first. Algernon, the mouse who underwent the same procedure, has died and showed physical brain changes like Charlie's predicted mental decline. Charlie is becoming irritable and having angry outbursts as his condition worsens. He is afraid of regressing completely and wants to document his experience while he still can. Charlie has reunited with Alice, who understands his situation and wants to spend time with him while possible. He struggles with wanting to cherish this time with Alice but also isolate himself as he declines. Charlie is angry and confused when Alice organizes his scattered belongings, seeing it as humoring his condition. He recognizes he is losing motor control, coordination and memory, frequently blaming Alice until realizing it's his own impairment. Charlie questions his unfair reactions to Alice's patient caregiving despite his condition worsening. Charlie relates to a book about Don Quixote, who sees things incorrectly. Charlie realizes he is also seeing things wrong.\n",
      "\n",
      "Charlie Gordon has memory and cognitive issues after the experiment to increase his intelligence. He does not want to see his friend Alice or Dr. Strauss. He wants to be left alone. His landlady Mrs. Mooney brings him food and checks on him. Charlie tries to read but has difficulty understanding books now. He gets frustrated. He misses his mouse Algernon who died. He puts flowers on Algernon's grave. Charlie lost his rabbit's foot and horseshoe which he thinks caused his bad luck. Charlie can no longer read his old progress reports from the experiment.\n",
      "\n",
      "Charlie asks for his old bakery job back. His coworkers are nice to him. One new coworker Meyer Klaus twists Charlie's arm and laughs at him. Charlie wets his pants. Other coworkers stand up for Charlie and threaten to get Klaus fired. But Charlie asks them to give Klaus another chance. Charlie mistakenly goes back to his old classroom. His teacher Miss Kinnian gets upset. Charlie realizes his mistake and leaves. Charlie decides to move to the Warren Home school to avoid embarrassing situations. Charlie took some books with him to keep practicing reading and writing, hoping to get a little smarter again. He also took lucky charms to help. Charlie said he was glad for the chance to be smart for a while. He learned about his family and the world. Charlie doesn't know why he lost his intelligence again. He wishes he could be smart once more and read books all the time. Charlie believes he contributed something important to science as the first \"dumb person\" to gain intelligence. Charlie asked them to put flowers on Algernon's grave.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "if RUN_EXAMPLES:\n",
    "    prompt_options = {}\n",
    "    prompt_options['prompt_type'] = \"summary\"\n",
    "    prompt_options['format_type'] = \"narrative\"\n",
    "    #prompt_options['manual_guidance'] = \"Assume you are an AWS Sales Executive.  Please include two sections in the summary, one for highlights and positive reactions, and one for challenges and negative reactions.\"\n",
    "    prompt_options['manual_guidance'] = \"\"\n",
    "    prompt_options['style_guide'] = \"\"\n",
    "\n",
    "    revised_summary = generate_single_doc_summary(full_text, prompt_options, AUTO_REFINE=True, DEBUG=True)\n",
    "    print (\"Final Summary:\")\n",
    "    print (revised_summary)\n",
    "    #put call history in pickle\n",
    "    save_calls(claude_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99536eaa-c864-4dc3-82e0-8c008066f8b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Multi-doc summarization example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1b9e787c-269c-447f-ba1a-bfb55edf782c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RUN_EXAMPLES = False\n",
    "if __name__ == '__main__':\n",
    "    RUN_EXAMPLES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1cc81a4c-700d-4bcb-8a4c-0fec1986c880",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "#quick doc summary grab\n",
    "if RUN_EXAMPLES:\n",
    "    text_to_open_doc_group = 'sample texts/docs.pkl'\n",
    "    with open(text_to_open_doc_group, 'rb') as file:\n",
    "        docs = pickle.load(file)\n",
    "    print (len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "26b79ef5-8cf7-4e5f-8888-4ecae7018598",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "questions = [\"What are the highlights?\",\n",
    "\"What are the challenges?\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "77cfd4fb-afa8-42e2-a960-1ee6674f9132",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 20 worker threads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2825/506474523.py:39: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n",
      "  worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tasks completed.\n",
      "All documents have been read.  Reducing answers into the final summary...\n",
      "Combining 8 chunks.  (Currently there are 10 answers to each question.)\n",
      "Starting 4 worker threads.\n",
      "All tasks completed.\n",
      "Creating the final summary for each question.\n",
      "Starting 2 worker threads.\n",
      "All tasks completed.\n",
      "\n",
      "What are the highlights?\n",
      "\n",
      "The Panel Sound pickleball set seems to provide good value and convenience for most customers, though some have had issues with durability. The set includes lightweight fiberglass paddles, indoor and outdoor balls, a carrying case, and cooling towels. Reviewers found the paddles easy to use and maneuverable, though they may not suit all players' preferences. One reviewer was new to pickleball and found the set a good way to try the sport. Some found the cooling towels small. \n",
      "There were a couple cases of Panel Sound paddles coming loose or breaking during use after several months. One customer could not get a refund or replacement due to the 30 day return policy. Another could not get warranty support, possibly due to the paddles being manufactured in China. \n",
      "One customer purchased an unspecified brand of pickleball paddle that broke in half after one use 9 months after purchase. They described the paddle as poor quality and could not find warranty information for the brand.\n",
      "In summary, the Panel Sound set seems decent though some paddles may have durability issues. There was also a report of another brand of paddle breaking quickly, which the customer felt was poor quality with no warranty support. Overall quality and durability of pickleball paddles, as well as warranty support, seem to be concerns for some customers.\n",
      "\n",
      "What are the challenges?\n",
      "\n",
      "Based on the summaries, there appear to be some quality and durability issues with certain pickleball paddle brands and products.  \n",
      "The Panel Sound pickleball paddle set received mixed reviews. Some customers found the paddles easy to use and liked the included carrying case. However, multiple customers reported problems with the paddles breaking or coming loose during play, causing them to lose power and control. One customer felt the Panel Sound paddles were too lightweight and lacked power compared to other paddles used. There were also complaints that the Panel Sound warranty was not honored and the manufacturer could not be contacted.\n",
      "Similarly, one unnamed pickleball paddle brand was described as not sturdy enough by a customer. This customer reported that their paddle from this brand shattered inside after less than a year of use, despite claiming to not be careless with it. They described the quality as poor. This customer was also unable to find warranty information for the defective paddle. \n",
      "In summary, the key challenges highlighted are:\n",
      "- Durability issues with paddles breaking, coming loose, or shattering during play after only a short period of use\n",
      "- Paddles losing power and control when defects occur\n",
      "- Lack of responsiveness from manufacturers regarding warranties for defective products\n",
      "- Inability to contact manufacturers for warranty claims or other issues\n",
      "- Frustration with paddle quality, construction and performance not meeting expectations\n",
      "CPU times: user 430 ms, sys: 10.5 ms, total: 440 ms\n",
      "Wall time: 54.3 s\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "if RUN_EXAMPLES:\n",
    "    doc_description = \"The texts are a collection of product reviews for a Pickle Ball paddle.\"\n",
    "    answers = generate_multiple_docs_summary(docs, questions, doc_description, DEBUG=True)\n",
    "\n",
    "    for question in answers:\n",
    "        print ()\n",
    "        print (question)\n",
    "        print ()\n",
    "        print (answers[question].replace(\"\\n\\n\",\"\\n\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57a1c75-7149-4a0a-ad7d-971fb519b870",
   "metadata": {},
   "source": [
    "#### Save Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4f9328b2-76db-458e-a5c3-8fcda3991561",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if RUN_EXAMPLES:\n",
    "    for question in questions:\n",
    "        with open(\"results/\"+question+\".txt\",\"w\") as fout:\n",
    "            fout.write(answers[question])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb186473-3b29-454c-be56-4b91fb625eb1",
   "metadata": {},
   "source": [
    "## Enable a Command Line interface, for interactive testing from CLI\n",
    "\n",
    "To use this, first convert the notebook to a python script, then run that from the CLI.\n",
    "\n",
    "jupyter nbconvert --to python summarize.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2c668273-0e1a-4ba6-a1f2-e487ea04d6a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "enable_CLI = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f0ebf7b5-f78e-40f2-aab8-88b8cb08a35e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from pickle.\n",
      "8 docs loaded.\n",
      "Please enter a single sentence description of what the docs are.  For example \"These documents are reviews from Amazon.com\"\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " The documents are a sales summaries for AWS.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What would you like to ask? (type quit to end the program.)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " quit\n"
     ]
    }
   ],
   "source": [
    "if enable_CLI:\n",
    "    print (\"Loading data from pickle.\")\n",
    "    text_to_open_doc_group = 'example.pkl'\n",
    "    with open(text_to_open_doc_group, 'rb') as file:\n",
    "        docs = pickle.load(file)\n",
    "    print(\"%s docs loaded.\"%len(docs))\n",
    "    \n",
    "    print (\"Please enter a single sentence description of what the docs are.  For example \\\"These documents are reviews from Amazon.com\\\"\")\n",
    "    doc_description = input()\n",
    "    while True:\n",
    "        print (\"\\nWhat would you like to ask? (type quit to end the program.)\\n\")\n",
    "        questions = [input()]\n",
    "        \n",
    "        if questions[0] in [\"quit\",\"q\"]:\n",
    "            break\n",
    "        \n",
    "        answers = generate_multiple_docs_summary(docs, questions, doc_description, DEBUG=True)\n",
    "        \n",
    "        for question in answers:\n",
    "            print ()\n",
    "            print (question)\n",
    "            print ()\n",
    "            print (answers[question].replace(\"\\n\\n\",\"\\n\"))\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5be49f-e8b6-4d65-b1fe-20f6ac0873a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
