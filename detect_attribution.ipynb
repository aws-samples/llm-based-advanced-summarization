{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2377fadb-db27-403d-99b1-262cd6113d71",
   "metadata": {},
   "source": [
    "# Detect Attributions with Amazon Bedrock\n",
    "Goodbye Hallucinations, hello Attributions.  When an LLM generates output, there's always a risk that it hallucinates, or makes up facts that are not true.  This causes users to lose trust in LLM based solutions.  While there are a number of methods that attempt to detect these hallucinations, this notebook is targeted towards the opposite; detecting attributions.  For every fact or claim in an LLM's output, we will try to detect exactly where that fact came from.  In this way we both give a user confidence that any fact is grounded in truth, and detect by omission any hallucinations, or claims that are not supported by facts.  This system is intended to work in the context of RAG, where we assume every fact needed to create the output is present in the input, and we want the LLM to base its response only on facts present in the input.\n",
    "  \n",
    "This notebook has three main parts:\n",
    "  1) Set up the environment.  We import libraries and create basic building blocks for part two\n",
    "  2) Build attribution functionality.  Here we build the capability to analyze LLM output, and list attributions.\n",
    "  3) Testing and examples.  Test the method on real data.\n",
    "  \n",
    "NOTE:  This notebook by default will load a cache of calls to Claude into memeory so that a duplicated request will instantly return the previous result, rather than asking Claude again.  This is helpful when testing and demoing, but should be turned off if you would like to generate new responses of the same request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ddb49-e60a-4c46-bd8a-e114f7f7f800",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1) Set up the environment\n",
    "These are basic functions and libraries that you might find in any application that uses Bedrock.  Note this must be run from a machine or account role that has permission to access Bedrock."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d964d193-fb69-46e8-a846-878eea7bea94",
   "metadata": {},
   "source": [
    "First, let's install some dependances:\n",
    "\n",
    "Note: We install the Anthropic SDK to do local token counting, but do not actually use this in any way to call out to Anthropic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "293ba85a-cb3e-48e2-9d21-38d295037ddd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1debc86-c81d-4ceb-b960-9fca1be17a04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "client = Anthropic()\n",
    "def count_tokens(text):\n",
    "    return client.count_tokens(text)\n",
    "#count_tokens('Hello world what is up?!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92d3e4c-b410-402a-a181-08a76a4222e5",
   "metadata": {},
   "source": [
    "We'll install the HuggingFace datasets library, for use in supplying sample data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca70da2c-268a-4dd3-8f30-13f27d9e18ec",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (13.0.0)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.26.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.64.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.6)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets)\n",
      "  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, pyarrow-hotfix, fsspec, huggingface-hub, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.7.1\n",
      "    Uninstalling fsspec-2022.7.1:\n",
      "      Successfully uninstalled fsspec-2022.7.1\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.17.3\n",
      "    Uninstalling huggingface-hub-0.17.3:\n",
      "      Successfully uninstalled huggingface-hub-0.17.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.3.3 which is incompatible.\n",
      "tokenizers 0.14.1 requires huggingface_hub<0.18,>=0.16.4, but you have huggingface-hub 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-2.16.1 fsspec-2023.10.0 huggingface-hub-0.20.3 pyarrow-hotfix-0.6 xxhash-3.4.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1421dbb-5c80-4493-955b-2cac3c001ab4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grab the sentance tokenizer, for use in assigning each sentance a number.  (may also need to pip install nltk)\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e4ef311-5fc5-4a76-a0ee-1fd02b694282",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle, os, re, json\n",
    "#we'll use time to track how long Bedrock takes to respond, which helps to estimate how long a job will take.\n",
    "import time\n",
    "\n",
    "#for ask_claude_threaded, import our threading libraries.\n",
    "from queue import Queue\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e5a6519-78a5-46c7-be15-198db4fef119",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Claude Cache is enabled.  Responses may be stale, and this should be turned off in helper_functions during production use to prevent memory overflow.\n"
     ]
    }
   ],
   "source": [
    "#set CACHE_RESPONCES to true to save all responses from Claude for reuse later.\n",
    "#when true, and request to Claude that has been made before will be served from this cache, rather then sending a request to Bedrock.\n",
    "CACHE_RESPONCES = True\n",
    "if CACHE_RESPONCES: print (\"WARNING: Claude Cache is enabled.  Responses may be stale, and this should be turned off in helper_functions during production use to prevent memory overflow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1007714a-804d-46f2-ac00-1e7fb8a9d113",
   "metadata": {},
   "source": [
    "Next, let's set up the connection to Bedrock:\n",
    "If needed, install at least 1.28.57 of Boto3 so that Bedrock is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b4147e0-80b3-4eb8-98f9-bfe3088d91aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install update boto3==1.28.57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c14936cf-b84e-4341-8206-8020747b4465",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for connecting with Bedrock, use Boto3\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "#increase the standard time out limits in boto3, because Bedrock may take a while to respond to large requests.\n",
    "my_config = Config(\n",
    "    connect_timeout=60*3,\n",
    "    read_timeout=60*3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6173c11-e878-4cd6-a1c6-3adc79057d01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock = boto3.client(service_name='bedrock-runtime',config=my_config)\n",
    "bedrock_service = boto3.client(service_name='bedrock',config=my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05a4a8be-9bdd-4c9a-8583-82092b107a6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#check that it's working:\n",
    "models = bedrock_service.list_foundation_models()\n",
    "if \"anthropic.claude-v2\" in str(models):\n",
    "    pass#print(\"Claud-v2 found!\")\n",
    "else:\n",
    "    print (\"Error, no model found.\")\n",
    "max_token_count = 100000 #property of Claude 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f23b8c22-6cfb-4cd3-aedf-16e4c57a69a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new, empty cache of Claude calls.\n"
     ]
    }
   ],
   "source": [
    "#Save our cache of calls to Claude\n",
    "#this speeds things up when testing, because we're often making the same calls to Claude over and over.\n",
    "claude_cache_pickel = \"claude_cache.pkl\"\n",
    "    \n",
    "def save_calls(claude_cache):\n",
    "    with open(claude_cache_pickel, 'wb') as file:\n",
    "        pickle.dump(claude_cache,file)\n",
    "#load our cached calls to Claude\n",
    "def load_calls():\n",
    "    with open(claude_cache_pickel, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "def clear_cache():\n",
    "    claude_cache = {}\n",
    "    save_calls()\n",
    "#a cache of recent requests, to speed up itteration while testing\n",
    "claude_cache = {}\n",
    "\n",
    "if not os.path.exists(claude_cache_pickel):\n",
    "    print (\"Creating new, empty cache of Claude calls.\")\n",
    "    save_calls(claude_cache)\n",
    "\n",
    "if CACHE_RESPONCES:\n",
    "    claude_cache = load_calls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dd85552-4bb8-41af-a852-30d4d1ac1731",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_ATTEMPTS = 5 #how many times to retry if Claude is not working.\n",
    "def ask_claude(prompt_text, DEBUG=False):\n",
    "    '''\n",
    "    Send a prompt to Bedrock, and return the response.  Debug is used to see exactly what is being sent to and from Bedrock.\n",
    "    '''\n",
    "    #usually, the prompt will have \"human\" and \"assistant\" tags already.  These are required, so if they are not there, add them in.\n",
    "    if not \"Assistant:\" in prompt_text:\n",
    "        prompt_text = \"\\n\\nHuman:\"+prompt_text+\"\\n\\Assistant: \"\n",
    "        \n",
    "    promt_json = {\n",
    "        \"prompt\": prompt_text,\n",
    "        \"max_tokens_to_sample\": 3000,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_k\": 250,\n",
    "        \"top_p\": 0.7,\n",
    "        \"stop_sequences\": [\"\\n\\nHuman:\"]\n",
    "    }\n",
    "    body = json.dumps(promt_json)\n",
    "    \n",
    "    #returned cashed results, if any\n",
    "    if body in claude_cache:\n",
    "        return claude_cache[body]\n",
    "    \n",
    "    if DEBUG: print(\"sending:\",prompt_text)\n",
    "    modelId = 'anthropic.claude-v2'\n",
    "    accept = 'application/json'\n",
    "    contentType = 'application/json'\n",
    "    \n",
    "    start_time = time.time()\n",
    "    attempt = 1\n",
    "    while True:\n",
    "        try:\n",
    "            query_start_time = time.time()\n",
    "            response = bedrock.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "            response_body = json.loads(response.get('body').read())\n",
    "\n",
    "            raw_results = response_body.get(\"completion\").strip()\n",
    "\n",
    "            #strip out HTML tags that Claude sometimes adds, such as <text>\n",
    "            #results = re.sub('<[^<]+?>', '', raw_results)\n",
    "            results = raw_results\n",
    "            \n",
    "            request_time = round(time.time()-start_time,2)\n",
    "            if DEBUG:\n",
    "                print(\"Recieved:\",results)\n",
    "                print(\"request time (sec):\",request_time)\n",
    "            #total_tokens = count_tokens(prompt_text+raw_results)\n",
    "            #output_tokens = count_tokens(raw_results)\n",
    "            #tokens_per_sec = round(total_tokens/request_time,2)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"Error with calling Bedrock: \"+str(e))\n",
    "            attempt+=1\n",
    "            if attempt>MAX_ATTEMPTS:\n",
    "                print(\"Max attempts reached!\")\n",
    "                results = str(e)\n",
    "                request_time = -1\n",
    "                #total_tokens = -1\n",
    "                #output_tokens = -1\n",
    "                #tokens_per_sec = -1\n",
    "                break\n",
    "            else:#retry in 10 seconds\n",
    "                time.sleep(10)\n",
    "    #store in cache only if it was not an error:\n",
    "    if request_time>0:\n",
    "        claude_cache[body] = (prompt_text,results,request_time)\n",
    "    \n",
    "    return(prompt_text,results,request_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56655094-f49c-48a4-95d4-9a0a95d13b34",
   "metadata": {},
   "source": [
    "In the next cell, we add queue handleing.  This allows us to make multiple requests to Bedrock at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94bded51-59a8-4c50-bd9c-7363c1f95e7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "# Threaded function for queue processing.\n",
    "def thread_request(q, result):\n",
    "    while not q.empty():\n",
    "        work = q.get()                      #fetch new work from the Queue\n",
    "        thread_start_time = time.time()\n",
    "        try:\n",
    "            data = ask_claude(work[1])\n",
    "            result[work[0]] = data          #Store data back at correct index\n",
    "        except Exception as e:\n",
    "            error_time = time.time()\n",
    "            print('Error with prompt!',str(e))\n",
    "            result[work[0]] = (work[1],str(e),round(error_time-thread_start_time,2))\n",
    "        #signal to the queue that task has been processed\n",
    "        q.task_done()\n",
    "    return True\n",
    "\n",
    "def ask_claude_threaded(prompts,DEBUG=False):\n",
    "    '''\n",
    "    Call ask_claude, but multi-threaded.\n",
    "    Returns a dict of the prompts and responces.\n",
    "    '''\n",
    "    q = Queue(maxsize=0)\n",
    "    num_theads = min(50, len(prompts))\n",
    "    \n",
    "    #Populating Queue with tasks\n",
    "    results = [{} for x in prompts];\n",
    "    #load up the queue with the promts to fetch and the index for each job (as a tuple):\n",
    "    for i in range(len(prompts)):\n",
    "        #need the index and the url in each queue item.\n",
    "        q.put((i,prompts[i]))\n",
    "        \n",
    "    #Starting worker threads on queue processing\n",
    "    for i in range(num_theads):\n",
    "        #print('Starting thread ', i)\n",
    "        worker = Thread(target=thread_request, args=(q,results))\n",
    "        worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to \n",
    "                                  #exit eventually even if these dont finish \n",
    "                                  #correctly.\n",
    "        worker.start()\n",
    "\n",
    "    #now we wait until the queue has been processed\n",
    "    q.join()\n",
    "\n",
    "    if DEBUG:print('All tasks completed.')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b4000b4-ffe8-49d0-8a99-01ce2903efcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing!  Is Claude working? I'm afraid I don't have enough information to know if someone named Claude is working or not. As an AI assistant without personal knowledge of Claude, I can't make assumptions about what specific people are currently doing.\n"
     ]
    }
   ],
   "source": [
    "#test if a singe Claude call is working\n",
    "#print(\"Testing!  Is Claude working? \"+ask_claude(\"Is Claude working?\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b00be8c-d926-41ec-a05f-b413b2965582",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8808/4180077186.py:39: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n",
      "  worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\\n\\nHuman:Please say the number one.\\n\\\\Assistant: ', 'One.', 1.84), ('\\n\\nHuman:Please say the number two.\\n\\\\Assistant: ', 'Two.', 2.89)]\n"
     ]
    }
   ],
   "source": [
    "#test if our threaded Claude calls are working\n",
    "#print(ask_claude_threaded([\"Please say the number one.\",\"Please say the number two.\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f335f4c-178c-4943-b9eb-b8e9187e093f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2) Build attribution functionality\n",
    "These are the functions specific to detecting attribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "24f7b085-df51-4527-8ff6-040446e7d30b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_line_numbers(text):\n",
    "    '''\n",
    "    This function takes a text, and adds XML style line numbers to each sentance.\n",
    "    This allows the LLM to have a way to reference which lines support attribution.\n",
    "    \n",
    "    The text is passed in as a plain text string, and is returned as a string with tags added.\n",
    "    '''\n",
    "    lines = nltk.sent_tokenize(text)\n",
    "    \n",
    "    labeled_text = \"\"\n",
    "    for line_number,line in enumerate(lines):\n",
    "        labeled_text += \"<line number=%s>%s</line>\"%(line_number+1,line)\n",
    "        \n",
    "    return labeled_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2691c233-1587-4faa-ab97-3df34b3b7099",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_line_numbers_dict(text):\n",
    "    '''\n",
    "    This function takes a text, and adds XML style line numbers to each sentance.\n",
    "    This allows the LLM to have a way to reference which lines support attribution.\n",
    "    \n",
    "    The text is passed in as a plain text string, but the results are returned as a dict.\n",
    "    This is useful for printing out the results of attribution.\n",
    "    '''\n",
    "    lines = nltk.sent_tokenize(text)\n",
    "    temp_dict = {}\n",
    "    for line_number,line in enumerate(lines):\n",
    "        temp_dict[line_number+1]=line\n",
    "        \n",
    "    return temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "d5cc1046-a42d-4dba-87fb-92cc27e9a1dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "detect_attribution_template = \"\"\"\\n\\nHuman:  You are given a reference source, and a statement based on that source.  Your job is to find every single line in the reference which helps to support the claims made in the statement.\n",
    "Here is the source and statement:\n",
    "<source>\n",
    "{{INPUT}}\n",
    "</source>\n",
    "<statement>\n",
    "{{OUTPUT}}\n",
    "</statement>\n",
    "For each line in the statement, please list all the line numbers from the source, if any, which support that line in the statement.\n",
    "Your response should be in JSON format, with a key for each line number in the statement, and values which are the supporting line numbers from the source.\n",
    "\\nAssistant:  Here is what you asked for:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_attribution_prompt(text,summary):\n",
    "    '''\n",
    "    create the prompt for detecting attribution\n",
    "    '''\n",
    "    \n",
    "    final_prompt = detect_attribution_template.replace(\"{{INPUT}}\",text).replace(\"{{OUTPUT}}\",summary)\n",
    "    return final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "8c09989a-7cd8-4ebd-9034-7424bcffa895",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "detect_fact_score_template = \"\"\"\\n\\nHuman: You will be given a statement.  It may contain false content, and therefore is being reviewed in a two step process.\n",
    "The first step is to identify all lines that make factual claims, and the second step is to find sources that support those claims.\n",
    "You are a detail oriented expert in charge of the first step of this process.\n",
    "You job is to look at each line in the statement, and score how likely it is to require proof from trustworthy sources.\n",
    "For example, the sentence \"I have a hat.\" makes a factual claim, and should score high on the need for support, while the sentence \"Here is a summary.\" is a helper sentence, and therefore does not need external support and should score lower.\n",
    "Here is the statement:\n",
    "<statement>\n",
    "{{TEXT}}\n",
    "</statement>\n",
    "For each line in the statement, please score each line on a scale from 0 to 100, where 0 indicates a helper sentence that requires no support and makes no factual claims, and 100 is a factual claim that requires support from trustworthy sources.\n",
    "Your response should be in JSON format, with a key for each line number in the statement, and values which are the score for that statement.\n",
    "\\nAssistant:  Here is what you asked for:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_fact_score_prompt(text):\n",
    "    '''\n",
    "    create the prompt for scoring each line's need for factual support.\n",
    "    '''\n",
    "    final_prompt = detect_fact_score_template.replace(\"{{TEXT}}\",text)\n",
    "    return final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f3ee1ace-d9a8-4727-bb47-5acc98c6d7d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_attribution(text,summary):\n",
    "    labeled_article = add_line_numbers(text)\n",
    "    labeled_summary = add_line_numbers(summary)\n",
    "\n",
    "    prompt = get_attribution_prompt(labeled_article,labeled_summary)\n",
    "    result = ask_claude(prompt)\n",
    "    return result\n",
    "def get_fact_score(text):\n",
    "    labeled_article = add_line_numbers(text)\n",
    "\n",
    "    prompt = get_fact_score_prompt(labeled_article)\n",
    "    result = ask_claude(prompt)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "13608420-635e-4760-965d-df5fe023cd71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_attribution_print_results(text,summary):\n",
    "    \"\"\"\n",
    "    Bundle everything up and print out results, for testing and seeing quick results.\n",
    "    \"\"\"\n",
    "    result = get_attribution(text,summary)[1]\n",
    "    score_result = get_fact_score(summary)[1]\n",
    "    \n",
    "    test_article_lines = add_line_numbers_dict(text)\n",
    "    test_highlights_lines = add_line_numbers_dict(summary)\n",
    "    result_json = json.loads(result)\n",
    "    score_result_json = json.loads(score_result) \n",
    "    \n",
    "    for line in result_json:\n",
    "        print (\"Output line %s: (Facts required score: %s)\"%(line,score_result_json[line]))\n",
    "        print(test_highlights_lines[int(line)])\n",
    "        print (\"Supporting facts:\")\n",
    "        if len(result_json[line])==0:\n",
    "            hallucination = \"\"\n",
    "            if score_result_json[line]>50:\n",
    "                hallucination = \"  Likely hallucination detected!\"\n",
    "            print (\"  No supporting facts found.\"+hallucination)\n",
    "        else:\n",
    "            for line_2 in result_json[line]:\n",
    "\n",
    "                print(\"  %s: \"%line_2+test_article_lines[int(line_2)])\n",
    "        print (\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4b5d63-a295-4588-bcee-af86fef1e12b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3). Testing and Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a7a99fa-c76b-4118-a23e-de5188a03af0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "375639910b3446a8a465d95b49769e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42bdb184818d48849a2d7dab8672413c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/257M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d70716aa318b46ba91122d21954f1b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/259M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def4b7f094d94953bb0baa1ac47d8c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/34.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bdfcd3b33c74072b180298da2679e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/30.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c4856e114e04a3a81add2c5cba01e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e8cb39081f84302aed7b0852f2a51f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38c809bf12f4c8bb88112ecfd7af425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#we'll start by downloading some sample data for testing.\n",
    "#we use https://huggingface.co/datasets/cnn_dailymail which is a collection of news articles and their highlights.\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"cnn_dailymail\",'1.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "aa6630c0-cba9-4f7a-a4f4-de527a2ab663",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CNN)Filipinos are being warned to be on guard for flash floods and landslides as tropical storm Maysak approached the Asian island nation Saturday. Just a few days ago, Maysak gained super typhoon status thanks to its sustained 150 mph winds. It has since lost a lot of steam as it has spun west in the Pacific Ocean. It's now classified as a tropical storm, according to the Philippine national weather service, which calls it a different name, Chedeng. It boasts steady winds of more than 70 mph (115 kph) and gusts up to 90 mph as of 5 p.m. (5 a.m. ET) Saturday. Still, that doesn't mean Maysak won't pack a wallop. Authorities took preemptive steps to keep people safe such as barring outdoor activities like swimming, surfing, diving and boating in some locales, as well as a number of precautionary evacuations. Gabriel Llave, a disaster official, told PNA that tourists who arrive Saturday in and around the coastal town of Aurora \"will not be accepted by the owners of hotels, resorts, inns and the like ... and will be advised to return to their respective places.\" Aldczar Aurelio, a meteorologist with the Philippine Atmospheric, Geophysical and Astronomical Services Administration (PAGASA), said the storm was centered 200 miles southwest of Aurora province as of 5 p.m. (5 a.m. ET) and heading west at a 12.5 mph clip. It's expected to make landfall Sunday morning on the southeastern coast of Isabela province and be out of the Philippines by Monday. Ahead of the storm, Isabela Gov Faustino Dry III warned Saturday that residents should act as if this will be \"no ordinary typhoon.\" Dry told PNA, \"We do not know what the impact will be once it will make landfall.\"\n",
      " \n",
      "Human generated highlights:\n",
      "Once a super typhoon, Maysak is now a tropical storm with 70 mph winds . It could still cause flooding, landslides and other problems in the Philippines .  I love to eat donuts for breakfast.\n"
     ]
    }
   ],
   "source": [
    "#let's pick an article for testing, grab one about weather so it's not too depressing...\n",
    "article_num = 8\n",
    "test_article = dataset['test'][article_num]['article']\n",
    "test_article = test_article.replace(\"Ahead of the storm.\",\"Ahead of the storm,\")#typo correction\n",
    "test_article = test_article.replace(\"Gov.\",\"Gov\")#easier to read\n",
    "test_highlight = dataset['test'][article_num]['highlights']\n",
    "fake_test_highlight = test_highlight + \"  I love to eat donuts for breakfast.\"\n",
    "print (test_article)\n",
    "print (\" \")\n",
    "print (\"Human generated highlights:\")\n",
    "print (fake_test_highlight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9c60b9ab-4c82-459b-aa38-dff257bc2e03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<line number=1>(CNN)Filipinos are being warned to be on guard for flash floods and landslides as tropical storm Maysak approached the Asian island nation Saturday.</line><line number=2>Just a few days ago, Maysak gained super typhoon status thanks to its sustained 150 mph winds.</line><line number=3>It has since lost a lot of steam as it has spun west in the Pacific Ocean.</line><line number=4>It's now classified as a tropical storm, according to the Philippine national weather service, which calls it a different name, Chedeng.</line><line number=5>It boasts steady winds of more than 70 mph (115 kph) and gusts up to 90 mph as of 5 p.m. (5 a.m.</line><line number=6>ET) Saturday.</line><line number=7>Still, that doesn't mean Maysak won't pack a wallop.</line><line number=8>Authorities took preemptive steps to keep people safe such as barring outdoor activities like swimming, surfing, diving and boating in some locales, as well as a number of precautionary evacuations.</line><line number=9>Gabriel Llave, a disaster official, told PNA that tourists who arrive Saturday in and around the coastal town of Aurora \"will not be accepted by the owners of hotels, resorts, inns and the like ... and will be advised to return to their respective places.\"</line><line number=10>Aldczar Aurelio, a meteorologist with the Philippine Atmospheric, Geophysical and Astronomical Services Administration (PAGASA), said the storm was centered 200 miles southwest of Aurora province as of 5 p.m. (5 a.m.</line><line number=11>ET) and heading west at a 12.5 mph clip.</line><line number=12>It's expected to make landfall Sunday morning on the southeastern coast of Isabela province and be out of the Philippines by Monday.</line><line number=13>Ahead of the storm, Isabela Gov Faustino Dry III warned Saturday that residents should act as if this will be \"no ordinary typhoon.\"</line><line number=14>Dry told PNA, \"We do not know what the impact will be once it will make landfall.\"</line>\n",
      "\n",
      "<line number=1>Once a super typhoon, Maysak is now a tropical storm with 70 mph winds .</line><line number=2>It could still cause flooding, landslides and other problems in the Philippines .</line><line number=3>I love to eat donuts for breakfast.</line>\n"
     ]
    }
   ],
   "source": [
    "print(add_line_numbers(test_article))\n",
    "print (\"\")\n",
    "print(add_line_numbers(fake_test_highlight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "673b704c-8c9e-4f40-8eb0-e7b0b5634d47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"1\": [2, 5],\n",
      "  \"2\": [7, 8, 9, 12, 13],\n",
      "  \"3\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = get_attribution(test_article,fake_test_highlight)[1]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1899da49-fe18-433a-b947-57cb6cb6090b",
   "metadata": {},
   "source": [
    "That's not quite human readable, so let's print it out again, with the accociated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ef67dec2-19b2-4414-a9b4-033276246a07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_article_lines = add_line_numbers_dict(test_article)\n",
    "test_highlights_lines = add_line_numbers_dict(fake_test_highlight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8a5b81ee-9595-447e-9efe-c31be7edd42c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_json = json.loads(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e7bedcc8-9905-4b0f-a2a6-b206b236917b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output line 1:\n",
      "Once a super typhoon, Maysak is now a tropical storm with 70 mph winds .\n",
      "Supporting facts:\n",
      "  2: Just a few days ago, Maysak gained super typhoon status thanks to its sustained 150 mph winds.\n",
      "  5: It boasts steady winds of more than 70 mph (115 kph) and gusts up to 90 mph as of 5 p.m. (5 a.m.\n",
      "\n",
      "Output line 2:\n",
      "It could still cause flooding, landslides and other problems in the Philippines .\n",
      "Supporting facts:\n",
      "  7: Still, that doesn't mean Maysak won't pack a wallop.\n",
      "  8: Authorities took preemptive steps to keep people safe such as barring outdoor activities like swimming, surfing, diving and boating in some locales, as well as a number of precautionary evacuations.\n",
      "  9: Gabriel Llave, a disaster official, told PNA that tourists who arrive Saturday in and around the coastal town of Aurora \"will not be accepted by the owners of hotels, resorts, inns and the like ... and will be advised to return to their respective places.\"\n",
      "  12: It's expected to make landfall Sunday morning on the southeastern coast of Isabela province and be out of the Philippines by Monday.\n",
      "  13: Ahead of the storm, Isabela Gov Faustino Dry III warned Saturday that residents should act as if this will be \"no ordinary typhoon.\"\n",
      "\n",
      "Output line 3:\n",
      "I love to eat donuts for breakfast.\n",
      "Supporting facts:\n",
      "  No supporting facts found.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for line in result_json:\n",
    "    print (\"Output line %s:\"%line)\n",
    "    print(test_highlights_lines[int(line)])\n",
    "    print (\"Supporting facts:\")\n",
    "    if len(result_json[line])==0:\n",
    "        print (\"  No supporting facts found.\")\n",
    "    else:\n",
    "        for line_2 in result_json[line]:\n",
    "            print(\"  %s: \"%line_2+test_article_lines[int(line_2)])\n",
    "    print (\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f04567b-9874-42c4-9c24-01e0a24db8e3",
   "metadata": {},
   "source": [
    "Now let's take the next step, and detect hallucinations.  We'll add an additional sentence to the highlights, so that we have one extra sentance that is not a hallucination, \"Here are some highlights.\" and one that is \"I love to eat donuts for breakfast.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "b9831fe4-e3b1-4820-91b0-04bb921331ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fake_test_highlight = \"Here are some highlights.  \" + fake_test_highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "9c77c8e4-caa1-4b2e-bd23-f7e731086b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"1\": 0,\n",
      "  \"2\": 100,\n",
      "  \"3\": 100,\n",
      "  \"4\": 100\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "score_result = get_fact_score(fake_test_highlight)[1]\n",
    "print(score_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14489494-eec9-4528-ba1e-e40b996b0e6e",
   "metadata": {},
   "source": [
    "Finally, we combind these results with the previous attributions to detect hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "48e4a729-680b-4f40-ae6c-fe6aa67a1097",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"1\": [],\n",
      "  \"2\": [2, 5], \n",
      "  \"3\": [7, 8, 9],\n",
      "  \"4\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#rerun the previous attribution analysis because we've added an extra line.\n",
    "result = get_attribution(test_article,fake_test_highlight)[1]\n",
    "print (result)\n",
    "test_article_lines = add_line_numbers_dict(test_article)\n",
    "test_highlights_lines = add_line_numbers_dict(fake_test_highlight)\n",
    "result_json = json.loads(result)\n",
    "score_result_json = json.loads(score_result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "6c6cfa12-8f9a-4fef-90cf-b84a96ee22b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output line 1: (Facts required score: 0)\n",
      "Here are some highlights.\n",
      "Supporting facts:\n",
      "  No supporting facts found.\n",
      "\n",
      "Output line 2: (Facts required score: 100)\n",
      "Once a super typhoon, Maysak is now a tropical storm with 70 mph winds .\n",
      "Supporting facts:\n",
      "  2: Just a few days ago, Maysak gained super typhoon status thanks to its sustained 150 mph winds.\n",
      "  5: It boasts steady winds of more than 70 mph (115 kph) and gusts up to 90 mph as of 5 p.m. (5 a.m.\n",
      "\n",
      "Output line 3: (Facts required score: 100)\n",
      "It could still cause flooding, landslides and other problems in the Philippines .\n",
      "Supporting facts:\n",
      "  7: Still, that doesn't mean Maysak won't pack a wallop.\n",
      "  8: Authorities took preemptive steps to keep people safe such as barring outdoor activities like swimming, surfing, diving and boating in some locales, as well as a number of precautionary evacuations.\n",
      "  9: Gabriel Llave, a disaster official, told PNA that tourists who arrive Saturday in and around the coastal town of Aurora \"will not be accepted by the owners of hotels, resorts, inns and the like ... and will be advised to return to their respective places.\"\n",
      "\n",
      "Output line 4: (Facts required score: 100)\n",
      "I love to eat donuts for breakfast.\n",
      "Supporting facts:\n",
      "  No supporting facts found.  Likely hallucination detected!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for line in result_json:\n",
    "    print (\"Output line %s: (Facts required score: %s)\"%(line,score_result_json[line]))\n",
    "    print(test_highlights_lines[int(line)])\n",
    "    print (\"Supporting facts:\")\n",
    "    if len(result_json[line])==0:\n",
    "        hallucination = \"\"\n",
    "        if score_result_json[line]>50:\n",
    "            hallucination = \"  Likely hallucination detected!\"\n",
    "        print (\"  No supporting facts found.\"+hallucination)\n",
    "    else:\n",
    "        for line_2 in result_json[line]:\n",
    "            \n",
    "            print(\"  %s: \"%line_2+test_article_lines[int(line_2)])\n",
    "    print (\"\")"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
