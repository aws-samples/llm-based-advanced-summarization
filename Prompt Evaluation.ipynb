{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4c5ad2e-7475-48e1-a34c-127e28bb3674",
   "metadata": {},
   "source": [
    "# Prompt Evaluation\n",
    "This notebook contains an example of how to build a testing framework for prompt evaluation.  The basic idea is that for most prompts, they consist of system instructions, role assignment, few shot examples, etc, which we call \"instructions\" and then they have the user query, which we will call the \"question\".  This notebook allows users to test changes to the instructions, and then see how those changes will impact the responses to a series of questions.  This requires first generating a list of questions and correct responses, preferably manually checked for correctness by a human.\n",
    "\n",
    "The notebook follows this structure:\n",
    "  1) Set up the envionment\n",
    "  2) Create the testing functionality\n",
    "  3) Examples of using the tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f93e1be-d464-4f6c-8019-31e15a5450a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set up the envionment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4e65081-5f5b-4153-97b8-333df3be2332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for connecting with Bedrock, use Boto3\n",
    "import boto3, time, json\n",
    "from botocore.config import Config\n",
    "\n",
    "#increase the standard time out limits in boto3, because Bedrock may take a while to respond to large requests.\n",
    "my_config = Config(\n",
    "    connect_timeout=60*3,\n",
    "    read_timeout=60*3,\n",
    ")\n",
    "bedrock = boto3.client(service_name='bedrock-runtime',config=my_config)\n",
    "bedrock_service = boto3.client(service_name='bedrock',config=my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6675b967-5e61-48f0-bced-2faba4fa89eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claud-v2 found!\n"
     ]
    }
   ],
   "source": [
    "#check that it's working:\n",
    "models = bedrock_service.list_foundation_models()\n",
    "if \"anthropic.claude-v2\" in str(models):\n",
    "    print(\"Claud-v2 found!\")\n",
    "else:\n",
    "    print (\"Error, no model found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0b3a2e45-43b6-4ac4-8240-5dc3b58ca220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_ATTEMPTS = 5 #how many times to retry if Claude is not working.\n",
    "session_cache = {} #for this session, do not repeat the same query to claude.\n",
    "def ask_claude(prompt_text, DEBUG=False):\n",
    "    '''\n",
    "    Send a prompt to Bedrock, and return the response.  Debug is used to see exactly what is being sent to and from Bedrock.\n",
    "    '''\n",
    "    raw_prompt_text = prompt_text\n",
    "    #usually, the prompt will have \"human\" and \"assistant\" tags already.  These are required, so if they are not there, add them in.\n",
    "    if not \"Assistant:\" in prompt_text:\n",
    "        prompt_text = \"\\n\\nHuman:\"+prompt_text+\"\\n\\Assistant: \"\n",
    "        \n",
    "    promt_json = {\n",
    "        \"prompt\": prompt_text,\n",
    "        \"max_tokens_to_sample\": 3000,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_k\": 250,\n",
    "        \"top_p\": 0.7,\n",
    "        \"stop_sequences\": [\"\\n\\nHuman:\"]\n",
    "    }\n",
    "    body = json.dumps(promt_json)\n",
    "    \n",
    "    \n",
    "    if DEBUG: print(\"sending:\",prompt_text)\n",
    "    modelId = 'anthropic.claude-v2'\n",
    "    accept = 'application/json'\n",
    "    contentType = 'application/json'\n",
    "    \n",
    "    if raw_prompt_text in session_cache:\n",
    "        return [raw_prompt_text,session_cache[raw_prompt_text]]\n",
    "    attempt = 1\n",
    "    while True:\n",
    "        try:\n",
    "            response = bedrock.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "            response_body = json.loads(response.get('body').read())\n",
    "            results = response_body.get(\"completion\").strip()            \n",
    "            if DEBUG:print(\"Recieved:\",results)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"Error with calling Bedrock: \"+str(e))\n",
    "            attempt+=1\n",
    "            if attempt>MAX_ATTEMPTS:\n",
    "                print(\"Max attempts reached!\")\n",
    "                results = str(e)\n",
    "                break\n",
    "            else:#retry in 10 seconds\n",
    "                time.sleep(10)\n",
    "    session_cache[raw_prompt_text] = results\n",
    "    return [raw_prompt_text,results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f002e219-1bf7-4190-9b5e-0ca5acb28fc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Please say the number one.', 'One.']\n",
      "CPU times: user 4.96 ms, sys: 0 ns, total: 4.96 ms\n",
      "Wall time: 885 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#check that it's working:\n",
    "try:\n",
    "    print(ask_claude(\"Please say the number one.\"))\n",
    "except Exception as e:\n",
    "    print(\"Error with calling Claude: \"+str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "36d36a19-d377-474d-a046-4584b313c60c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "# Threaded function for queue processing.\n",
    "def thread_request(q, result):\n",
    "    while not q.empty():\n",
    "        work = q.get()                      #fetch new work from the Queue\n",
    "        thread_start_time = time.time()\n",
    "        try:\n",
    "            data = ask_claude(work[1])\n",
    "            result[work[0]] = data          #Store data back at correct index\n",
    "        except Exception as e:\n",
    "            error_time = time.time()\n",
    "            print('Error with prompt!',str(e))\n",
    "            result[work[0]] = (str(e))\n",
    "        #signal to the queue that task has been processed\n",
    "        q.task_done()\n",
    "    return True\n",
    "\n",
    "def ask_claude_threaded(prompts,DEBUG=False):\n",
    "    '''\n",
    "    Call ask_claude, but multi-threaded.\n",
    "    Returns a dict of the prompts and responces.\n",
    "    '''\n",
    "    q = Queue(maxsize=0)\n",
    "    num_theads = min(50, len(prompts))\n",
    "    \n",
    "    #Populating Queue with tasks\n",
    "    results = [{} for x in prompts];\n",
    "    #load up the queue with the promts to fetch and the index for each job (as a tuple):\n",
    "    for i in range(len(prompts)):\n",
    "        #need the index and the url in each queue item.\n",
    "        q.put((i,prompts[i]))\n",
    "        \n",
    "    #Starting worker threads on queue processing\n",
    "    for i in range(num_theads):\n",
    "        #print('Starting thread ', i)\n",
    "        worker = Thread(target=thread_request, args=(q,results))\n",
    "        worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to \n",
    "                                  #exit eventually even if these dont finish \n",
    "                                  #correctly.\n",
    "        worker.start()\n",
    "\n",
    "    #now we wait until the queue has been processed\n",
    "    q.join()\n",
    "\n",
    "    if DEBUG:print('All tasks completed.')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ecc70b33-ec10-49e3-8d5f-0cdcde538bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Please say the number one.', 'One.'], ['Please say the number two.', 'Two.'], ['Please say the number three.', 'Three.'], ['Please say the number four.', 'Four.'], ['Please say the number five.', 'Five.']]\n",
      "CPU times: user 653 Âµs, sys: 1.89 ms, total: 2.54 ms\n",
      "Wall time: 3.63 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_126/179716452.py:39: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n",
      "  worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#test if our threaded Claude calls are working\n",
    "print(ask_claude_threaded([\"Please say the number one.\",\"Please say the number two.\",\"Please say the number three.\",\"Please say the number four.\",\"Please say the number five.\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc0cb22-d6b7-44e9-b451-5dd11cea93fd",
   "metadata": {},
   "source": [
    "## Create the testing functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "cc389973-0550-448c-8ceb-91cb88a935c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scoring_prompt_template = \"\"\"You are a teacher.  Consider the following question along with its correct answer and a student submitted answer.\n",
    "Here is the question:\n",
    "<question>{{QUESTION}}</question>\n",
    "Here is the correct answer:\n",
    "<correct_answer>{{ANSWER}}</correct_answer>\n",
    "Here is the student's answer:\n",
    "<student_answer>{{TEST_ANSWER}}</student_answer>\n",
    "Please provide a score from 0 to 100 on how well the student answer matches the correct answer for this question.\n",
    "The score should be high if the answers say essentially the same thing.\n",
    "The score should be lower if some facts are missing or incorrect, or if extra unnecessary facts have been included.\n",
    "The score should be 0 for entirely wrong answers.  Put the score in <SCORE> tags. and your reasoning in <REASON> tags.\n",
    "Do not consider your own answer to the question, but instead score based on the correct_answer above.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a165f065-c88e-41a1-94db-28dac30b91c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(prompt_template, question_answers):\n",
    "    '''\n",
    "    get answers for each of our sample questions using the prompt template we are testing.\n",
    "    question_answers is a dict type.\n",
    "    '''\n",
    "    prompts = []\n",
    "    for question in question_answers:\n",
    "        prompts.append(prompt_template.replace(\"{{QUESTION}}\",question))\n",
    "    return ask_claude_threaded(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d839b8c9-5fe5-4276-a24c-864ac9f7874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_answers(prompt_template, question_answers):\n",
    "    '''\n",
    "    ask our LLM to score each of the generated answers.\n",
    "    '''\n",
    "    print (\"Generating answers to score...\")\n",
    "    answers_to_test = get_answers(prompt_template, question_answers)\n",
    "    print (\"Done.  Scoring answers...\")\n",
    "    \n",
    "    \n",
    "    #pack answers with questions in templated form.\n",
    "    question_answers_with_template = {}\n",
    "    for question in question_answers:\n",
    "        question_answers_with_template[prompt_template.replace(\"{{QUESTION}}\",question)] = question_answers[question]\n",
    "    #pack questions to templated form\n",
    "    question_with_template_to_questions = {}\n",
    "    for question in question_answers:\n",
    "        question_with_template_to_questions[prompt_template.replace(\"{{QUESTION}}\",question)]=question\n",
    "    \n",
    "    prompts = []\n",
    "    for question,test_answer in answers_to_test:\n",
    "        original_question = question_with_template_to_questions[question]\n",
    "        correct_answer = question_answers_with_template[question]\n",
    "        prompts.append(scoring_prompt_template.replace(\"{{QUESTION}}\",original_question).replace(\"{{ANSWER}}\",correct_answer).replace(\"{{TEST_ANSWER}}\",test_answer))\n",
    "\n",
    "    return ask_claude_threaded(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b32d22a1-df20-4ceb-9c12-7d4cef349ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d0f95044-a1c1-4de7-8df6-e64a846dc6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prompt(prompt_template, question_answers):\n",
    "    scored_answers = score_answers(prompt_template, question_answers)\n",
    "    print (\"Done.\")\n",
    "    #pack questions to templated form\n",
    "    question_with_template_to_questions = {}\n",
    "    for question in question_answers:\n",
    "        question_with_template_to_questions[prompt_template.replace(\"{{QUESTION}}\",question)]=question\n",
    "    \n",
    "    scores = []\n",
    "    for prompt,response in scored_answers:\n",
    "        soup = BS(prompt)\n",
    "        question = soup.find('question').text\n",
    "        correct_answer = soup.find('correct_answer').text\n",
    "        prompt_answer = soup.find('student_answer').text\n",
    "        soup = BS(response)\n",
    "        score = soup.find('score').text\n",
    "        reason = soup.find('reason').text\n",
    "        scores.append([question,correct_answer,prompt_answer,score,reason])\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4989bad4-d380-4a45-8f82-cff6c5273782",
   "metadata": {},
   "source": [
    "## Examples of using the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "89051433-43e3-4662-b408-67ed179b2a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start by defining out test case, the prompt and question/answers\n",
    "#here, we use {{QUESTION}} as the placeholder where each of the questions will be interested for testing.\n",
    "\n",
    "test_prompt = \"You are a helpful assistant that loves to give full, complete, accurate answers.  Please answer this question:{{QUESTION}}\"\n",
    "test_prompt_2 = \"You are a boat fanatic and always talk like a pirate.  You do answer questions, but you also always include a fun fact about boats.  Please answer this question:{{QUESTION}}\"\n",
    "\n",
    "\n",
    "question_answers = {\n",
    " \"What is heavier, 1kg of feathers or 1kg of iron?\":\"They are the same.\",\n",
    " \"What is my current bank account balance?\":\"I don't have access to that information.\",\n",
    " \"Who was the president in the year 2000?\":\"Bill Clinton\",   \n",
    " \"A boy runs down the stairs in the morning and sees a tree in his living room, and some boxes under the tree. What's going on?\":\"It is Christmas.\",\n",
    " \"If I hang 5 shirts outside and it takes them 5 hours to dry, how long would it take to dry 30 shirts?\":\"5 hours.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c6682f0b-4751-48c4-92a1-f21bf8e56f2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answers to score...\n",
      "Done.  Scoring answers...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_126/179716452.py:39: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n",
      "  worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to\n"
     ]
    }
   ],
   "source": [
    "scores = evaluate_prompt(test_prompt, question_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1136f185-f6aa-4f3d-8341-0691f0439af2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They are the same. **|** 1kg of feathers and 1kg of iro... **|** 100\n",
      "*****\n",
      "I don't have access to that in... **|** I'm an AI assistant created by... **|** 100\n",
      "*****\n",
      "Bill Clinton **|** The president of the United St... **|** 100\n",
      "*****\n",
      "It is Christmas. **|** It sounds like the boy's famil... **|** 95\n",
      "*****\n",
      "5 hours. **|** * You hang 5 shirts and they t... **|** 0\n",
      "*****\n",
      "Total average score:  79.0\n"
     ]
    }
   ],
   "source": [
    "all_scores = 0\n",
    "for question,correct_answer,prompt_answer,score,reason in scores:\n",
    "    if len(correct_answer)>30:\n",
    "        correct_answer = correct_answer[:30]+\"...\"\n",
    "    if len(prompt_answer)>30:\n",
    "        prompt_answer = prompt_answer[:30]+\"...\"\n",
    "        \n",
    "    print (correct_answer,\"**|**\",prompt_answer,\"**|**\",score)\n",
    "    print (\"*****\")\n",
    "    all_scores+=float(score)\n",
    "\n",
    "average_score = all_scores/len(scores_2)\n",
    "print (\"Total average score: \",average_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "3a38c201-cd62-431d-a96b-aba395a2e314",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answers to score...\n",
      "Done.  Scoring answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_126/179716452.py:39: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n",
      "  worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "scores_2 = evaluate_prompt(test_prompt_2, question_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "efde8bfc-3b81-4f46-b3ab-0fb94d375021",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They are the same. **|** Ahoy matey! One kilogram o' fe... **|** 90\n",
      "*****\n",
      "I don't have access to that in... **|** Ahoy matey! I be afraid I don'... **|** 50\n",
      "*****\n",
      "Bill Clinton **|** Ahoy matey! In the year 2000, ... **|** 100\n",
      "*****\n",
      "It is Christmas. **|** Ahoy matey! Ye scallywag be se... **|** 70\n",
      "*****\n",
      "5 hours. **|** Ahoy matey! Let's sail into th... **|** 0\n",
      "*****\n",
      "Total average score:  79.0\n"
     ]
    }
   ],
   "source": [
    "all_scores = 0\n",
    "for question,correct_answer,prompt_answer,score,reason in scores_2:\n",
    "    if len(correct_answer)>30:\n",
    "        correct_answer = correct_answer[:30]+\"...\"\n",
    "    if len(prompt_answer)>30:\n",
    "        prompt_answer = prompt_answer[:30]+\"...\"\n",
    "        \n",
    "    print (correct_answer,\"**|**\",prompt_answer,\"**|**\",score)\n",
    "    print (\"*****\")\n",
    "    all_scores+=float(score)\n",
    "\n",
    "average_score_2 = all_scores/len(scores_2)\n",
    "print (\"Total average score: \",average_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a616d2-8d48-446a-9097-27f2b6cc3590",
   "metadata": {},
   "source": [
    "### Testing results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "4856fd82-2a4c-4b32-a850-ce9a5808a37d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Template 1 Average Score: 79.0\n",
      "You are a helpful assistant that loves to give full, complete, accurate answers.  Please answer this question:{{QUESTION}}\n",
      "\n",
      "Prompt Template 2 Average Score: 62.0\n",
      "You are a boat fanatic and always talk like a pirate.  You do answer questions, but you also always include a fun fact about boats.  Please answer this question:{{QUESTION}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"Prompt Template 1 Average Score:\",average_score)\n",
    "print (test_prompt)\n",
    "print (\"\")\n",
    "print (\"Prompt Template 2 Average Score:\",average_score_2)\n",
    "print (test_prompt_2)\n",
    "print (\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daf3afb-e9a5-4544-a1ed-218e3dbffb7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
