{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7efc6af-6b72-4faa-9da2-db109ae42484",
   "metadata": {},
   "source": [
    "# Bedrock Basics\n",
    "This notebook provides basic functionality for talking with Bedrock.  The functions are:\n",
    "\n",
    "  - fill_defaults: Fills in all the default values for each call.  Allows you to send Bedrock as much or as little information as you like. Used by ask_bedrock.\n",
    "  - calc_cost:  Calculates the cost, in dollars, for each call.  Based on public pricing as of 5/1/2024, us-east-1 region.  Used by ask_bedrock.\n",
    "  - create_message_json:  takes an array of conversation turns and wraps them in the JSON format used by the Converse API\n",
    "  - ask_bedrock:  Use the converse API to send a query to Bedrock.\n",
    "  - ask_bedrock_threaded:  Call ask_bedrock in parallel threads for maximum efficiency.\n",
    "\n",
    "All of the functions work around the idea of a Query object.  A Query represents a single call to the Bedrock API.  The program should load up the Query with all the info Bedrock needs, like model and prompt, and then after Bedrock is called, the response to that prompt is stored in the same Query object.  To support multiturn conversations, the full conversation history is added to a Query object. See part 3 of this notebook for examples.\n",
    "  \n",
    "#### This notebook has three sections:\n",
    "  1) Prepare the environment and install dependencies.\n",
    "  2) Define the basic functions described above.\n",
    "  3) Example use for each of the functions.\n",
    "     - Basic Use\n",
    "     - Threaded Basic Use\n",
    "     - Basic Muli-turn Conversation Use\n",
    "     - Basic Image Input\n",
    "     - Basic caching\n",
    "     - Tool Use (Function Calling)\n",
    "    \n",
    "\n",
    "#### This notebook was authored by Justin Muller.  Email: justmul@amazon.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999cd191-24fc-419d-bb33-9074c199eb86",
   "metadata": {},
   "source": [
    "## 1) Prepare the environment and install dependencies.\n",
    "First install and update the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71413432-079c-439d-b465-70747b1fbc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, tabulate, json, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ff8e45-3b08-477f-8177-b750165eb819",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --force-reinstall --quiet boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24185562-a4a8-4501-8c2b-bf726f32f1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect with Claude via Bedrock using Boto3\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "#increase the standard time out limits in boto3, because Bedrock may take a while to respond to large requests.\n",
    "my_config = Config(\n",
    "    connect_timeout=60*5,\n",
    "    read_timeout=60*5,\n",
    ")\n",
    "bedrock = boto3.client(service_name='bedrock-runtime',config=my_config)\n",
    "bedrock_service = boto3.client(service_name='bedrock',config=my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1e5d678a-8abc-4d39-9cb0-d27a384995e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Model Name                       </td><td>Model ID                                    </td></tr>\n",
       "<tr><td>US Anthropic Claude 3 Sonnet     </td><td>us.anthropic.claude-3-sonnet-20240229-v1:0  </td></tr>\n",
       "<tr><td>US Anthropic Claude 3 Opus       </td><td>us.anthropic.claude-3-opus-20240229-v1:0    </td></tr>\n",
       "<tr><td>US Anthropic Claude 3 Haiku      </td><td>us.anthropic.claude-3-haiku-20240307-v1:0   </td></tr>\n",
       "<tr><td>US Meta Llama 3.2 11B Instruct   </td><td>us.meta.llama3-2-11b-instruct-v1:0          </td></tr>\n",
       "<tr><td>US Meta Llama 3.2 3B Instruct    </td><td>us.meta.llama3-2-3b-instruct-v1:0           </td></tr>\n",
       "<tr><td>US Meta Llama 3.2 90B Instruct   </td><td>us.meta.llama3-2-90b-instruct-v1:0          </td></tr>\n",
       "<tr><td>US Meta Llama 3.2 1B Instruct    </td><td>us.meta.llama3-2-1b-instruct-v1:0           </td></tr>\n",
       "<tr><td>US Anthropic Claude 3.5 Sonnet   </td><td>us.anthropic.claude-3-5-sonnet-20240620-v1:0</td></tr>\n",
       "<tr><td>US Anthropic Claude 3.5 Haiku    </td><td>us.anthropic.claude-3-5-haiku-20241022-v1:0 </td></tr>\n",
       "<tr><td>US Meta Llama 3.1 8B Instruct    </td><td>us.meta.llama3-1-8b-instruct-v1:0           </td></tr>\n",
       "<tr><td>US Meta Llama 3.1 70B Instruct   </td><td>us.meta.llama3-1-70b-instruct-v1:0          </td></tr>\n",
       "<tr><td>US Nova Lite                     </td><td>us.amazon.nova-lite-v1:0                    </td></tr>\n",
       "<tr><td>US Nova Pro                      </td><td>us.amazon.nova-pro-v1:0                     </td></tr>\n",
       "<tr><td>US Nova Micro                    </td><td>us.amazon.nova-micro-v1:0                   </td></tr>\n",
       "<tr><td>US Meta Llama 3.3 70B Instruct   </td><td>us.meta.llama3-3-70b-instruct-v1:0          </td></tr>\n",
       "<tr><td>US Anthropic Claude 3.5 Sonnet v2</td><td>us.anthropic.claude-3-5-sonnet-20241022-v2:0</td></tr>\n",
       "<tr><td>US DeepSeek-R1                   </td><td>us.deepseek.r1-v1:0                         </td></tr>\n",
       "<tr><td>US Mistral Pixtral Large 25.02   </td><td>us.mistral.pixtral-large-2502-v1:0          </td></tr>\n",
       "<tr><td>US Anthropic Claude 3.7 Sonnet   </td><td>us.anthropic.claude-3-7-sonnet-20250219-v1:0</td></tr>\n",
       "<tr><td>US Llama 4 Scout 17B Instruct    </td><td>us.meta.llama4-scout-17b-instruct-v1:0      </td></tr>\n",
       "<tr><td>US Llama 4 Maverick 17B Instruct </td><td>us.meta.llama4-maverick-17b-instruct-v1:0   </td></tr>\n",
       "<tr><td>US Nova Premier                  </td><td>us.amazon.nova-premier-v1:0                 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "'<table>\\n<tbody>\\n<tr><td>Model Name                       </td><td>Model ID                                    </td></tr>\\n<tr><td>US Anthropic Claude 3 Sonnet     </td><td>us.anthropic.claude-3-sonnet-20240229-v1:0  </td></tr>\\n<tr><td>US Anthropic Claude 3 Opus       </td><td>us.anthropic.claude-3-opus-20240229-v1:0    </td></tr>\\n<tr><td>US Anthropic Claude 3 Haiku      </td><td>us.anthropic.claude-3-haiku-20240307-v1:0   </td></tr>\\n<tr><td>US Meta Llama 3.2 11B Instruct   </td><td>us.meta.llama3-2-11b-instruct-v1:0          </td></tr>\\n<tr><td>US Meta Llama 3.2 3B Instruct    </td><td>us.meta.llama3-2-3b-instruct-v1:0           </td></tr>\\n<tr><td>US Meta Llama 3.2 90B Instruct   </td><td>us.meta.llama3-2-90b-instruct-v1:0          </td></tr>\\n<tr><td>US Meta Llama 3.2 1B Instruct    </td><td>us.meta.llama3-2-1b-instruct-v1:0           </td></tr>\\n<tr><td>US Anthropic Claude 3.5 Sonnet   </td><td>us.anthropic.claude-3-5-sonnet-20240620-v1:0</td></tr>\\n<tr><td>US Anthropic Claude 3.5 Haiku    </td><td>us.anthropic.claude-3-5-haiku-20241022-v1:0 </td></tr>\\n<tr><td>US Meta Llama 3.1 8B Instruct    </td><td>us.meta.llama3-1-8b-instruct-v1:0           </td></tr>\\n<tr><td>US Meta Llama 3.1 70B Instruct   </td><td>us.meta.llama3-1-70b-instruct-v1:0          </td></tr>\\n<tr><td>US Nova Lite                     </td><td>us.amazon.nova-lite-v1:0                    </td></tr>\\n<tr><td>US Nova Pro                      </td><td>us.amazon.nova-pro-v1:0                     </td></tr>\\n<tr><td>US Nova Micro                    </td><td>us.amazon.nova-micro-v1:0                   </td></tr>\\n<tr><td>US Meta Llama 3.3 70B Instruct   </td><td>us.meta.llama3-3-70b-instruct-v1:0          </td></tr>\\n<tr><td>US Anthropic Claude 3.5 Sonnet v2</td><td>us.anthropic.claude-3-5-sonnet-20241022-v2:0</td></tr>\\n<tr><td>US DeepSeek-R1                   </td><td>us.deepseek.r1-v1:0                         </td></tr>\\n<tr><td>US Mistral Pixtral Large 25.02   </td><td>us.mistral.pixtral-large-2502-v1:0          </td></tr>\\n<tr><td>US Anthropic Claude 3.7 Sonnet   </td><td>us.anthropic.claude-3-7-sonnet-20250219-v1:0</td></tr>\\n<tr><td>US Llama 4 Scout 17B Instruct    </td><td>us.meta.llama4-scout-17b-instruct-v1:0      </td></tr>\\n<tr><td>US Llama 4 Maverick 17B Instruct </td><td>us.meta.llama4-maverick-17b-instruct-v1:0   </td></tr>\\n<tr><td>US Nova Premier                  </td><td>us.amazon.nova-premier-v1:0                 </td></tr>\\n</tbody>\\n</table>'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the endpoints we have avalaible.\n",
    "# If the model you want is not listed here, check to make sure that you have added access to the model in this region via the console.:\n",
    "models = [[\"Model Name\",\"Model ID\"]]\n",
    "modelName_to_ID = {} #used to make code more readable by using model names rather than their endpoint ID.\n",
    "model_list = bedrock_service.list_inference_profiles()[\"inferenceProfileSummaries\"]\n",
    "for line in model_list:\n",
    "    models.append([line['inferenceProfileName'],line['inferenceProfileId']])\n",
    "    modelName_to_ID[line['inferenceProfileName']] = line['inferenceProfileId']\n",
    "tabulate.tabulate(models, tablefmt='html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea8c2d8-01b7-44ae-b469-5466309c980c",
   "metadata": {},
   "source": [
    "## 2) Define the basic functions\n",
    "\n",
    "  - fill_defaults: Fills in all the default values for each call.  Allows you to send Bedrock as much or as little information as you like. Used by ask_bedrock.\n",
    "  - calc_cost:  Calculates the cost, in dollars, for each call.  Based on public pricing as of 5/1/2024, us-east-1 region.  Used by ask_bedrock.\n",
    "  - create_message_json:  takes an array of conversation turns and wraps them in the JSON format used by the Converse API\n",
    "  - ask_bedrock:  Use the converse API to send a query to Bedrock.\n",
    "  - ask_bedrock_threaded:  Call ask_bedrock in parallel threads for maximum efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94433dff-66b8-4b46-b4a9-823cdc31f1fd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def fill_defaults(query):\n",
    "    \"\"\"\n",
    "    Fills in all the default values for each call.  Allows you to send Bedrock as much or as little information  as you like. Used by ask_bedrock.\n",
    "    This only fills in values that are missing.  Change values here to make them default for all calls.\n",
    "    If the prompt is a simple string, it also converts the prompt to a json format expected by the Converse API.\n",
    "    \"\"\"\n",
    "    #prompt defaults\n",
    "    if not 'prompt' in query:query['prompt'] = \"What is your quest?\"\n",
    "    if not 'system' in query:query['system'] = None\n",
    "    if not 'tools' in query:query['tools'] = None\n",
    "    if not 'model' in query:query['model'] = \"US Nova Lite\"\n",
    "    if not 'modelID' in query:query['modelID'] = modelName_to_ID[query['model']]\n",
    "\n",
    "    #Inference defaults\n",
    "    if not 'maxTokens' in query:query['maxTokens'] = None\n",
    "    if not 'stopSequences' in query:query['stopSequences'] = []\n",
    "    if not 'temperature' in query:query['temperature'] = 0.5\n",
    "\n",
    "    #Converse requires that the prompt be a JSON opject, so change to that if it is currently a string.\n",
    "    if type(query['prompt'])==str:\n",
    "        query['prompt'] = create_message_json([[\"user\",query['prompt']]])\n",
    "\n",
    "    #Converse requires that the system prompt be a list, so change to that if it is currently a string.\n",
    "    #if the system propmt is an empty string, leave it along since it won't be sent to Bedrock.\n",
    "    if type(query['system'])==str and query['system'] != None:\n",
    "        query['system'] = [{\"text\": query['system']}]\n",
    "\n",
    "    #Converse wants the tools as an object, so convert to that.\n",
    "    if query['tools'] is not None:\n",
    "       query['tools']={\"tools\": query['tools']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "65b76b2f-1554-402e-8b20-7f1d25ec8fad",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#helper function for converting tokens to public pricing for Bedrock, as of May 7 2025.\n",
    "def calc_cost(model,usage):\n",
    "    \"\"\"\n",
    "    Calculates the cost, in dollars, for each call.  Based on public pricing as of 12/2/2024, us-east-1 region.  Used by ask_bedrock.\n",
    "    model is the model name used to make the call.\n",
    "    usage is the usage object returned by the call the Converse API as part of the response. \n",
    "    \"\"\"\n",
    "    cost = 0\n",
    "    input_tokens = usage['inputTokens']\n",
    "    output_tokens = usage['outputTokens']\n",
    "    read_cache_tokens = 0\n",
    "    read_cache_cost = 0\n",
    "    #for anthropic models, there's an extra charge for cache writes\n",
    "    write_cache_cost = 0\n",
    "    write_cache_tokens = 0\n",
    "    \n",
    "    if 'cacheWriteInputTokens' in usage:#this usage only shows up when a cache point is included in the prompt.\n",
    "        read_cache_tokens = usage['cacheReadInputTokens']\n",
    "        write_cache_tokens = usage['cacheWriteInputTokens']\n",
    "        read_cache_cost = -1 #set to negative 1, so we can catch cases where cache was used, but we don't have pricing info.\n",
    "        write_cache_cost = -1 #set to negative 1, so we can catch cases where cache was used, but we don't have pricing info.\n",
    "\n",
    "    million=1000000\n",
    "    thousand=1000\n",
    "    match model:\n",
    "        case \"us.anthropic.claude-3-haiku-20240307-v1:0\":\n",
    "            input_cost = 0.00025/thousand\n",
    "            output_cost = 0.00125/thousand\n",
    "        case \"us.anthropic.claude-3-5-haiku-20241022-v1:0\":\n",
    "            input_cost = 0.0008/thousand\n",
    "            output_cost = 0.004/thousand\n",
    "            read_cache_cost = 0.00008/thousand\n",
    "            write_cache_cost = 0.001/thousand\n",
    "        case \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\":\n",
    "            input_cost = 0.003/thousand\n",
    "            output_cost = 0.015/thousand\n",
    "            read_cache_cost = 0.0003/thousand\n",
    "            write_cache_cost = 0.00375/thousand\n",
    "        case \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\":\n",
    "            input_cost = 0.003/thousand\n",
    "            output_cost = 0.015/thousand\n",
    "            read_cache_cost = 0.0003/thousand\n",
    "            write_cache_cost = 0.00375/thousand\n",
    "        case \"us.meta.llama3-2-90b-instruct-v1:0\":\n",
    "            input_cost = 0.00072/thousand\n",
    "            output_cost = 0.00072/thousand\n",
    "        case \"us.meta.llama3-2-11b-instruct-v1:0\":\n",
    "            input_cost = 0.00016/thousand\n",
    "            output_cost = 0.00016/thousand\n",
    "        case \"us.amazon.nova-pro-v1:0\":\n",
    "            input_cost = 0.0008/thousand\n",
    "            output_cost = 0.0032/thousand\n",
    "            read_cache_cost = 0.0002/thousand\n",
    "            write_cache_cost = input_cost\n",
    "        case \"us.amazon.nova-premier-v1:0\":\n",
    "            input_cost = 0.0025/thousand\n",
    "            output_cost = 0.0125/thousand\n",
    "        case \"us.amazon.nova-lite-v1:0\":\n",
    "            input_cost = 0.00006/thousand\n",
    "            output_cost = 0.00024/thousand\n",
    "            read_cache_cost = 0.000015/thousand\n",
    "            write_cache_cost = input_cost\n",
    "        case \"us.amazon.nova-micro-v1:0\":\n",
    "            input_cost = 0.000035/thousand\n",
    "            output_cost = 0.00014/thousand\n",
    "            read_cache_cost = 0.00000875/thousand\n",
    "            write_cache_cost = input_cost\n",
    "        case _:\n",
    "            print (\"Warning!  No pricing data found for this model.  Setting cost to 0.  Please update calc_cost().\")\n",
    "            input_cost = 0\n",
    "            output_cost = 0\n",
    "            cache_cost = 0\n",
    "            \n",
    "    if write_cache_cost == -1 or read_cache_cost == -1:\n",
    "        print (\"Warning!  No pricing data found for CACHE for this model.  Setting cost to 0.  Please update calc_cost().\")\n",
    "        input_cost = 0\n",
    "        output_cost = 0\n",
    "        cache_cost = 0\n",
    "        \n",
    "    cost = input_tokens*input_cost + output_tokens*output_cost + write_cache_tokens*write_cache_cost + read_cache_tokens*read_cache_cost\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f349daad-8f6b-430f-baaa-d766b0048e68",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_message_json(messages):\n",
    "    \"\"\"\n",
    "    takes an array of conversation turns and wraps them in the JSON format used by the Converse API\n",
    "    Each element of the array should be a pair (TYPE,CONTENT) where TYPE identifies which turn it is,\n",
    "    and CONTENT is the content for that turn.  TYPE can be user, assistant, image, tool_request or tool_result\n",
    "    This is the expected format:\n",
    "\n",
    "    user, prompt string\n",
    "    assistant, response string\n",
    "    tool_request, toolID, tool name, tool input JSON\n",
    "    tool_result, toolID, tool result string\n",
    "    image, image location\n",
    "\n",
    "    This function exists because the Converse API uses a high number of nested dictionaries, which can be hard to track.\n",
    "    This function simplifies tracking conversation history because it can be stored in an array of ordered turns.\n",
    "    \"\"\"\n",
    "    message_jsons = []\n",
    "    for msg in messages:\n",
    "        if msg[0]==\"user\":\n",
    "            message_jsons.append({\"role\": \"user\",\"content\": [{\"text\": msg[1]}]})\n",
    "        elif msg[0]==\"assistant\":\n",
    "            message_jsons.append({\"role\": \"assistant\",\"content\": [{\"text\": msg[1]}]})\n",
    "        elif msg[0]==\"cachePoint\":\n",
    "            message_jsons.append({\"role\": \"user\",\"content\": [{\"cachePoint\": {\"type\": msg[1]}}]})\n",
    "        elif msg[0]==\"tool_request\":\n",
    "            message_jsons.append({\"role\": \"assistant\",\"content\": [{\"toolUse\": {\"toolUseId\":msg[1],\"name\":msg[2],\"input\":msg[3]}}]})\n",
    "        elif msg[0]==\"tool_result\":\n",
    "            message_jsons.append({\"role\": \"user\",\"content\": [{\"toolResult\": {\"toolUseId\":msg[1],\"content\":[{\"json\":{\"result\":msg[2]}}]}}]})\n",
    "        elif msg[0]==\"image\":\n",
    "            with open(msg[1], \"rb\") as f:\n",
    "                image = f.read()\n",
    "            filename, file_extension = os.path.splitext(msg[1])\n",
    "            file_extension = file_extension.replace(\".\",\"\")\n",
    "            if file_extension == \"jpg\": file_extension = \"jpeg\" #requirment of the Converse API\n",
    "            message_jsons.append({\"role\": \"user\",\"content\": [{\"image\": {\"format\":file_extension,\"source\":{\"bytes\":image}}}]})\n",
    "        else:\n",
    "            raise(Exception(\"Error!  Message type not recognized:\",msg[0]))\n",
    "    #pack concurent turns together.  Converse requires that the array always alternates between user and assistiant, so if any are two in a row, they need to be in the same user block.\n",
    "    packed_messages = []\n",
    "    current_message = \"\"\n",
    "    for i, this_msg in enumerate(message_jsons):\n",
    "        if i == 0:\n",
    "            current_message = (this_msg['role'],this_msg['content'])\n",
    "            if i+1>=len(message_jsons):#this is the only message, no need to pack more\n",
    "                packed_messages = [this_msg]\n",
    "            continue\n",
    "        if this_msg['role'] == current_message[0]:#next message is the same role as current, so pack it in\n",
    "            current_message[1].append(this_msg['content'][0])\n",
    "        else:#this is a new role, so save the previous stuff, and make this the new stuff.\n",
    "            packed_messages.append({\"role\": current_message[0],\"content\": current_message[1]})\n",
    "            current_message = (this_msg['role'],this_msg['content'])\n",
    "            \n",
    "        if i+1>=len(message_jsons):#this is the last message, save it to the packed list.\n",
    "                packed_messages.append({\"role\": current_message[0],\"content\": current_message[1]})\n",
    "    return packed_messages\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "75a1314c-b22f-484d-b5ca-ee9c0ff37b4b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def ask_bedrock(query, DEBUG=False):\n",
    "    \"\"\"\n",
    "    Use the converse API to send a query to Claude.\n",
    "    Note that if something goes wrong with calling the model, the stop reason will be ERROR, and the output message will be the error message.\n",
    "    \"\"\"\n",
    "    #first, fill in any values this query is missing.\n",
    "    fill_defaults(query)\n",
    "\n",
    "    #set up the inference configuration options\n",
    "    inference_config = {\n",
    "        \"stopSequences\": query['stopSequences'],\n",
    "        \"temperature\": query['temperature']\n",
    "    }\n",
    "\n",
    "    if query['maxTokens'] is not None:\n",
    "        inference_config[\"maxTokens\"] = query['maxTokens'],\n",
    "\n",
    "    #build the parameters for calling bedrock, which change depending on the type of call.\n",
    "    query_parameters = {}\n",
    "    query_parameters['modelId'] = query['modelID']\n",
    "    query_parameters['messages'] = query['prompt']\n",
    "    query_parameters['inferenceConfig'] = inference_config\n",
    "\n",
    "    if query['system'] is not  None:\n",
    "        query_parameters['system'] = query['system']\n",
    "\n",
    "    if query['tools'] is not  None:\n",
    "        query_parameters['toolConfig'] = query['tools']\n",
    "    \n",
    "    try:\n",
    "        #make the call to Bedrock\n",
    "        response = bedrock.converse(**query_parameters)\n",
    "        \n",
    "        #unpack the response from Bedrock\n",
    "        query['stopReason'] = response['stopReason']\n",
    "            \n",
    "        if query['stopReason'] == \"tool_use\":\n",
    "            #for tool use, we capture the relevant tool related information.\n",
    "            content_blocks = response['output']['message']['content']\n",
    "            for content in content_blocks:#skip to the block with the tool use request\n",
    "                if not 'toolUse' in content:continue\n",
    "                query['output'] = content#used as a conversation turn.\n",
    "                query['toolUseId'] = content['toolUse']['toolUseId']\n",
    "                query['toolName'] = content['toolUse']['name']\n",
    "                query['toolInput'] = content['toolUse']['input']\n",
    "        else:\n",
    "            query['output'] = response['output']['message']['content'][0]['text']\n",
    "\n",
    "        #grab the usage information\n",
    "        query['usage'] = response['usage'] #contains input and output token counts\n",
    "        query['latencyMs'] = response['metrics']['latencyMs']\n",
    "        query['cost'] = calc_cost(query['modelID'],response['usage'])\n",
    "        \n",
    "    except Exception as E:\n",
    "        if DEBUG:\n",
    "            print (\"Warning!  Model returned the following error:\")\n",
    "            print (E)\n",
    "        query['output'] = str(E)\n",
    "        query['stopReason'] = \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dbdc0901-7b59-4ed2-bfdb-49b3a16b418e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "# Threading function for queue processing.\n",
    "def thread_request(q):\n",
    "    while not q.empty():\n",
    "        this_query = q.get()    #fetch new work from the Queue\n",
    "        try:\n",
    "            ask_bedrock(this_query[0],DEBUG=this_query[1])\n",
    "        except Exception as e:\n",
    "            print('Error with threaded query:',str(e))\n",
    "        #signal to the queue that task has been processed\n",
    "        q.task_done()\n",
    "    return True\n",
    "\n",
    "def ask_bedrock_threaded(queries,MAX_THREADS = 50,DEBUG=False):\n",
    "    '''\n",
    "    Call ask_bedrock in parallel threads for maximum efficiency.\n",
    "    queries is just a list of query objects.  The threads do not return data because they add to each query object directly.\n",
    "    MAX_THREADS is how many queries to make in parallel.  Adjust this to avoid throttling.\n",
    "    '''\n",
    "    q = Queue(maxsize=0)\n",
    "    num_theads = min(MAX_THREADS, len(queries))\n",
    "    \n",
    "    #Populating Queue with tasks\n",
    "    for query in queries:\n",
    "        q.put((query,DEBUG))\n",
    "        \n",
    "    #Starting worker threads on queue processing\n",
    "    if DEBUG:print(\"Starting %s threads.\"%str(num_theads))\n",
    "    for i in range(num_theads):\n",
    "        #print('Starting thread ', i)\n",
    "        worker = Thread(target=thread_request, args=(q,))\n",
    "        worker.daemon = True\n",
    "        worker.start()\n",
    "\n",
    "    #now we wait until the queue has been processed\n",
    "    q.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8828513-689d-4400-8159-11265f7d9a3d",
   "metadata": {},
   "source": [
    "## 3) Example Use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702f0893-1386-4e48-bcbf-bf489c518522",
   "metadata": {},
   "source": [
    "### 3a) Basic use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "14616775-91d6-4ca6-a5c5-c0f971ee27ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today? If you have any questions or need information on a particular topic, feel free to ask. Whether it's about science, technology, history, or something else, I'm here to help.\n"
     ]
    }
   ],
   "source": [
    "#Just send a prompt, everything else is a default value.\n",
    "query = {}\n",
    "query[\"prompt\"] = \"Hello!\"\n",
    "ask_bedrock(query)\n",
    "print (query['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1756f9-a7f6-45ff-b946-e7c8f999dc2a",
   "metadata": {},
   "source": [
    "### 3b) Threading Basic Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "31ee5ac0-c5ee-45a1-818a-45d38455c492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infinite, beautiful, complex. \n",
      "\n",
      "These words encapsulate the multifaceted nature of love, highlighting its boundless potential, its capacity to bring beauty into the world, and the intricate layers that make it such a profound and intricate human experience.  (Latency:395ms)\n",
      "Infinite, beautiful, complex. \n",
      "\n",
      "These words capture the multifaceted nature of love, suggesting its boundless potential, its capacity to bring beauty into the world, and the intricate layers that make it such a profound and challenging human experience.  (Latency:397ms)\n",
      "Infinite, beautiful, complex. \n",
      "\n",
      "These words capture some of the essence of love, suggesting its boundless nature, its capacity to bring beauty into the world, and the intricate, multifaceted experience it often represents.  (Latency:360ms)\n",
      "CPU times: user 12.1 ms, sys: 0 ns, total: 12.1 ms\n",
      "Wall time: 414 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#testing ask_bedrock_threaded with the default values\n",
    "query_1 = {'prompt':\"In three words, what is love?\"}\n",
    "query_2 = {'prompt':\"In three words, what is love?\"}\n",
    "query_3 = {'prompt':\"In three words, what is love?\"}\n",
    "ask_bedrock_threaded([query_1,query_2,query_3], DEBUG=False)\n",
    "print (query_1['output'], \" (Latency:%sms)\"%query_1['latencyMs'])\n",
    "print (query_2['output'], \" (Latency:%sms)\"%query_2['latencyMs'])\n",
    "print (query_3['output'], \" (Latency:%sms)\"%query_3['latencyMs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2281c003-924c-4c42-94b3-6b935a9c31bb",
   "metadata": {},
   "source": [
    "### 3c) Basic Muli-turn Conversation Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "59a6ac2d-260c-490b-92cf-ce7ef1c6b93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please start the conversation: (enter STOP to end)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: Hello!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Hello! How can I assist you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: What do you think the color green tastes like?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: That's an interesting and imaginative question! Since colors are a visual experience and tastes are a sensory experience, there isn't a direct correspondence between them. However, we can use our creativity and associations to imagine what a color might taste like.\n",
      "\n",
      "Some common associations with the color green include:\n",
      "\n",
      "- Fresh, crisp flavors like green apples, limes, or fresh herbs like mint or basil.\n",
      "- Earthy, vegetal flavors like green beans, asparagus, or fresh greens like spinach or kale.\n",
      "- Grassy, herbal flavors like freshly cut grass or green tea.\n",
      "\n",
      "So if I had to imagine what the color green might taste like, I'd envision something bright, fresh, and slightly bitter or tangy - perhaps a blend of green apple, fresh herbs, and leafy greens. Of course, this is just my interpretation using my imagination and knowledge. What do you think green might taste like?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: STOP\n"
     ]
    }
   ],
   "source": [
    "conversation_history = []\n",
    "print (\"Please start the conversation: (enter STOP to end)\")\n",
    "while True:\n",
    "    user_input = input(\"User:\")\n",
    "    if user_input == \"STOP\": break\n",
    "    conversation_history.append(['user',user_input])\n",
    "    query = {}\n",
    "    query['prompt'] = create_message_json(conversation_history)\n",
    "    ask_bedrock(query)\n",
    "    response = query['output']\n",
    "    print (\"Assistant:\",response)\n",
    "    conversation_history.append(['assistant',response])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce5c721-ad7d-4be7-ad78-bf2f2d3407ae",
   "metadata": {},
   "source": [
    "### 3d) Basic Image Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "66537b24-3f91-452d-8ce8-d5976c9374f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a picture of a large, inflatable yellow rubber duck floating in the water in front of a cityscape.\n"
     ]
    }
   ],
   "source": [
    "image_query = {}\n",
    "prompt = create_message_json([(\"user\",\"what is this a picture of?  Please be concise.\"),('image','duck.jpg')])\n",
    "image_query['prompt'] = prompt\n",
    "ask_bedrock(image_query)\n",
    "print (image_query[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a4ec67-cff5-4b43-9934-7557d2fe6286",
   "metadata": {},
   "source": [
    "### 3e) Basic caching\n",
    "#### To use caching, simple add (\"cachePoint\",\"default\") to your prompt.  Everything before that point is added to the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c7127fac-5431-4995-b026-4957e5d54c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call:\n",
      "Latency:  387\n",
      "Usage:  {'inputTokens': 12, 'outputTokens': 2, 'totalTokens': 6013, 'cacheReadInputTokens': 0, 'cacheWriteInputTokens': 5999}\n",
      "Cost for 1000 of these calls:  0.36114\n",
      "Second call:\n",
      "Latency:  212\n",
      "Usage:  {'inputTokens': 12, 'outputTokens': 2, 'totalTokens': 6013, 'cacheReadInputTokens': 5999, 'cacheWriteInputTokens': 0}\n",
      "Cost for 1000 of these calls:  0.09118500000000002\n"
     ]
    }
   ],
   "source": [
    "cache_query = {}\n",
    "\n",
    "#caching only works for prompts longer than 1K tokens, so lets make a long prompt.\n",
    "long_string = \"I love tacos! :)  \" * 1000\n",
    "\n",
    "prompt = create_message_json([(\"user\",long_string),(\"cachePoint\",\"default\"),(\"user\",\"What do I love?  Respond with a single word.\")])\n",
    "cache_query['prompt'] = prompt\n",
    "ask_bedrock(cache_query)\n",
    "print (\"First call:\")\n",
    "print (\"Latency: \",cache_query[\"latencyMs\"])\n",
    "print (\"Usage: \",cache_query[\"usage\"])\n",
    "print (\"Cost for 1000 of these calls: \", cache_query[\"cost\"]*1000)\n",
    "ask_bedrock(cache_query)\n",
    "print (\"Second call:\")\n",
    "print (\"Latency: \",cache_query[\"latencyMs\"])\n",
    "print (\"Usage: \",cache_query[\"usage\"])\n",
    "print (\"Cost for 1000 of these calls: \", cache_query[\"cost\"]*1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef0feec-9363-4a42-ba1b-49cd4444d8e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3f) Basic Tool Use\n",
    "#### Start by defining our tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2a1231b9-7b65-4d9b-9f5c-7f5e7e07a6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the actual tool:\n",
    "def example_weather_tool(time_of_day):\n",
    "    if time_of_day == 'AM':\n",
    "        return \"Sunny\"\n",
    "    if time_of_day == 'PM':\n",
    "        return \"Rainy\"\n",
    "    else:\n",
    "        return \"Error\"\n",
    "\n",
    "#the config so that the model knows about this tool\n",
    "example_tool_config = [\n",
    "    {\n",
    "        \"toolSpec\": {\n",
    "            \"name\": \"weather\",\n",
    "            \"description\": \"Get the local weather.\",\n",
    "            \"inputSchema\": {\n",
    "                \"json\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"time_of_day\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The time of day to get weather for, either AM or PM.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"time_of_day\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb88131-5aa2-4ca7-8790-5d886d253fba",
   "metadata": {},
   "source": [
    "#### Now we make a call where the model may want to use a tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5fd04f1c-e57a-4257-a41e-fc0010e20cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool requested: weather\n",
      "Tool input: {'time_of_day': 'AM'}\n"
     ]
    }
   ],
   "source": [
    "#create a list of one or more tools.  ask_clade will package this into the proper call format.\n",
    "query = {}\n",
    "query[\"tools\"] = example_tool_config\n",
    "msg_1 = \"What is the weather this morning?\"\n",
    "query[\"prompt\"] = msg_1\n",
    "ask_bedrock(query)\n",
    "print (\"Tool requested:\",query['toolName'])\n",
    "print (\"Tool input:\",query['toolInput'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eb4c00-db57-4ec1-865f-e3aa61c3f42f",
   "metadata": {},
   "source": [
    "#### Next, call the tool as requested by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5dfbd344-b4f9-4441-aed9-980378974b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tool_result = example_weather_tool(query['toolInput']['time_of_day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c96f610-10b1-46aa-a046-f2d8f9977a11",
   "metadata": {},
   "source": [
    "#### Finally, return the tool's response to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "af56ef9a-8302-4de2-80aa-04fbf6c5d61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<thinking> The weather tool has provided the result for the morning weather. I can now provide this information to the User. </thinking>\n",
      "The weather this morning is sunny.\n"
     ]
    }
   ],
   "source": [
    "tool_request_from_claude = query['output']\n",
    "message_list = []\n",
    "message_list.append([\"user\",msg_1])\n",
    "message_list.append([\"tool_request\",query['toolUseId'],query['toolName'],query['toolInput']])\n",
    "message_list.append(['tool_result',query['toolUseId'],tool_result])\n",
    "\n",
    "query_2 = {}\n",
    "#when passing the tool results to the model, it still needs to understand to original tool.\n",
    "query_2[\"tools\"] = example_tool_config\n",
    "#change our message history into the Converse API nested JSON format.\n",
    "prompt = create_message_json(message_list)\n",
    "query_2['prompt'] = prompt\n",
    "\n",
    "#send the full message history, tool use request, and tool use response to Claude so that it can answer the original question.\n",
    "ask_bedrock(query_2)\n",
    "print (query_2['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51822882-7365-4c99-85a4-80cc07bfcd28",
   "metadata": {},
   "source": [
    "### Next example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b23645-e72c-4c4a-b84a-779b0c121015",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
