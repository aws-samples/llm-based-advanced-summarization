{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7efc6af-6b72-4faa-9da2-db109ae42484",
   "metadata": {},
   "source": [
    "# Bedrock Basics\n",
    "This notebook provides basic functionality for talking with Bedrock.  The functions are:\n",
    "\n",
    "  - fill_defaults: Fills in all the default values for each call.  Allows you to send Bedrock as much or as little information as you like. Used by ask_bedrock.\n",
    "  - calc_cost:  Calculates the cost, in dollars, for each call.  Based on public pricing as of 5/1/2024, us-east-1 region.  Used by ask_bedrock.\n",
    "  - create_message_json:  takes an array of conversation turns and wraps them in the JSON format used by the Converse API\n",
    "  - ask_bedrock:  Use the converse API to send a query to Bedrock.\n",
    "  - ask_bedrock_threaded:  Call ask_bedrock in parallel threads for maximum efficiency.\n",
    "\n",
    "All of the functions work around the idea of a Query object.  A Query represents a single call to the Bedrock API.  The program should load up the Query with all the info Bedrock needs, like model and prompt, and then after Bedrock is called, the response to that prompt is stored in the same Query object.  To support multiturn conversations, the full conversation history is added to a Query object. See part 3 of this notebook for examples.\n",
    "  \n",
    "#### This notebook has three sections:\n",
    "  1) Prepare the environment and install dependencies.\n",
    "  2) Define the basic functions described above.\n",
    "  3) Example use for each of the functions.\n",
    "     - Basic Use\n",
    "     - Threaded Basic Use\n",
    "     - Basic Muli-turn Conversation Use\n",
    "     - Basic Image Input\n",
    "     - Basic caching\n",
    "     - Tool Use (Function Calling)\n",
    "     - Bring it all together - a simple chat bot with web search enabled\n",
    "    \n",
    "\n",
    "#### This notebook was authored by Justin Muller.  Email: justmul@amazon.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999cd191-24fc-419d-bb33-9074c199eb86",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1) Prepare the environment and install dependencies.\n",
    "First install and update the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71413432-079c-439d-b465-70747b1fbc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, tabulate, json, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ff8e45-3b08-477f-8177-b750165eb819",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --force-reinstall --quiet boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24185562-a4a8-4501-8c2b-bf726f32f1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect with Claude via Bedrock using Boto3\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "#increase the standard time out limits in boto3, because Bedrock may take a while to respond to large requests.\n",
    "my_config = Config(\n",
    "    connect_timeout=60*5,\n",
    "    read_timeout=60*5,\n",
    ")\n",
    "bedrock = boto3.client(service_name='bedrock-runtime',config=my_config)\n",
    "bedrock_service = boto3.client(service_name='bedrock',config=my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e5d678a-8abc-4d39-9cb0-d27a384995e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Model Name                       </td><td>Model ID                                    </td></tr>\n",
       "<tr><td>US Anthropic Claude 3 Sonnet     </td><td>us.anthropic.claude-3-sonnet-20240229-v1:0  </td></tr>\n",
       "<tr><td>US Anthropic Claude 3 Opus       </td><td>us.anthropic.claude-3-opus-20240229-v1:0    </td></tr>\n",
       "<tr><td>US Anthropic Claude 3 Haiku      </td><td>us.anthropic.claude-3-haiku-20240307-v1:0   </td></tr>\n",
       "<tr><td>US Meta Llama 3.2 11B Instruct   </td><td>us.meta.llama3-2-11b-instruct-v1:0          </td></tr>\n",
       "<tr><td>US Meta Llama 3.2 3B Instruct    </td><td>us.meta.llama3-2-3b-instruct-v1:0           </td></tr>\n",
       "<tr><td>US Meta Llama 3.2 90B Instruct   </td><td>us.meta.llama3-2-90b-instruct-v1:0          </td></tr>\n",
       "<tr><td>US Meta Llama 3.2 1B Instruct    </td><td>us.meta.llama3-2-1b-instruct-v1:0           </td></tr>\n",
       "<tr><td>US Anthropic Claude 3.5 Sonnet   </td><td>us.anthropic.claude-3-5-sonnet-20240620-v1:0</td></tr>\n",
       "<tr><td>US Anthropic Claude 3.5 Haiku    </td><td>us.anthropic.claude-3-5-haiku-20241022-v1:0 </td></tr>\n",
       "<tr><td>US Meta Llama 3.1 8B Instruct    </td><td>us.meta.llama3-1-8b-instruct-v1:0           </td></tr>\n",
       "<tr><td>US Meta Llama 3.1 70B Instruct   </td><td>us.meta.llama3-1-70b-instruct-v1:0          </td></tr>\n",
       "<tr><td>US Nova Lite                     </td><td>us.amazon.nova-lite-v1:0                    </td></tr>\n",
       "<tr><td>US Nova Pro                      </td><td>us.amazon.nova-pro-v1:0                     </td></tr>\n",
       "<tr><td>US Nova Micro                    </td><td>us.amazon.nova-micro-v1:0                   </td></tr>\n",
       "<tr><td>US Meta Llama 3.3 70B Instruct   </td><td>us.meta.llama3-3-70b-instruct-v1:0          </td></tr>\n",
       "<tr><td>US Anthropic Claude 3.5 Sonnet v2</td><td>us.anthropic.claude-3-5-sonnet-20241022-v2:0</td></tr>\n",
       "<tr><td>US DeepSeek-R1                   </td><td>us.deepseek.r1-v1:0                         </td></tr>\n",
       "<tr><td>US Mistral Pixtral Large 25.02   </td><td>us.mistral.pixtral-large-2502-v1:0          </td></tr>\n",
       "<tr><td>US Anthropic Claude 3.7 Sonnet   </td><td>us.anthropic.claude-3-7-sonnet-20250219-v1:0</td></tr>\n",
       "<tr><td>US Llama 4 Scout 17B Instruct    </td><td>us.meta.llama4-scout-17b-instruct-v1:0      </td></tr>\n",
       "<tr><td>US Llama 4 Maverick 17B Instruct </td><td>us.meta.llama4-maverick-17b-instruct-v1:0   </td></tr>\n",
       "<tr><td>US Nova Premier                  </td><td>us.amazon.nova-premier-v1:0                 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "'<table>\\n<tbody>\\n<tr><td>Model Name                       </td><td>Model ID                                    </td></tr>\\n<tr><td>US Anthropic Claude 3 Sonnet     </td><td>us.anthropic.claude-3-sonnet-20240229-v1:0  </td></tr>\\n<tr><td>US Anthropic Claude 3 Opus       </td><td>us.anthropic.claude-3-opus-20240229-v1:0    </td></tr>\\n<tr><td>US Anthropic Claude 3 Haiku      </td><td>us.anthropic.claude-3-haiku-20240307-v1:0   </td></tr>\\n<tr><td>US Meta Llama 3.2 11B Instruct   </td><td>us.meta.llama3-2-11b-instruct-v1:0          </td></tr>\\n<tr><td>US Meta Llama 3.2 3B Instruct    </td><td>us.meta.llama3-2-3b-instruct-v1:0           </td></tr>\\n<tr><td>US Meta Llama 3.2 90B Instruct   </td><td>us.meta.llama3-2-90b-instruct-v1:0          </td></tr>\\n<tr><td>US Meta Llama 3.2 1B Instruct    </td><td>us.meta.llama3-2-1b-instruct-v1:0           </td></tr>\\n<tr><td>US Anthropic Claude 3.5 Sonnet   </td><td>us.anthropic.claude-3-5-sonnet-20240620-v1:0</td></tr>\\n<tr><td>US Anthropic Claude 3.5 Haiku    </td><td>us.anthropic.claude-3-5-haiku-20241022-v1:0 </td></tr>\\n<tr><td>US Meta Llama 3.1 8B Instruct    </td><td>us.meta.llama3-1-8b-instruct-v1:0           </td></tr>\\n<tr><td>US Meta Llama 3.1 70B Instruct   </td><td>us.meta.llama3-1-70b-instruct-v1:0          </td></tr>\\n<tr><td>US Nova Lite                     </td><td>us.amazon.nova-lite-v1:0                    </td></tr>\\n<tr><td>US Nova Pro                      </td><td>us.amazon.nova-pro-v1:0                     </td></tr>\\n<tr><td>US Nova Micro                    </td><td>us.amazon.nova-micro-v1:0                   </td></tr>\\n<tr><td>US Meta Llama 3.3 70B Instruct   </td><td>us.meta.llama3-3-70b-instruct-v1:0          </td></tr>\\n<tr><td>US Anthropic Claude 3.5 Sonnet v2</td><td>us.anthropic.claude-3-5-sonnet-20241022-v2:0</td></tr>\\n<tr><td>US DeepSeek-R1                   </td><td>us.deepseek.r1-v1:0                         </td></tr>\\n<tr><td>US Mistral Pixtral Large 25.02   </td><td>us.mistral.pixtral-large-2502-v1:0          </td></tr>\\n<tr><td>US Anthropic Claude 3.7 Sonnet   </td><td>us.anthropic.claude-3-7-sonnet-20250219-v1:0</td></tr>\\n<tr><td>US Llama 4 Scout 17B Instruct    </td><td>us.meta.llama4-scout-17b-instruct-v1:0      </td></tr>\\n<tr><td>US Llama 4 Maverick 17B Instruct </td><td>us.meta.llama4-maverick-17b-instruct-v1:0   </td></tr>\\n<tr><td>US Nova Premier                  </td><td>us.amazon.nova-premier-v1:0                 </td></tr>\\n</tbody>\\n</table>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the endpoints we have avalaible.\n",
    "# If the model you want is not listed here, check to make sure that you have added access to the model in this region via the console.:\n",
    "models = [[\"Model Name\",\"Model ID\"]]\n",
    "modelName_to_ID = {} #used to make code more readable by using model names rather than their endpoint ID.\n",
    "model_list = bedrock_service.list_inference_profiles()[\"inferenceProfileSummaries\"]\n",
    "for line in model_list:\n",
    "    models.append([line['inferenceProfileName'],line['inferenceProfileId']])\n",
    "    modelName_to_ID[line['inferenceProfileName']] = line['inferenceProfileId']\n",
    "tabulate.tabulate(models, tablefmt='html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3754c8-73f0-4c95-9b53-8fe69dc2d672",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Use the above list of model names (not ID's) to pick which model to use!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea8c2d8-01b7-44ae-b469-5466309c980c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2) Define the basic functions\n",
    "\n",
    "  - fill_defaults: Fills in all the default values for each call.  Allows you to send Bedrock as much or as little information as you like. Used by ask_bedrock.\n",
    "  - calc_cost:  Calculates the cost, in dollars, for each call.  Based on public pricing as of 5/1/2024, us-east-1 region.  Used by ask_bedrock.\n",
    "  - create_message_json:  takes an array of conversation turns and wraps them in the JSON format used by the Converse API\n",
    "  - ask_bedrock:  Use the converse API to send a query to Bedrock.\n",
    "  - ask_bedrock_threaded:  Call ask_bedrock in parallel threads for maximum efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94433dff-66b8-4b46-b4a9-823cdc31f1fd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def fill_defaults(query):\n",
    "    \"\"\"\n",
    "    Fills in all the default values for each call.  Allows you to send Bedrock as much or as little information  as you like. Used by ask_bedrock.\n",
    "    This only fills in values that are missing.  Change values here to make them default for all calls.\n",
    "    If the prompt is a simple string, it also converts the prompt to a json format expected by the Converse API.\n",
    "    \"\"\"\n",
    "    #prompt defaults\n",
    "    if not 'prompt' in query:query['prompt'] = \"What is your quest?\"\n",
    "    if not 'system' in query:query['system'] = None\n",
    "    if not 'tools' in query:query['tools'] = None\n",
    "    if not 'model' in query:query['model'] = \"US Nova Lite\"\n",
    "    if not 'modelID' in query:query['modelID'] = modelName_to_ID[query['model']]\n",
    "\n",
    "    #Inference defaults\n",
    "    if not 'maxTokens' in query:query['maxTokens'] = None\n",
    "    if not 'stopSequences' in query:query['stopSequences'] = []\n",
    "    if not 'temperature' in query:query['temperature'] = 0.5\n",
    "\n",
    "    #Converse requires that the prompt be a JSON opject, so change to that if it is currently a string.\n",
    "    if type(query['prompt'])==str:\n",
    "        query['prompt'] = create_message_json([[\"user\",query['prompt']]])\n",
    "\n",
    "    #Converse requires that the system prompt be a list, so change to that if it is currently a string.\n",
    "    #if the system propmt is an empty string, leave it along since it won't be sent to Bedrock.\n",
    "    if type(query['system'])==str and query['system'] != None:\n",
    "        query['system'] = [{\"text\": query['system']}]\n",
    "\n",
    "    #Converse wants the tools as an object, so convert to that.\n",
    "    if query['tools'] is not None:\n",
    "       query['tools']={\"tools\": query['tools']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65b76b2f-1554-402e-8b20-7f1d25ec8fad",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#helper function for converting tokens to public pricing for Bedrock, as of May 7 2025.\n",
    "def calc_cost(model,usage):\n",
    "    \"\"\"\n",
    "    Calculates the cost, in dollars, for each call.  Based on public pricing as of 12/2/2024, us-east-1 region.  Used by ask_bedrock.\n",
    "    model is the model name used to make the call.\n",
    "    usage is the usage object returned by the call the Converse API as part of the response. \n",
    "    \"\"\"\n",
    "    cost = 0\n",
    "    input_tokens = usage['inputTokens']\n",
    "    output_tokens = usage['outputTokens']\n",
    "    read_cache_tokens = 0\n",
    "    read_cache_cost = 0\n",
    "    #for anthropic models, there's an extra charge for cache writes\n",
    "    write_cache_cost = 0\n",
    "    write_cache_tokens = 0\n",
    "    \n",
    "    if 'cacheWriteInputTokens' in usage:#this usage only shows up when a cache point is included in the prompt.\n",
    "        read_cache_tokens = usage['cacheReadInputTokens']\n",
    "        write_cache_tokens = usage['cacheWriteInputTokens']\n",
    "        read_cache_cost = -1 #set to negative 1, so we can catch cases where cache was used, but we don't have pricing info.\n",
    "        write_cache_cost = -1 #set to negative 1, so we can catch cases where cache was used, but we don't have pricing info.\n",
    "\n",
    "    million=1000000\n",
    "    thousand=1000\n",
    "    match model:\n",
    "        case \"us.anthropic.claude-3-haiku-20240307-v1:0\":\n",
    "            input_cost = 0.00025/thousand\n",
    "            output_cost = 0.00125/thousand\n",
    "        case \"us.anthropic.claude-3-5-haiku-20241022-v1:0\":\n",
    "            input_cost = 0.0008/thousand\n",
    "            output_cost = 0.004/thousand\n",
    "            read_cache_cost = 0.00008/thousand\n",
    "            write_cache_cost = 0.001/thousand\n",
    "        case \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\":\n",
    "            input_cost = 0.003/thousand\n",
    "            output_cost = 0.015/thousand\n",
    "            read_cache_cost = 0.0003/thousand\n",
    "            write_cache_cost = 0.00375/thousand\n",
    "        case \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\":\n",
    "            input_cost = 0.003/thousand\n",
    "            output_cost = 0.015/thousand\n",
    "            read_cache_cost = 0.0003/thousand\n",
    "            write_cache_cost = 0.00375/thousand\n",
    "        case \"us.meta.llama3-2-90b-instruct-v1:0\":\n",
    "            input_cost = 0.00072/thousand\n",
    "            output_cost = 0.00072/thousand\n",
    "        case \"us.meta.llama3-2-11b-instruct-v1:0\":\n",
    "            input_cost = 0.00016/thousand\n",
    "            output_cost = 0.00016/thousand\n",
    "        case \"us.amazon.nova-pro-v1:0\":\n",
    "            input_cost = 0.0008/thousand\n",
    "            output_cost = 0.0032/thousand\n",
    "            read_cache_cost = 0.0002/thousand\n",
    "            write_cache_cost = input_cost\n",
    "        case \"us.amazon.nova-premier-v1:0\":\n",
    "            input_cost = 0.0025/thousand\n",
    "            output_cost = 0.0125/thousand\n",
    "        case \"us.amazon.nova-lite-v1:0\":\n",
    "            input_cost = 0.00006/thousand\n",
    "            output_cost = 0.00024/thousand\n",
    "            read_cache_cost = 0.000015/thousand\n",
    "            write_cache_cost = input_cost\n",
    "        case \"us.amazon.nova-micro-v1:0\":\n",
    "            input_cost = 0.000035/thousand\n",
    "            output_cost = 0.00014/thousand\n",
    "            read_cache_cost = 0.00000875/thousand\n",
    "            write_cache_cost = input_cost\n",
    "        case _:\n",
    "            print (\"Warning!  No pricing data found for this model.  Setting cost to 0.  Please update calc_cost().\")\n",
    "            input_cost = 0\n",
    "            output_cost = 0\n",
    "            cache_cost = 0\n",
    "            \n",
    "    if write_cache_cost == -1 or read_cache_cost == -1:\n",
    "        print (\"Warning!  No pricing data found for CACHE for this model.  Setting cost to 0.  Please update calc_cost().\")\n",
    "        input_cost = 0\n",
    "        output_cost = 0\n",
    "        cache_cost = 0\n",
    "        \n",
    "    cost = input_tokens*input_cost + output_tokens*output_cost + write_cache_tokens*write_cache_cost + read_cache_tokens*read_cache_cost\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f349daad-8f6b-430f-baaa-d766b0048e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message_json(messages):\n",
    "    \"\"\"\n",
    "    takes an array of conversation turns and wraps them in the JSON format used by the Converse API\n",
    "    Each element of the array should be a pair (TYPE,CONTENT) where TYPE identifies which turn it is,\n",
    "    and CONTENT is the content for that turn.  TYPE can be user, assistant, image, tool_request or tool_result\n",
    "    This is the expected format:\n",
    "\n",
    "    user, prompt string\n",
    "    assistant, response string\n",
    "    tool_request, toolID, tool name, tool input JSON\n",
    "    tool_result, toolID, tool result string\n",
    "    image, image location\n",
    "\n",
    "    This function exists because the Converse API uses a high number of nested dictionaries, which can be hard to track.\n",
    "    This function simplifies tracking conversation history because it can be stored in an array of ordered turns.\n",
    "    \"\"\"\n",
    "    message_jsons = []\n",
    "    for msg in messages:\n",
    "        if msg[0]==\"user\":\n",
    "            message_jsons.append({\"role\": \"user\",\"content\": [{\"text\": msg[1]}]})\n",
    "        elif msg[0]==\"assistant\":\n",
    "            message_jsons.append({\"role\": \"assistant\",\"content\": [{\"text\": msg[1]}]})\n",
    "        elif msg[0]==\"cachePoint\":\n",
    "            message_jsons.append({\"role\": \"user\",\"content\": [{\"cachePoint\": {\"type\": msg[1]}}]})\n",
    "        elif msg[0]==\"tool_request\":\n",
    "            message_jsons.append({\"role\": \"assistant\",\"content\": [{\"toolUse\": {\"toolUseId\":msg[1],\"name\":msg[2],\"input\":msg[3]}}]})\n",
    "        elif msg[0]==\"tool_result\":\n",
    "            message_jsons.append({\"role\": \"user\",\"content\": [{\"toolResult\": {\"toolUseId\":msg[1],\"content\":[{\"json\":{\"result\":msg[2]}}]}}]})\n",
    "        elif msg[0]==\"image\":\n",
    "            with open(msg[1], \"rb\") as f:\n",
    "                image = f.read()\n",
    "            filename, file_extension = os.path.splitext(msg[1])\n",
    "            file_extension = file_extension.replace(\".\",\"\")\n",
    "            if file_extension == \"jpg\": file_extension = \"jpeg\" #requirment of the Converse API\n",
    "            message_jsons.append({\"role\": \"user\",\"content\": [{\"image\": {\"format\":file_extension,\"source\":{\"bytes\":image}}}]})\n",
    "        else:\n",
    "            raise(Exception(\"Error!  Message type not recognized:\",msg[0]))\n",
    "    #pack concurent turns together.  Converse requires that the array always alternates between user and assistiant, so if any are two in a row, they need to be in the same user block.\n",
    "    packed_messages = []\n",
    "    current_message = \"\"\n",
    "    for i, this_msg in enumerate(message_jsons):\n",
    "        if i == 0:\n",
    "            current_message = (this_msg['role'],this_msg['content'])\n",
    "            if i+1>=len(message_jsons):#this is the only message, no need to pack more\n",
    "                packed_messages = [this_msg]\n",
    "            continue\n",
    "        if this_msg['role'] == current_message[0]:#next message is the same role as current, so pack it in\n",
    "            current_message[1].append(this_msg['content'][0])\n",
    "        else:#this is a new role, so save the previous stuff, and make this the new stuff.\n",
    "            packed_messages.append({\"role\": current_message[0],\"content\": current_message[1]})\n",
    "            current_message = (this_msg['role'],this_msg['content'])\n",
    "            \n",
    "        if i+1>=len(message_jsons):#this is the last message, save it to the packed list.\n",
    "                packed_messages.append({\"role\": current_message[0],\"content\": current_message[1]})\n",
    "    return packed_messages\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75a1314c-b22f-484d-b5ca-ee9c0ff37b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_bedrock(query, DEBUG=False):\n",
    "    \"\"\"\n",
    "    Use the converse API to send a query to Claude.\n",
    "    Note that if something goes wrong with calling the model, the stop reason will be ERROR, and the output message will be the error message.\n",
    "    \"\"\"\n",
    "    #first, fill in any values this query is missing.\n",
    "    fill_defaults(query)\n",
    "\n",
    "    #set up the inference configuration options\n",
    "    inference_config = {\n",
    "        \"stopSequences\": query['stopSequences'],\n",
    "        \"temperature\": query['temperature']\n",
    "    }\n",
    "\n",
    "    if query['maxTokens'] is not None:\n",
    "        inference_config[\"maxTokens\"] = query['maxTokens'],\n",
    "\n",
    "    #build the parameters for calling bedrock, which change depending on the type of call.\n",
    "    query_parameters = {}\n",
    "    query_parameters['modelId'] = query['modelID']\n",
    "    query_parameters['messages'] = query['prompt']\n",
    "    query_parameters['inferenceConfig'] = inference_config\n",
    "\n",
    "    if query['system'] is not  None:\n",
    "        query_parameters['system'] = query['system']\n",
    "\n",
    "    if query['tools'] is not  None:\n",
    "        query_parameters['toolConfig'] = query['tools']\n",
    "    \n",
    "    try:\n",
    "        #make the call to Bedrock\n",
    "        response = bedrock.converse(**query_parameters)\n",
    "        \n",
    "        #unpack the response from Bedrock\n",
    "        query['stopReason'] = response['stopReason']\n",
    "            \n",
    "        if query['stopReason'] == \"tool_use\":\n",
    "            #for tool use, we capture the relevant tool related information.\n",
    "            content_blocks = response['output']['message']['content']\n",
    "            for content in content_blocks:#skip to the block with the tool use request\n",
    "                if not 'toolUse' in content:continue\n",
    "                query['output'] = content#used as a conversation turn.\n",
    "                query['toolUseId'] = content['toolUse']['toolUseId']\n",
    "                query['toolName'] = content['toolUse']['name']\n",
    "                query['toolInput'] = content['toolUse']['input']\n",
    "        else:\n",
    "            query['output'] = response['output']['message']['content'][0]['text']\n",
    "\n",
    "        #grab the usage information\n",
    "        query['usage'] = response['usage'] #contains input and output token counts\n",
    "        query['latencyMs'] = response['metrics']['latencyMs']\n",
    "        query['cost'] = calc_cost(query['modelID'],response['usage'])\n",
    "        \n",
    "    except Exception as E:\n",
    "        if DEBUG:\n",
    "            print (\"Warning!  Model returned the following error:\")\n",
    "            print (E)\n",
    "        query['output'] = str(E)\n",
    "        query['stopReason'] = \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbdc0901-7b59-4ed2-bfdb-49b3a16b418e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "# Threading function for queue processing.\n",
    "def thread_request(q):\n",
    "    while not q.empty():\n",
    "        this_query = q.get()    #fetch new work from the Queue\n",
    "        try:\n",
    "            ask_bedrock(this_query[0],DEBUG=this_query[1])\n",
    "        except Exception as e:\n",
    "            print('Error with threaded query:',str(e))\n",
    "        #signal to the queue that task has been processed\n",
    "        q.task_done()\n",
    "    return True\n",
    "\n",
    "def ask_bedrock_threaded(queries,MAX_THREADS = 50,DEBUG=False):\n",
    "    '''\n",
    "    Call ask_bedrock in parallel threads for maximum efficiency.\n",
    "    queries is just a list of query objects.  The threads do not return data because they add to each query object directly.\n",
    "    MAX_THREADS is how many queries to make in parallel.  Adjust this to avoid throttling.\n",
    "    '''\n",
    "    q = Queue(maxsize=0)\n",
    "    num_theads = min(MAX_THREADS, len(queries))\n",
    "    \n",
    "    #Populating Queue with tasks\n",
    "    for query in queries:\n",
    "        q.put((query,DEBUG))\n",
    "        \n",
    "    #Starting worker threads on queue processing\n",
    "    if DEBUG:print(\"Starting %s threads.\"%str(num_theads))\n",
    "    for i in range(num_theads):\n",
    "        #print('Starting thread ', i)\n",
    "        worker = Thread(target=thread_request, args=(q,))\n",
    "        worker.daemon = True\n",
    "        worker.start()\n",
    "\n",
    "    #now we wait until the queue has been processed\n",
    "    q.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8828513-689d-4400-8159-11265f7d9a3d",
   "metadata": {},
   "source": [
    "## 3) Example Use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702f0893-1386-4e48-bcbf-bf489c518522",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3a) Basic use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14616775-91d6-4ca6-a5c5-c0f971ee27ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today? If you have any questions or need information on a particular topic, feel free to ask. Whether it's about science, technology, history, or something else, I'm here to help.\n"
     ]
    }
   ],
   "source": [
    "#Just send a prompt, everything else is a default value.\n",
    "query = {}\n",
    "query[\"prompt\"] = \"Hello!\"\n",
    "ask_bedrock(query)\n",
    "print (query['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9d34438-f36f-427a-9ae9-f936e1067235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt - [{'role': 'user', 'content': [{'text': 'Hello!'}]}]\n",
      "system - None\n",
      "tools - None\n",
      "model - US Nova Lite\n",
      "modelID - us.amazon.nova-lite-v1:0\n",
      "maxTokens - None\n",
      "stopSequences - []\n",
      "temperature - 0.5\n",
      "stopReason - end_turn\n",
      "output - Hello! How can I assist you today? If you have any questions or need information on a particular topic, feel free to ask. Whether it's about science, technology, history, or something else, I'm here to help.\n",
      "usage - {'inputTokens': 2, 'outputTokens': 49, 'totalTokens': 51}\n",
      "latencyMs - 322\n",
      "cost - 1.1880000000000001e-05\n"
     ]
    }
   ],
   "source": [
    "#Everything is stored in the query object.  Let's see what's there.\n",
    "for key in query:\n",
    "    print (key,\"-\",query[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1756f9-a7f6-45ff-b946-e7c8f999dc2a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3b) Threading Basic Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31ee5ac0-c5ee-45a1-818a-45d38455c492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infinite, beautiful, complex. \n",
      "\n",
      "These words encapsulate some of the multifaceted nature of love, suggesting its boundless potential, its capacity to bring beauty into the world, and its intricate and often challenging dynamics.  (Latency:403ms)\n",
      "Infinite, beautiful, complex. \n",
      "\n",
      "These words capture some of the multifaceted nature of love, suggesting its boundless potential, its capacity to bring beauty into the world, and the intricate layers that make it such a profound and often challenging experience.  (Latency:462ms)\n",
      "Infinite, beautiful, complex. \n",
      "\n",
      "These words capture the essence of love, highlighting its boundless nature, its capacity to bring beauty into the world, and the intricate layers that make it such a multifaceted experience.  (Latency:369ms)\n",
      "CPU times: user 55.7 ms, sys: 8.05 ms, total: 63.7 ms\n",
      "Wall time: 1.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#testing ask_bedrock_threaded with the default values\n",
    "query_1 = {'prompt':\"In three words, what is love?\"}\n",
    "query_2 = {'prompt':\"In three words, what is love?\"}\n",
    "query_3 = {'prompt':\"In three words, what is love?\"}\n",
    "ask_bedrock_threaded([query_1,query_2,query_3], DEBUG=False)\n",
    "print (query_1['output'], \" (Latency:%sms)\"%query_1['latencyMs'])\n",
    "print (query_2['output'], \" (Latency:%sms)\"%query_2['latencyMs'])\n",
    "print (query_3['output'], \" (Latency:%sms)\"%query_3['latencyMs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2281c003-924c-4c42-94b3-6b935a9c31bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3c) Basic Muli-turn Conversation Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59a6ac2d-260c-490b-92cf-ce7ef1c6b93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please start the conversation: (enter STOP to end)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: Hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Hello! How can I assist you today? If you have any questions or need information on a particular topic, feel free to ask. Whether it's about science, technology, history, or something else, I'm here to help.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: What is love?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Love is a complex and multifaceted emotion that can be experienced in various forms and intensities. It is often described as a deep affection, attachment, or strong positive feeling towards someone or something. Here are some common types of love:\n",
      "\n",
      "1. **Romantic Love**: This is the intense affection and attachment one feels towards a romantic partner. It often includes passion, intimacy, and a desire for union.\n",
      "\n",
      "2. **Familial Love**: This refers to the love and affection felt towards family members, such as parents, siblings, and children. It is often characterized by a sense of duty, care, and support.\n",
      "\n",
      "3. **Platonic Love**: This is a deep, non-romantic friendship where individuals share a strong bond and affection without any sexual or romantic involvement.\n",
      "\n",
      "4. **Self-Love**: This is the love and acceptance of oneself. It involves recognizing one's own worth, valuing oneself, and treating oneself with kindness and respect.\n",
      "\n",
      "5. **Unconditional Love**: This type of love is given without any conditions or expectations of reciprocity. It is often associated with parental love and the love of a higher power.\n",
      "\n",
      "6. **Companionate Love**: This is a deep affection and commitment that develops over time, often seen in long-term relationships. It is characterized by trust, respect, and a strong sense of partnership.\n",
      "\n",
      "7. **Agape Love**: This is a selfless, unconditional love that is often associated with altruism and the well-being of others. It is sometimes referred to as \"divine love\" or \"charity.\"\n",
      "\n",
      "Love can manifest in many ways, and its expression can vary greatly among individuals and cultures. It is a fundamental human experience that plays a crucial role in our social connections and overall well-being.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: STOP\n"
     ]
    }
   ],
   "source": [
    "conversation_history = []\n",
    "print (\"Please start the conversation: (enter STOP to end)\")\n",
    "while True:\n",
    "    user_input = input(\"User:\")\n",
    "    if user_input == \"STOP\": break\n",
    "    conversation_history.append(['user',user_input])\n",
    "    query = {}\n",
    "    query['prompt'] = create_message_json(conversation_history)\n",
    "    ask_bedrock(query)\n",
    "    response = query['output']\n",
    "    print (\"Assistant:\",response)\n",
    "    conversation_history.append(['assistant',response])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce5c721-ad7d-4be7-ad78-bf2f2d3407ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3d) Basic Image Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66537b24-3f91-452d-8ce8-d5976c9374f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a picture of a large inflatable yellow duck floating in the water in front of a city skyline.\n"
     ]
    }
   ],
   "source": [
    "image_query = {}\n",
    "prompt = create_message_json([(\"user\",\"what is this a picture of?  Please be concise.\"),('image','duck.jpg')])\n",
    "image_query['prompt'] = prompt\n",
    "ask_bedrock(image_query)\n",
    "print (image_query[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09cce0f-b111-4043-bb2b-e9850b053417",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3e) Basic caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a079a-1fcb-4a35-93dd-803d80b11d82",
   "metadata": {},
   "source": [
    "#### To use caching, simple add (\"cachePoint\",\"default\") to your prompt.  Everything before that point is added to the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c7127fac-5431-4995-b026-4957e5d54c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call:\n",
      "Usage:  {'inputTokens': 11, 'outputTokens': 3, 'totalTokens': 4015, 'cacheReadInputTokens': 0, 'cacheWriteInputTokens': 4001}\n",
      "Latency:  501\n",
      "Cost for 1000 of these calls:  3.2192000000000003\n",
      "Second call:\n",
      "Usage:  {'inputTokens': 11, 'outputTokens': 2, 'totalTokens': 4014, 'cacheReadInputTokens': 4001, 'cacheWriteInputTokens': 0}\n",
      "Latency: 247 (51% faster)\n",
      "Cost for 1000 of these calls: 0.8154000000000001 (75% cheaper)\n"
     ]
    }
   ],
   "source": [
    "cache_query = {}\n",
    "\n",
    "#caching only works for prompts longer than 1K tokens, so lets make a long prompt.\n",
    "long_string = \"I love tacos! \" * 1000\n",
    "\n",
    "prompt = create_message_json([(\"user\",long_string),(\"cachePoint\",\"default\"),(\"user\",\"What do I love?  Respond with a single word.\")])\n",
    "cache_query['prompt'] = prompt\n",
    "cache_query['model'] = 'US Nova Pro'\n",
    "ask_bedrock(cache_query)\n",
    "first_call_latency = cache_query[\"latencyMs\"]\n",
    "first_call_cost = cache_query[\"cost\"]\n",
    "print (\"First call:\")\n",
    "print (\"Usage: \",cache_query[\"usage\"])\n",
    "print (\"Latency: \",first_call_latency)\n",
    "print (\"Cost for 1000 of these calls: \", first_call_cost*1000)\n",
    "ask_bedrock(cache_query)\n",
    "second_call_latency = cache_query[\"latencyMs\"]\n",
    "second_call_cost = cache_query[\"cost\"]\n",
    "print (\"Second call:\")\n",
    "print (\"Usage: \",cache_query[\"usage\"])\n",
    "print (f\"Latency: {second_call_latency} ({round((first_call_latency-second_call_latency)/first_call_latency*100)}% faster)\")\n",
    "print (f\"Cost for 1000 of these calls: {second_call_cost*1000} ({round((first_call_cost-second_call_cost)/first_call_cost*100)}% cheaper)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e9e2ba7c-eb11-48d0-80bf-7486a5d16b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model US Anthropic Claude 3 Sonnet\n",
      "system None\n",
      "tools None\n",
      "modelID us.anthropic.claude-3-sonnet-20240229-v1:0\n",
      "maxTokens None\n",
      "stopSequences []\n",
      "temperature 0.5\n",
      "output An error occurred (AccessDeniedException) when calling the Converse operation: You invoked an unsupported model or your request did not allow prompt caching. See the documentation for more information.\n",
      "stopReason ERROR\n"
     ]
    }
   ],
   "source": [
    "for key in cache_query:\n",
    "    if key == 'prompt':continue\n",
    "    print (key,cache_query[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920fbc60-6d70-4336-83f0-c85a49c53324",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3f) Basic Tool Use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf337ca8-add2-4c5c-b587-6632cf80eb7f",
   "metadata": {},
   "source": [
    "#### Start by defining our tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a1231b9-7b65-4d9b-9f5c-7f5e7e07a6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the actual tool:\n",
    "def example_weather_tool(time_of_day):\n",
    "    if time_of_day == 'AM':\n",
    "        return \"Sunny\"\n",
    "    if time_of_day == 'PM':\n",
    "        return \"Rainy\"\n",
    "    else:\n",
    "        return \"Error\"\n",
    "\n",
    "#the config so that the model knows about this tool\n",
    "example_tool_config = [\n",
    "    {\n",
    "        \"toolSpec\": {\n",
    "            \"name\": \"weather\",\n",
    "            \"description\": \"Get the local weather.\",\n",
    "            \"inputSchema\": {\n",
    "                \"json\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"time_of_day\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The time of day to get weather for, either AM or PM.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"time_of_day\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb88131-5aa2-4ca7-8790-5d886d253fba",
   "metadata": {},
   "source": [
    "#### Now we make a call where the model may want to use a tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5fd04f1c-e57a-4257-a41e-fc0010e20cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool requested: weather\n",
      "Tool input: {'time_of_day': 'AM'}\n"
     ]
    }
   ],
   "source": [
    "#create a list of one or more tools.  ask_clade will package this into the proper call format.\n",
    "query = {}\n",
    "query[\"tools\"] = example_tool_config\n",
    "msg_1 = \"What is the weather this morning?\"\n",
    "query[\"prompt\"] = msg_1\n",
    "ask_bedrock(query)\n",
    "print (\"Tool requested:\",query['toolName'])\n",
    "print (\"Tool input:\",query['toolInput'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eb4c00-db57-4ec1-865f-e3aa61c3f42f",
   "metadata": {},
   "source": [
    "#### Next, call the tool as requested by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5dfbd344-b4f9-4441-aed9-980378974b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_result = example_weather_tool(query['toolInput']['time_of_day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c96f610-10b1-46aa-a046-f2d8f9977a11",
   "metadata": {},
   "source": [
    "#### Finally, return the tool's response to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af56ef9a-8302-4de2-80aa-04fbf6c5d61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<thinking> The weather tool has provided the result that it is sunny this morning. I can now provide this information to the User. </thinking>\n",
      "The weather this morning is sunny.\n"
     ]
    }
   ],
   "source": [
    "tool_request_from_claude = query['output']\n",
    "message_list = []\n",
    "message_list.append([\"user\",msg_1])\n",
    "message_list.append([\"tool_request\",query['toolUseId'],query['toolName'],query['toolInput']])\n",
    "message_list.append(['tool_result',query['toolUseId'],tool_result])\n",
    "\n",
    "query_2 = {}\n",
    "#when passing the tool results to the model, it still needs to understand to original tool.\n",
    "query_2[\"tools\"] = example_tool_config\n",
    "#change our message history into the Converse API nested JSON format.\n",
    "prompt = create_message_json(message_list)\n",
    "query_2['prompt'] = prompt\n",
    "\n",
    "#send the full message history, tool use request, and tool use response to Claude so that it can answer the original question.\n",
    "ask_bedrock(query_2)\n",
    "print (query_2['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c23746-d211-4b85-9129-2f8d742dc3f3",
   "metadata": {},
   "source": [
    "### 3g) Bring it all together - Simple Chat with web search enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9892b16-e876-48d9-9fee-0ad18fef3609",
   "metadata": {},
   "source": [
    "This is a bare bones chat bot which can search the web as needed.  It doesn't use any framework or gui, just the native Bedrock API.  The intent is to show the most simply way to set this up with no framework or agents required, in order to provide a building block for more complicated applications.\n",
    "#### Start by defining our tools.\n",
    "For this example, we use a simple google search library for web searching. We then create two tools.  The first is a web search, that takes a topic and returns a list of search results.  The second is a simple raw text page retrieval, so that the LLM can look at the search results, and then pick a page from the results list and retrieve it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "06b89347-5fa2-4c7d-80b4-e7980dc23c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup #for parsing HTML\n",
    "import requests, json, lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "eeb4e70c-743e-447e-97fc-e3c9375e1874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googlesearch-python\n",
      "  Downloading googlesearch_python-1.3.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.9 in /opt/conda/lib/python3.11/site-packages (from googlesearch-python) (4.12.3)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/conda/lib/python3.11/site-packages (from googlesearch-python) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.20->googlesearch-python) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.20->googlesearch-python) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.20->googlesearch-python) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.20->googlesearch-python) (2024.7.4)\n",
      "Downloading googlesearch_python-1.3.0-py3-none-any.whl (5.6 kB)\n",
      "Installing collected packages: googlesearch-python\n",
      "Successfully installed googlesearch-python-1.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install googlesearch-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "1f8247d3-5ec4-48c1-a92d-bd54d443d207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "11b23645-e72c-4c4a-b84a-779b0c121015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SearchResult(url=https://en.wikipedia.org/wiki/Taco, title=Taco - Wikipedia, description= According to one etymological theory, the culinary origin of the term \"taco\" in Mexico can be traced to its employment, among Mexican silver miners, as a term ... )SearchResult(url=https://www.twistedtaco.com/the-history-of-the-taco, title=History Of The Taco, description= The Name “Taco”: A Modern Invention   The word \"taco\" is quite new. It originated from Mexican silver miners in the 18 th century. Gunpowder was wrapped in a ... )SearchResult(url=https://www.smithsonianmag.com/arts-culture/where-did-the-taco-come-from-81228162/, title=Where Did the Taco Come From? - Smithsonian Magazine, description= May 3, 2012  ·  The word “taco” in a restaurant name was actually a way of selling Mexican food to non-Mexicans. What Glen Bell was doing was allowing Americans ... )SearchResult(url=https://www.britannica.com/topic/taco, title=Taco | Definition, Origins, Ingredients, & Types - Britannica, description= The name taco may come from the Spanish word for dowel, as in a plug to fill a hungry stomach, or, perhaps likelier, from the Nahuatl word tlacoyo, the name of ... )SearchResult(url=https://unocasa.com/blogs/tips/history-of-tacos, title=The History of Tacos: Ancient and Modern - Uno Casa, description= The origin of the word taco comes from the Nahuatl's “tlahco,” translating to “half, or in the middle” in English, describing the way we fold this tasty ... )\n"
     ]
    }
   ],
   "source": [
    "def web_search(topic):\n",
    "    \"\"\" This tool searches Google for a topic, and returns a string of the top 5 results deliniating the url, title, and description of each result.\n",
    "    \"\"\"\n",
    "    result_string = \"\"\n",
    "    results = search(topic, num_results=5, advanced=True)\n",
    "    for result in results:\n",
    "        result_string += str(result)\n",
    "\n",
    "    return result_string\n",
    "\n",
    "Example_Web_Search = True\n",
    "if Example_Web_Search:\n",
    "    results = run_requested_tool(query)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d3296f6c-b351-46ee-9fa3-4b9befe69733",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taco - WikipediaJump to contentMain menuMain menumove to sidebarhide\t\tNavigation\tMain pageContentsCurrent eventsRandom articleAbout WikipediaContact us\t\tContribute\tHelpLearn to editCommunity portalRecent changesUpload fileSpecial pagesSearchSearchAppearanceDonateCreate accountLog inPersonal toolsDonate Create account Log in\t\tPages for logged out editors learn moreContributionsTalkContentsmove to sidebarhide(Top)1Etymology2History3Traditional variations4Non-traditional variationsToggle Non-traditional variations subsection4.1Hard-shell tacos4.2Soft-shell tacos4.3Breakfast taco4.4Indian taco4.5Puffy tacos, taco kits, and tacodillas5See also6References7Bibliography8External linksToggle the table of contentsTaco64 languagesAfrikaansالعربيةAzərbaycancaБеларускаяБеларуская (тарашкевіца)БългарскиCatalàČeštinaCorsuCymraegDanskDeutschEestiΕλληνικάEspañolEsperantoEuskaraفارسیFrançaisGalego한국어ՀայերենHrvatskiBahasa IndonesiaItalianoעבריתJawaಕನ್ನಡLatinaLietuviųMagyarМакедонскиमराठीBahasa Melayuမြန်မာဘာသာNederlandsनेपालीनेपाल भाषा日本語Norsk bokmålNorsk nynorskOʻzbekcha / ўзбекчаPolskiPortuguêsRomânăРусскийSimple EnglishСрпски / srpskiSuomiSvenskaTagalogதமிழ்Татарча / tatarçaไทยTürkçeУкраїнськаاردوئۇيغۇرچە / UyghurcheTiếng ViệtWinaray粵語中文KadazandusunKumoringEdit linksArticleTalkEnglishReadView sourceView historyToolsToolsmove to sidebarhide\t\tActions\tReadView sourceView history\t\tGeneral\tWhat links hereRelated changesUpload filePermanent linkPage informationCite this pageGet shortened URLDownload QR code\t\tPrint/export\tDownload as PDFPrintable version\t\tIn other projects\tWikimedia CommonsWikibooksWikidata itemAppearancemove to sidebarhideFrom Wikipedia, the free encyclopediaMexican filled tortilla dishFor other uses, see Taco (disambiguation).TacoThree varieties of taco (clockwise from left): carnitas, carne asada, and al pastor.  As is traditional, they are garnished simply with cilantro (fresh coriander) and chopped onion, and served with lime on the side for seasoning according to the diner's taste.TypeFinger foodPlace of originMexicoMain ingredientsTortillaProtein (animal or vegetable)Ingredients generally usedVegetablesCheeseSauces Cookbook: Taco  Media: TacoA taco (US: /ˈtɑːkoʊ/, UK: /ˈtækoʊ/, Spanish: [ˈtako]) is a traditional Mexican dish consisting of a small hand-sized corn- or wheat-based tortilla topped with a filling. The tortilla is then folded around the filling and eaten by hand. A taco can be made with a variety of fillings, including beef, pork, chicken, seafood, beans, vegetables, and cheese, and garnished with various condiments, such as salsa, guacamole, or sour cream, and vegetables, such as lettuce, coriander, onion, tomatoes, and chiles.[1] Tacos are a common form of antojitos, or Mexican street food, which have spread around the world.[2]Tacos can be contrasted with similar foods such as burritos, which are often much larger and rolled rather than folded; taquitos, which are rolled and fried; or chalupas/tostadas, in which the tortilla is fried before filling.EtymologyThe origins of the taco are not precisely known, and etymologies for the culinary usage of the word are generally theoretical.[3][4] Taco in the sense of a typical Mexican dish comprising a maize tortilla folded around food is just one of the meanings connoted by the word, according to the Real Academia Española, publisher of Diccionario de la Lengua Española.[5] This meaning of the Spanish word \"taco\" is a Mexican innovation,[4] but the word \"taco\" is used in other contexts to mean \"wedge; wad, plug; billiard cue; blowpipe; ramrod; short, stocky person; [or] short, thick piece of wood.\"[5] The etymological origin of this sense of the word is Germanic and has cognates in other European languages, including the French word tache and the English word \"tack\".[6]In Spain, the word \"taco\" can also be used in the context of tacos de jamón [es]:  these are diced pieces of ham, or sometimes bits and shavings of ham leftover after a larger piece is sliced.[7] They can be served on their own as tapas or street food, or can be added to other dishes such as salmorejo, omelettes, stews, empanadas, or melón con jamón [es].[8][9][10]According to one etymological theory, the culinary origin of the term \"taco\" in Mexico can be traced to its employment, among Mexican silver miners, as a term signifying \"plug.\" The miners used explosive charges in plug form, consisting of a paper wrapper and gunpowder filling.[3]Indigenous origins are also \n"
     ]
    }
   ],
   "source": [
    "def get_page(url):\n",
    "    \"\"\"this function takes a URL and returns the raw text from that page.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    bs = BeautifulSoup(response.text,'html.parser')\n",
    "    return bs.text\n",
    "\n",
    "Example_Get_Page = True\n",
    "if Example_Get_Page:\n",
    "    page = get_page(\"https://en.wikipedia.org/wiki/Taco\")\n",
    "    print (page[:5000].replace(\"\\n\",\"\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "356ad403-2b3d-4f13-a8f1-a76079bc3606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#and a quick helper function, to run the tool requested by the LLM\n",
    "def run_requested_tool(query):\n",
    "    tool_result = \"ERROR\"\n",
    "    if query['toolName'] == \"web_search\":\n",
    "        tool_result = web_search(query['toolInput']['topic'])\n",
    "    if query['toolName'] == \"get_page\":\n",
    "        tool_result = web_search(query['toolInput']['url'])\n",
    "    return tool_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e6f2f0-1b2d-4a70-b978-da1d34253d0f",
   "metadata": {},
   "source": [
    "#### Next, we set up the tool config for these two tools, so that the model knows how to call them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "4caba18d-b538-4be7-98cf-97910d15e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the config so that the model knows about the tools\n",
    "web_search_tool_config = [\n",
    "    {\n",
    "        \"toolSpec\": {\n",
    "            \"name\": \"web_search\",\n",
    "            \"description\": \"Search the web for a given topic.\",\n",
    "            \"inputSchema\": {\n",
    "                \"json\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"topic\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The topic to search for.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"topic\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"toolSpec\": {\n",
    "            \"name\": \"get_page\",\n",
    "            \"description\": \"Get the raw text from a web page\",\n",
    "            \"inputSchema\": {\n",
    "                \"json\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"url\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The URL of the page to retrieve.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"url\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f09a3e-0ad2-4487-b082-cbb965e85069",
   "metadata": {},
   "source": [
    "#### Next, we create a system prompt to explain to the bot what it's job is, and how to use the tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "48656dfb-d0f8-4252-be90-19bd628fac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_chat_system_prompt = \"\"\"\n",
    "You are a helpful AI assistant, designed for web based reseach.  \n",
    "You can chat with a user, but if the user asks for any kind of information, you need to get that information using the tools available to you.  \n",
    "\n",
    "First, you send a relevant topic to the web_search tool, and you will revieve a simplifed version of the search results page with HTML tags removed.  \n",
    "Based on the search results, you will pick one of the results, and send that URL to the get_page tool.  That tool will give you the raw text from that page.\n",
    "\n",
    "If you can answer based on the information from the page, then do so.  If not, try another URL from the search results, or else try searching for a different topic.\n",
    "\n",
    "When responding to the user, if tools were used be sure to cite your source, including the title and URL of the page used.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "beb361a8-82f7-4077-a56b-f3008811050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_a_tool(thinking_thread,tool_query,SHOW_TOOL_USE):\n",
    "    if SHOW_TOOL_USE: print (f\"**SYSTEM MESSAGE** Calling tool: {tool_query['toolName']} with parameters: {tool_query['toolInput']}\")\n",
    "    tool_result = run_requested_tool(tool_query)\n",
    "\n",
    "    #add the tool request and responce to our message history\n",
    "    thinking_thread.append([\"tool_request\",query['toolUseId'],query['toolName'],query['toolInput']])\n",
    "    thinking_thread.append(['tool_result',query['toolUseId'],tool_result])\n",
    "    tool_prompt = create_message_json(thinking_thread)\n",
    "\n",
    "    #send the results back to the LLM\n",
    "    thinking_query = {}\n",
    "    thinking_query[\"system\"] = tool_query[\"system\"]\n",
    "    thinking_query[\"tools\"] = tool_query[\"tools\"]['tools']#unpack so that our auto pack doesn't double nest.\n",
    "    thinking_query['prompt'] = tool_prompt\n",
    "    ask_bedrock(thinking_query)\n",
    "\n",
    "    #check to see if we're done, or need another tool call\n",
    "    if thinking_query['stopReason'] == 'tool_use':\n",
    "        return use_a_tool(thinking_thread[:],thinking_query,SHOW_TOOL_USE)\n",
    "    else:\n",
    "         return thinking_query['output']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "b17a77c6-1291-47f3-8af3-11aecbb43925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please start the conversation: (enter STOP to end)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: What is a taco?  Please do not explain your process, merely give an answer and cite your sources.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: A taco is a traditional Mexican dish consisting of a small hand-sized corn- or wheat-based tortilla topped with a filling. The tortilla is then folded around the filling. \n",
      "\n",
      "Source: \"Taco - Wikipedia\"\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: What was the URL for that source?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The URL for the source is \"https://en.wikipedia.org/wiki/Taco\". The title of the page used for the result is \"Taco - Wikipedia\". The explanation provided is based on the description from the Wikipedia page.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: Thanks!  Have a good day.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: You're welcome! If you have any more questions in the future, feel free to ask. Have a great day!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: STOP\n"
     ]
    }
   ],
   "source": [
    "conversation_history = []\n",
    "SHOW_TOOL_USE = False #show the internal tool use steps as part of the conversation.\n",
    "\n",
    "print (\"Please start the conversation: (enter STOP to end)\")\n",
    "while True:\n",
    "    user_input = input(\"User:\")\n",
    "    if user_input == \"STOP\": break\n",
    "    conversation_history.append(['user',user_input])\n",
    "    query = {}\n",
    "    query[\"system\"] = tool_chat_system_prompt\n",
    "    query[\"tools\"] = web_search_tool_config\n",
    "    query['prompt'] = create_message_json(conversation_history)\n",
    "    ask_bedrock(query)\n",
    "\n",
    "    if query['stopReason'] == 'tool_use':\n",
    "        query['output'] = use_a_tool(conversation_history[:],query,SHOW_TOOL_USE)\n",
    "    \n",
    "    response = query['output']\n",
    "    print (\"Assistant:\",response)\n",
    "    conversation_history.append(['assistant',response])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975c730a-c851-4ca0-a845-278f1de08091",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
