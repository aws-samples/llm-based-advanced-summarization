{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4c5ad2e-7475-48e1-a34c-127e28bb3674",
   "metadata": {},
   "source": [
    "# Basic Anthropic Claude 3 on Bedrock\n",
    "This notebook contains a collection of basic helper functions which are useful for connecting to Bedrock.  Varations of these are used for many other code samples and use cases.\n",
    "\n",
    "The helper functins include:\n",
    "  * Setting up a connection with Bedrock including longer connection timeout times.\n",
    "  * Cost calculation, which converts the token counts to dollars based on public pricing.\n",
    "  * ask_claude, a simple way to send prompts to Claude\n",
    "  * ask_claude_threaded, a simple way to send multiple prompts at the same time\n",
    "  * evaluate_prompt, a simple way to test a prompt against a set of gold standard input/output pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4e65081-5f5b-4153-97b8-333df3be2332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for connecting with Bedrock, use Boto3\n",
    "import boto3, time, json\n",
    "from botocore.config import Config\n",
    "\n",
    "#increase the standard time out limits in boto3, because Bedrock may take a while to respond to large requests.\n",
    "my_config = Config(\n",
    "    connect_timeout=60*5,\n",
    "    read_timeout=60*5,\n",
    ")\n",
    "bedrock = boto3.client(service_name='bedrock-runtime',config=my_config)\n",
    "bedrock_service = boto3.client(service_name='bedrock',config=my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6675b967-5e61-48f0-bced-2faba4fa89eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claud-v3 found!\n"
     ]
    }
   ],
   "source": [
    "#check that it's working:\n",
    "models = bedrock_service.list_foundation_models()\n",
    "for line in models[\"modelSummaries\"]:\n",
    "    #print (line[\"modelId\"])\n",
    "    pass\n",
    "if \"anthropic.claude-3\" in str(models):\n",
    "    print(\"Claud-v3 found!\")\n",
    "else:\n",
    "    print (\"Error, no model found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ba055bcc-f414-45ef-a0f9-4b4a6a3b902b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#helper function for converting tokens to public pricing for Claude.\n",
    "input_token_haiku = 0.25/1000000\n",
    "output_token_haiku = 1.25/1000000\n",
    "input_token_sonnet = 3.00/1000000\n",
    "output_token_sonnet = 15.00/1000000\n",
    "input_token_opus = 15.00/1000000\n",
    "output_token_opus = 75.00/1000000\n",
    "def calculate_cost(usage, model):\n",
    "    '''\n",
    "    Takes the usage tokens returned by Bedrock in input and output, and coverts to cost in dollars.\n",
    "    '''\n",
    "    cost = 0\n",
    "    if model=='haiku':\n",
    "        cost+= usage['input_tokens']*input_token_haiku\n",
    "        cost+= usage['output_tokens']*output_token_haiku\n",
    "    if model=='sonnet':\n",
    "        cost+= usage['input_tokens']*input_token_sonnet\n",
    "        cost+= usage['output_tokens']*output_token_sonnet\n",
    "    if model=='opus':\n",
    "        cost+= usage['input_tokens']*input_token_opus\n",
    "        cost+= usage['output_tokens']*output_token_opus\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0b3a2e45-43b6-4ac4-8240-5dc3b58ca220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_ATTEMPTS = 1 #how many times to retry if Claude is not working.\n",
    "session_cache = {} #all calls are stored in the cache.  \n",
    "def ask_claude(messages,system=\"\", model=\"haiku\", ignore_cache=False, DEBUG=False):\n",
    "    '''\n",
    "    Send a prompt to Bedrock, and return the response.\n",
    "    messages can be an array of role/message pairs, or a string.\n",
    "    DEBUG is used to see exactly what is being sent to and from Bedrock.\n",
    "    model can be haiku or sonnet\n",
    "    Set ignore_cache to True if you want to force a new call to Bedrock\n",
    "    '''\n",
    "    raw_system_prompt_text = system+str(messages)\n",
    "    raw_prompt_text = str(messages)\n",
    "    \n",
    "    #if the messages are just a string, convert to the Messages API format.\n",
    "    if type(messages)==str:\n",
    "        messages = [{\"role\": \"user\", \"content\": messages}]\n",
    "    \n",
    "    #build the JSON to send to Bedrock\n",
    "    prompt_json = {\n",
    "        \"system\":system,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 4096, # 4096 is a hard limit to output length in Claude 3\n",
    "        \"temperature\": 0.5, #creativity on a scale from 0-1.\n",
    "        \"anthropic_version\":\"\",\n",
    "        \"top_k\": 250,\n",
    "        \"top_p\": 0.7,\n",
    "        \"stop_sequences\": [\"\\n\\nHuman:\"]\n",
    "    }\n",
    "    \n",
    "    \n",
    "    if DEBUG: print(\"Sending:\\nSystem:\\n\",system,\"\\nMessages:\\n\",str(messages))\n",
    "    \n",
    "    #pick the correct endpoint for the model we want to use.\n",
    "    if model== \"opus\":\n",
    "        modelId = 'error'\n",
    "    elif model== \"sonnet\":\n",
    "        modelId = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "    elif model== \"haiku\":\n",
    "        modelId = 'anthropic.claude-3-haiku-20240307-v1:0'\n",
    "    else:\n",
    "        print (\"ERROR:  Bad model, must be opus, sonnet, or haiku.\")\n",
    "        modelId = 'error'\n",
    "    \n",
    "    #if this is already in the cashe, return data from the cache and skip Bedrock.\n",
    "    if raw_system_prompt_text in session_cache and not ignore_cache:\n",
    "        if DEBUG: print (\"Using results from cache, skipping Bedrock.\")\n",
    "        cached = session_cache[raw_system_prompt_text]\n",
    "        return [raw_prompt_text,cached[0],cached[1],cached[2],cached[3]]\n",
    "    \n",
    "    attempt = 1\n",
    "    query_time = -1\n",
    "    usage = (-1,-1)\n",
    "    while True:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = bedrock.invoke_model(body=json.dumps(prompt_json), modelId=modelId, accept='application/json', contentType='application/json')\n",
    "            response_body = json.loads(response.get('body').read())\n",
    "            #print (response_body)\n",
    "            results = response_body.get(\"content\")[0].get(\"text\")\n",
    "            usage = response_body.get(\"usage\")\n",
    "            query_time = round(time.time()-start_time,2)\n",
    "            if DEBUG:print(\"Recieved:\",results)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"Error with calling Bedrock: \"+str(e))\n",
    "            attempt+=1\n",
    "            if attempt>MAX_ATTEMPTS:\n",
    "                print(\"Max attempts reached!\")\n",
    "                results = str(e)\n",
    "                break\n",
    "            else:#retry in 10 seconds\n",
    "                time.sleep(10)\n",
    "    session_cache[raw_system_prompt_text] = [results,usage,query_time,system]\n",
    "    return [raw_prompt_text,results,usage,query_time,system]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f002e219-1bf7-4190-9b5e-0ca5acb28fc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Instructions:  You always reply in Spanish.\n",
      "Prompt:  Please say the number four.\n",
      "Response:  Cuatro.\n",
      "{'input_tokens': 19, 'output_tokens': 7}\n",
      "Query time:  0.42 seconds\n"
     ]
    }
   ],
   "source": [
    "#check that it's working:\n",
    "try:\n",
    "    query = \"Please say the number four.\"\n",
    "    system = \"You always reply in Spanish.\"\n",
    "    result = ask_claude(query,system=system,ignore_cache=True,DEBUG=False)\n",
    "    print(\"System Instructions: \",result[4])\n",
    "    print(\"Prompt: \",query)\n",
    "    print(\"Response: \",result[1])\n",
    "    print(result[2])\n",
    "    print(\"Query time: \",result[3],\"seconds\")\n",
    "except Exception as e:\n",
    "    print(\"Error with calling Claude: \"+str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8b5cc5cd-44c3-494f-8d1a-7d68f5ab9877",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_tokens': 19, 'output_tokens': 7}\n",
      "Cost for running this query 1 million times: $ 13.5\n"
     ]
    }
   ],
   "source": [
    "#check that cost calculation is working\n",
    "print(result[2])\n",
    "cost = calculate_cost(result[2], 'haiku')\n",
    "print(\"Cost for running this query 1 million times: $\",cost*1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "36d36a19-d377-474d-a046-4584b313c60c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "# Threaded function for queue processing.\n",
    "def thread_request(q, result):\n",
    "    while not q.empty():\n",
    "        work = q.get()    #fetch new work from the Queue\n",
    "        try:\n",
    "            data = ask_claude(work[1],system=work[2],model=work[3],ignore_cache=work[4])\n",
    "            result[work[0]] = data  #Store data back at correct index\n",
    "        except Exception as e:\n",
    "            print('Error with prompt!',str(e))\n",
    "            result[work[0]] = (str(e))\n",
    "        #signal to the queue that task has been processed\n",
    "        q.task_done()\n",
    "    return True\n",
    "\n",
    "def ask_claude_threaded(prompts,system=\"\",model=\"haiku\",ignore_cache=False):\n",
    "    '''\n",
    "    Call ask_claude, but multi-threaded.\n",
    "    Returns a dict of the prompts and responces.\n",
    "    '''\n",
    "    q = Queue(maxsize=0)\n",
    "    num_theads = min(50, len(prompts))\n",
    "    #Populating Queue with tasks\n",
    "    results = [{} for x in prompts];\n",
    "    #load up the queue with the promts to fetch and the index for each job (as a tuple):\n",
    "    for i in range(len(prompts)):\n",
    "        #need the index and the url in each queue item.\n",
    "        q.put((i,prompts[i],system,model,ignore_cache))\n",
    "        \n",
    "    #Starting worker threads on queue processing\n",
    "    for i in range(num_theads):\n",
    "        #print('Starting thread ', i)\n",
    "        worker = Thread(target=thread_request, args=(q,results))\n",
    "        worker.daemon = True\n",
    "        worker.start()\n",
    "\n",
    "    #now we wait until the queue has been processed\n",
    "    q.join()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ecc70b33-ec10-49e3-8d5f-0cdcde538bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"[{'role': 'user', 'content': 'Please say the number one.'}]\", 'Uno.', {'input_tokens': 18, 'output_tokens': 6}, 0.23, 'you only reply in spanish']\n",
      "[\"[{'role': 'user', 'content': 'Please say the number two.'}]\", 'Dos.', {'input_tokens': 18, 'output_tokens': 6}, 0.34, 'you only reply in spanish']\n",
      "[\"[{'role': 'user', 'content': 'Please say the number three.'}]\", 'Tres.', {'input_tokens': 18, 'output_tokens': 6}, 0.33, 'you only reply in spanish']\n",
      "CPU times: user 21.2 ms, sys: 0 ns, total: 21.2 ms\n",
      "Wall time: 346 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#test if our threaded Claude calls are working\n",
    "q1 = [{\"role\": \"user\", \"content\": \"Please say the number one.\"}]\n",
    "q2 = [{\"role\": \"user\", \"content\": \"Please say the number two.\"}]\n",
    "q3 = [{\"role\": \"user\", \"content\": \"Please say the number three.\"}]\n",
    "results = ask_claude_threaded([q1,q2,q3],system=\"you only reply in spanish\",model='haiku',ignore_cache=True)\n",
    "for r in results:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ff4846-c795-41af-ae20-69adfe402198",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluation Function: evaluate_prompt()\n",
    "Our final piece of setup is an evaluation function.  It is presented in a compact form here, but please see this blog for a more complete explanation of this critial step.  This takes a prompt template to test, and a dictonary of input/output pairs, and provides an accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "61274e2b-4c2c-4966-a1d1-d5db5254de3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scoring_prompt_template = \"\"\"You are a teacher.  Consider the following question along with its correct answer and a student submitted answer.\n",
    "Here is the question:\n",
    "<question>{{QUESTION}}</question>\n",
    "Here is the correct answer:\n",
    "<correct_answer>{{ANSWER}}</correct_answer>\n",
    "Here is the student's answer:\n",
    "<student_answer>{{TEST_ANSWER}}</student_answer>\n",
    "Please provide a score from 0 to 100 on how well the student answer matches the correct answer for this question.\n",
    "The score should be high if the answers say essentially the same thing.\n",
    "The score should be lower if some facts are missing or incorrect, or if extra unnecessary facts have been included.\n",
    "The score should be 0 for entirely wrong answers.  Put the score in <SCORE> tags. and your reasoning in <REASON> tags.\n",
    "Do not consider your own answer to the question, but instead score based on the correct_answer above.\"\"\"\n",
    "\n",
    "def score_answers(prompt_template, input_output, system):\n",
    "    '''\n",
    "    ask our LLM to score each of the generated answers.\n",
    "    '''\n",
    "    print (\"Generating results to score...\")\n",
    "    prompts = []\n",
    "    for i in input_output:\n",
    "        prompts.append(prompt_template.replace(\"{{QUESTION}}\",i))\n",
    "    answers_to_test = ask_claude_threaded(prompts, system=system)\n",
    "    print (\"Done.  Scoring answers...\")\n",
    "    \n",
    "    \n",
    "    #pack answers with questions in templated form.\n",
    "    question_answers_with_template = {}\n",
    "    question_with_template_to_questions = {}\n",
    "    for question in input_output:\n",
    "        question_answers_with_template[prompt_template.replace(\"{{QUESTION}}\",question)] = input_output[question]\n",
    "        question_with_template_to_questions[prompt_template.replace(\"{{QUESTION}}\",question)]=question\n",
    "    \n",
    "    prompts = []\n",
    "    for question,test_answer,usage,query_time,system in answers_to_test:\n",
    "        original_question = question_with_template_to_questions[question]\n",
    "        correct_answer = question_answers_with_template[question]\n",
    "        prompts.append(scoring_prompt_template.replace(\"{{QUESTION}}\",original_question).replace(\"{{ANSWER}}\",correct_answer).replace(\"{{TEST_ANSWER}}\",test_answer))\n",
    "\n",
    "    return ask_claude_threaded(prompts)\n",
    "\n",
    "from bs4 import BeautifulSoup as BS\n",
    "\n",
    "def evaluate_prompt(prompt_template, question_answers, threshhold, system=\"\", print_out=False):\n",
    "    \"\"\"\n",
    "    Call score answers and format the results once all threads have returned.\n",
    "    \"\"\"\n",
    "    scored_answers = score_answers(prompt_template, question_answers, system)\n",
    "    print (\"Done.\")\n",
    "    #pack questions to templated form\n",
    "    question_with_template_to_questions = {}\n",
    "    for question in question_answers:\n",
    "        question_with_template_to_questions[prompt_template.replace(\"{{QUESTION}}\",question)]=question\n",
    "    \n",
    "    scores = []\n",
    "    total_scored = 0\n",
    "    total_passed = 0\n",
    "    for prompt,response,usage,query_time,system in scored_answers:\n",
    "        soup = BS(prompt)\n",
    "        question = soup.find('question').text\n",
    "        correct_answer = soup.find('correct_answer').text\n",
    "        prompt_answer = soup.find('student_answer').text\n",
    "        soup = BS(response)\n",
    "        score = soup.find('score').text\n",
    "        reason = soup.find('reason').text\n",
    "        passed = True\n",
    "        \n",
    "        if int(score)<threshhold:\n",
    "            passed = False\n",
    "            \n",
    "        #keep track for printing locally\n",
    "        total_scored+=1\n",
    "        if passed: total_passed+=1\n",
    "        \n",
    "        scores.append([question,correct_answer,prompt_answer,score,reason,passed])\n",
    "    if print_out:\n",
    "        print(\"Total inputs:\",total_scored)\n",
    "        print(\"Total Correct:\",total_passed)\n",
    "        print(\"Accuracy:\",round(total_passed/total_scored,2)*100,\"%\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "28381b3a-41c6-4457-bae5-ca755bea84a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating results to score...\n",
      "Done.  Scoring answers...\n",
      "Done.\n",
      "Total inputs: 5\n",
      "Total Correct: 1\n",
      "Accuracy: 20.0 %\n"
     ]
    }
   ],
   "source": [
    "#run a quick test to make sure evaluation is working:\n",
    "inputs_outputs = {\n",
    " \"What is heavier, 1kg of feathers or 1kg of iron?\":\"They are the same.\",\n",
    " \"What is my current bank account balance?\":\"I don't have access to that information.\",\n",
    " \"Who was the president in the year 2000?\":\"Bill Clinton\",   \n",
    " \"A boy runs down the stairs in the morning and sees a tree in his living room, and some boxes under the tree. What's going on?\":\"It is Christmas.\",\n",
    " \"If I hang 5 shirts outside and it takes them 5 hours to dry, how long would it take to dry 30 shirts?\":\"5 hours.\"\n",
    "}\n",
    "system = \"you always keep your reply as short as possible.\"\n",
    "test_prompt = \"You are a boat fanatic and always talk like a pirate.  You do answer questions, but you also always include a fun fact about boats.  Please answer this question:{{QUESTION}}\"\n",
    "scores = evaluate_prompt(test_prompt, inputs_outputs, threshhold=90, system=system, print_out=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785edc91-6184-429f-9420-ce6bfdadac0b",
   "metadata": {},
   "source": [
    "## 2a) Task Based Decomposition\n",
    "For this example, let's consider the use case of a marketing copy editor who is reviewing hundreds of AWS blogs.  It is important to AWS branding that all services mentioned in our blogs are referred to by the correct name.  AWS has hundreds of services, and each one has a marketing approved name, which may vary depending on the context.  In general, the guidelines ask that services are referred to their full name when first mentioned, and then can be referred to by a shortened version after that.  \n",
    "\n",
    "For our use case, imagine that you have hundreds of pages of text that needs to be reviewed for compliance against this set of rules.  We will explore doing this in a single prompt and measure the cost, latency, and accuracy of that approach.  Next, we will decompose this task into multiple steps, and then compare the difference in cost, latency, and accuracy.\n",
    "\n",
    "Like every good Generative AI project, we will start by setting up our test cases, so that we can measure the impact and quality of everything else we do.  (More information on the prompt evaluation used here can be found in [this blog](https://medium.com/@flux07/prompt-evaluation-systematically-testing-and-improving-your-gen-ai-prompts-at-scale-784e54efe83d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13649fa-22a4-435e-aa2b-88b8b67a60e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gold standard test cases.  The format is \"input\":\"Correct output\"\n",
    "test_cases = {\n",
    "    \"What do you think about SQS?\":\"This violoates the AWS Mareting policy.  The first mention of \\\"SQS\\\" should be \\\"Amazon Simple Notification Service (Amazon SNS)\\\".\",\n",
    "    \"Things to think about. Amazon FinSpace: Company X tried FinSpace and it worked.\":\"\",\n",
    "    \"I like to use AWS HealthLake, but sometimes HealthLake does not like me Although HealthLake does work.\":\"\",\n",
    "    \"Amazon Simple Notification Service (Amazon SNS): CompanyX used Amazon SNS to fanout messages to multiple recipients for parallel processing. SNS allowed them to easily distribute messages to different systems and applications that needed to take action based on the fulfillment process.\":\"\",\n",
    "    \"\":\"\",\n",
    "    \"\":\"\",\n",
    "    \"\":\"\",\n",
    "    \"\":\"\",\n",
    "    \"\":\"\",\n",
    "    \"\":\"\"\n",
    "}\n",
    "\n",
    "test_cases = {\n",
    "    \"What do you think about SQS?\":[\"The first mention of \\\"SQS\\\" must be \\\"Amazon Simple Queue Service (Amazon SQS)\\\".\"],\n",
    "    \"Amazon EMR's newest feature is great\":[\"The possessive form of \\\"EMR\\\" is not allowed\"],\n",
    "    \"The service Amazon FinSpace: Company X tried FinSpace and it worked.\":[], # Correct usage so no output\n",
    "    \"I like to use Amazon HealthLake, but sometimes HealthLake needs some tuning, when done Amazon HealthLake work well.\":[\"The wrong prefix is used, \\\"AWS HealthLake\\\" is the correct prefix on first use.\",\"The wrong prefix is used, subsequent uses of this service name require the correct prefix use of \\\"AWS HealthLake\\\", no prefix is also acceptable.\"],\n",
    "    \"SQS and SNS can be used together. SNS can fan out while SQS can handle higher message rates\":[\"The first mention of \\\"SQS\\\" must be \\\"Amazon Simple Queue Service (Amazon SQS)\\\".\", \"The first mention of \\\"SNS\\\" must be \\\"Amazon Simple Notification Service (Amazon SNS)\\\".\", \"Warning: The short version of \\\"SQS\\\" should only be used on subsequent use when space is limited, please use \\\"Amazon SQS\\\" in most scenarios.\", \"Warning: The short version of \\\"SNS\\\" should only be used on subsequent use when space is limited, please use \\\"Amazon SNS\\\" in most scenarios.\"],\n",
    "    \"AWS Lambda is great a scaling to make sure your lambda is run at full speed\": [\"Do not use offering names, or Amazon or AWS trademarks, as common nouns or verbs such as \\\"your lambda\\\"\"],\n",
    "    \"When setting up your system you may want to use AppConfig.\": [\"Prefix \\\"AWS\\\" is required for \\\"AppConfig\\\"\"],\n",
    "    \"To orchistrate code please use step functions.\": [\"The capitalized version of this name must be used \\\"Step Functions\\\"\"],\n",
    "    \"Some people still use Amazon Sumerian\": [\"Warning: \\\"Amazon Sumerian\\\" is marked as \\\"Deprecated\\\"\"],\n",
    "    \"When doing migrations consider using DataSync.\\nUsing DataSync will help you migrate your data with end-to-end security.\": [\"The first mention of \\\"DataSync\\\" must be \\\"AWS DataSync\\\".\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b26801b-1da5-4bc9-bc4e-5fdd77db20b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2b) Volume based decomposition\n",
    "Here we'll consider an example use case of a user who would like to undersand how many unique characters are in a novel, and learn a bit about the three most common characters.  We start by downloading the novel.  For this example we use Frankenstein by Mary Shelley, as it is in public domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c8ce4203-a672-43ec-9d45-d9f3c84c3548",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests, re\n",
    "from bs4 import BeautifulSoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3573c8e5-9d04-437e-a0b4-f5c530591913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#grab the text from the Gutenberg project, a collection of public domain works.\n",
    "#We use Beautiful Soup to parse the HTML of the webpage.\n",
    "url = \"https://www.gutenberg.org/files/84/84-h/84-h.htm\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "raw_full_text_webpage = soup.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a425cf36-5bc9-4715-9639-2cf0aafd32cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate word count: 76553\n",
      "Approximate page count: 153\n"
     ]
    }
   ],
   "source": [
    "#Cut the top and bottom of the webpage so that we only have the text of the book.\n",
    "raw_full_text = raw_full_text_webpage[raw_full_text_webpage.index(\"Letter 1\\n\\nTo Mrs. Saville, England.\"):raw_full_text_webpage.index(\"*** END OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN ***\")].replace(\"\\r\\n\",\" \").replace(\"\\n\", \" \")\n",
    "#encode some misc unicode charaters.\n",
    "full_text = raw_full_text.encode('raw_unicode_escape').decode()\n",
    "#show that we found the expected length\n",
    "words_count = len(full_text.split(\" \"))\n",
    "pages_count = int(words_count/500)#quick estimate, real page count is dependant on page and font size.\n",
    "print (\"Approximate word count:\",words_count)\n",
    "print (\"Approximate page count:\",pages_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2982aa16-8f91-49f2-a5b4-14a8182d10c3",
   "metadata": {},
   "source": [
    "### Now that we have our novel, let's try to find all the unique characters with a single prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e8518fc8-561b-4141-a990-528f18f34b4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "long_prompt_template = \"\"\"Consider the following novel:\n",
    "<novel>\n",
    "{{NOVEL}}\n",
    "</novel>\n",
    "\n",
    "How many unique characters are there with at least one spoken line of dialog?  Please also provide a brief description of the top three most common characters in separate paragraphs. \n",
    "Only count charaters that have at least one spoken line of dialog.\n",
    "\"\"\"\n",
    "\n",
    "long_prompt = long_prompt_template.replace(\"{{NOVEL}}\",full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "bf7d679c-9a1a-46b9-b8eb-9fd7bf3a3d81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time at Bedrock:  42.2 sec\n",
      "Tokens:  98285\n",
      "Responce from model:\n",
      "Based on the novel, there are 8 unique characters that have at least one spoken line of dialog.\n",
      "\n",
      "1. Victor Frankenstein:\n",
      "Victor Frankenstein is the protagonist of the novel. He is a scientist who creates a grotesque but sentient creature through an unorthodox scientific experiment. His obsession with his work and the consequences of his creation drive the plot forward. He is portrayed as a complex character, torn between his ambition and the guilt and remorse he feels for his actions.\n",
      "\n",
      "2. The Creature/Monster:\n",
      "The Creature, often referred to as the Monster, is Frankenstein's creation. Initially seeking companionship and acceptance, he becomes embittered and vengeful after being rejected by his creator and society. He is highly intelligent and articulate, but his hideous appearance and the mistreatment he faces lead him down a path of violence and retribution.\n",
      "\n",
      "3. Robert Walton:\n",
      "Robert Walton is the explorer who rescues Victor Frankenstein near the end of the novel. He serves as the narrator for the framing story and provides a sympathetic ear for Frankenstein's tragic tale. Walton's own ambition and desire for glory parallel Frankenstein's, allowing him to understand the scientist's motivations and struggles.\n",
      "CPU times: user 10.1 ms, sys: 1.13 ms, total: 11.2 ms\n",
      "Wall time: 42.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "long_responce = ask_claude(long_prompt, model=\"sonnet\",ignore_cache=True)\n",
    "print(\"Time at Bedrock: \",long_responce[3],\"sec\")\n",
    "print(\"Tokens: \",long_responce[2][\"input_tokens\"]+long_responce[2][\"output_tokens\"])\n",
    "print(\"Responce from model:\")\n",
    "print(long_responce[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fe75e7-432c-4764-bef0-46ed14dd979c",
   "metadata": {},
   "source": [
    "### Not bad!  93K tokens processed in about 40 seconds.  Let's see if we can make that faster and cheaper using prompt decomposition.\n",
    "### We'll divide the novel into thirds, run each third in parallel, then write a fourth prompt to combine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f29a01a5-0f19-4fbe-a5db-a86607d3e329",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "short_prompt_template = \"\"\"Consider the following portion of a novel:\n",
    "<novel>\n",
    "{{NOVEL}}\n",
    "</novel>\n",
    "\n",
    "Please provide a list of unique characters, each in a character tag.  Inside the character tag should be a name tag with their name,\n",
    "a count tag with an exact count of times they appear, and a description tag with a brief description of that character.\n",
    "Only count charaters that have at least one spoken line of dialog.\n",
    "\"\"\"\n",
    "\n",
    "#let's cut the novel into thirds.\n",
    "third = int(len(full_text)/3)\n",
    "short_prompt_1 = short_prompt_template.replace(\"{{NOVEL}}\",full_text[:third])\n",
    "short_prompt_2 = short_prompt_template.replace(\"{{NOVEL}}\",full_text[third:third+third])\n",
    "short_prompt_3 = short_prompt_template.replace(\"{{NOVEL}}\",full_text[third+third:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8494205-5ae6-4fd6-9f87-345aa5d36b5b",
   "metadata": {},
   "source": [
    "### Now let's run these three prompts in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f2ca0d24-c013-4af4-a647-7ac0e426886b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time at Bedrock:  16.38 sec\n",
      "Example Output:\n",
      "Here is a list of unique characters with their names, counts, and descriptions, based on the provided text:\n",
      "\n",
      "<character>\n",
      "  <name>Victor Frankenstein</name>\n",
      "  <count>138</count>\n",
      "  <description>The narrator and protagonist, a young scientist who creates a hideous sapient creature in an unorthodox scientific experiment.</description>\n",
      "</character>\n",
      "\n",
      "<character>\n",
      "  <name>Elizabeth Lavenza</name>\n",
      "  <count>16</count>\n",
      "  <description>Victor's adopted sister and love interest, who is kind and innocent.</description>\n",
      "</character>\n",
      "\n",
      "<character>\n",
      "  <name>Alphonse Frankenstein</name>\n",
      "  <count>4</count>\n",
      "  <description>Victor's father, who is caring and supportive.</description>\n",
      "</character>\n",
      "\n",
      "<character>\n",
      "  <nam ...\n"
     ]
    }
   ],
   "source": [
    "short_responces = ask_claude_threaded([short_prompt_1,short_prompt_2,short_prompt_3],model='sonnet',ignore_cache=False)\n",
    "time_1 = short_responces[0][3]\n",
    "time_2 = short_responces[1][3]\n",
    "time_3 = short_responces[2][3]\n",
    "average_time = round((time_1+time_2+time_3)/3,2)\n",
    "print(\"Average time at Bedrock: \",average_time,\"sec\")\n",
    "\n",
    "#show the reply from one of the three prompts\n",
    "print(\"Example Output:\")\n",
    "print(short_responces[0][1][:700]+\" ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395a8345-7e65-42e4-a030-37e079c9869b",
   "metadata": {},
   "source": [
    "### So far it's looking good!  We've processed the whole novel in around 17 seconds, down from 42.  Let's make a final call to get a final result that matches our original long prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f49b632f-8909-4eeb-b247-9e2dcbb2669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt_template = \"\"\"Consider the following list of charaters from a novel.  Each entry contains the character's name,\n",
    "a count of the number of times they appeared, and a brief description of that charater:\n",
    "<characters>\n",
    "{{CHARACTERS}}\n",
    "</characters>\n",
    "Some charaters may be listed more than once.  Use the name and description to determine that two entries are the same, \n",
    "and if they are, sum their count to support your responce.\n",
    "\n",
    "How many unique characters are there?  Please also provide a brief description of the top three most common characters in separate paragraphs. \n",
    "\"\"\"\n",
    "\n",
    "characters = short_responces[0][1]+short_responces[1][1]+short_responces[2][1]\n",
    "\n",
    "final_prompt = final_prompt_template.replace(\"{{CHARACTERS}}\",characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a6e003ef-8553-4c1e-b2f9-39e72a5a85bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided list of characters, there are 10 unique characters in total. I have summed the counts for characters with the same name and description.\n",
      "\n",
      "The top three most common characters are:\n",
      "\n",
      "1. Victor Frankenstein (Count: 333)\n",
      "Victor Frankenstein is the protagonist and narrator of the story. He is a young scientist who creates a hideous sapient creature in an unorthodox scientific experiment. Victor's creation haunts him and seeks revenge after being rejected by his creator and society.\n",
      "\n",
      "2. The creature/monster/daemon (Count: 66)\n",
      "The creature, also referred to as the monster or daemon, is the hideous but intelligent being created by Victor Frankenstein. It develops a longing for human companionship and demands that Victor create a female companion for him. After being rejected by Victor and society, the creature seeks revenge.\n",
      "\n",
      "3. Robert Walton (Count: 16)\n",
      "Robert Walton is the explorer who rescues Victor Frankenstein and records his story. He serves as the narrator in the framing story, listening to Victor's account of creating the creature and the subsequent events.\n",
      "CPU times: user 3.05 ms, sys: 3.33 ms, total: 6.38 ms\n",
      "Wall time: 4.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "session_cache = {}#don't use cached info, since we. want to time this.\n",
    "final_responce = ask_claude(final_prompt, model=\"sonnet\")\n",
    "print(final_responce[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1824c0df-796e-4daa-8200-27ca955773c1",
   "metadata": {},
   "source": [
    "### Results: Twice as fast!\n",
    "This final prompt took about 5 seconds to run.  The original long prompt took 42 seconds to run, and our decomposed version took 17s + 5, or 22 seconds total.  Almost twice as fast to do the same amount work!\n",
    "Note that the decomposed version actually found 11 characters, not 10.  This is somewhat common, that the quality will slightly improve with smaller, more focused prompts, because the LLM can focus more when the prompt is smaller."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
